# Self-Organizing-Mixture-of-Experts-SoME-
The core thesis of SoME is that continual learning can be achieved not by retraining a model's entire knowledge base, but by optimizing the pathways to a static library of computational primitives. If this is true it could be a way to effectively recycle the computational primitives of older, open-source models that are just sitting on a shelf. 
