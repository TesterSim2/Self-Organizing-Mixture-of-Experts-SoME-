{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SoME v7 (The Clamp Fix) code\n",
        "\n",
        "# Based on v6, correcting the \"Immortal Expert\" bug by removing forced normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "print(\"--- Part 1: Setup and Dependencies ---\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs (Crucial for 40GB optimization)\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    # Enable benchmark mode for cuDNN\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Core SoME Framework (v7 Clamp Fix)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# --- 1. Model Component Classes ---\n",
        "\n",
        "class Expert(nn.Module):\n",
        "   \"\"\"An expert module with configurable random weight initialization.\"\"\"\n",
        "   def __init__(self, d_model, d_ffn, init_method='default'):\n",
        "      super().__init__()\n",
        "\n",
        "    self.w_down = nn.Linear(d_model, d_ffn)\n",
        "    self.activation = nn.GELU()\n",
        "    self.w_up = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "    if init_method == 'orthogonal':\n",
        "        nn.init.orthogonal_(self.w_down.weight)\n",
        "        nn.init.orthogonal_(self.w_up.weight)\n",
        "    elif init_method == 'sparse':\n",
        "        nn.init.sparse_(self.w_down.weight, sparsity=0.5)\n",
        "        nn.init.sparse_(self.w_up.weight, sparsity=0.5)\n",
        "    elif init_method != 'default':\n",
        "        raise ValueError(f\"Unknown initialization method: {init_method}\")\n",
        "\n",
        "    nn.init.zeros_(self.w_down.bias)\n",
        "    nn.init.zeros_(self.w_up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, some_config):\n",
        "     super().__init__()\n",
        "     self.d_model = d_model\n",
        "     self.num_experts = some_config['num_experts']\n",
        "     self.d_ffn = some_config['d_ffn']\n",
        "     self.top_k = some_config['top_k']\n",
        "\n",
        "    # Heuristic update parameters\n",
        "    self.alpha = some_config['alpha']\n",
        "    self.beta = some_config['beta']\n",
        "    self.delta = some_config['delta']\n",
        "\n",
        "    # v6 Feature: Respawn Threshold (The \"Phoenix\" Trigger)\n",
        "    self.respawn_threshold = some_config.get('respawn_threshold', 0.1)\n",
        "\n",
        "    # Key management parameters\n",
        "    self.theta_percentile = some_config['theta_percentile']\n",
        "    self.warmup_steps = some_config['warmup_steps']\n",
        "    self.ema_decay = some_config['ema_decay']\n",
        "\n",
        "    self.ablation_flags = some_config.get(\n",
        "      'ablation_flags',\n",
        "      {'use_alpha': True, 'use_beta': True, 'use_delta': True}\n",
        "    )\n",
        "\n",
        "print(f\"SoME Layer Ablation Flags: {self.ablation_flags}\")\n",
        "\n",
        "# Router / query network (trainable)\n",
        "self.router_type = (some_config.get(\"router_type\", \"linear\") or \"linear\").lower()\n",
        "if self.router_type == \"linear\":\n",
        "    self.query_network = nn.Linear(d_model, d_model, bias=True)\n",
        "elif self.router_type in (\"mlp\", \"mlp_router\", \"mlp-router\"):\n",
        "    mult = float(some_config.get(\"router_mlp_mult\", 1.0))\n",
        "    hidden = max(1, int(d_model * mult))\n",
        "    dropout = float(some_config.get(\"router_dropout\", 0.0))\n",
        "    layers = [nn.Linear(d_model, hidden, bias=True), nn.GELU()]\n",
        "    if dropout > 0.0:\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "    layers.append(nn.Linear(hidden, d_model, bias=True))\n",
        "    self.query_network = nn.Sequential(*layers)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown router_type={self.router_type!r}\")\n",
        "\n",
        "# Init query network\n",
        "for m in self.query_network.modules():\n",
        "   if isinstance(m, nn.Linear):\n",
        "       nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "       if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias)\n",
        "\n",
        "# Initialize keys on unit sphere\n",
        "keys = torch.randn(self.num_experts, d_model)\n",
        "self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "\n",
        "# Usage tracking\n",
        "self.register_buffer(\"usage_count\", torch.zeros(self.num_experts))\n",
        "self.register_buffer(\"usage_mass\", torch.zeros(self.num_experts))\n",
        "self.register_buffer(\"steps\", torch.zeros((), dtype=torch.long))\n",
        "\n",
        "# Dead expert tracking for logging\n",
        "self.register_buffer(\"dead_expert_count\", torch.zeros((), dtype=torch.long))\n",
        "\n",
        "self.experts = nn.ModuleList([\n",
        "   Expert(d_model, self.d_ffn, init_method=some_config['init_method'])\n",
        "   for _ in range(self.num_experts)\n",
        "])\n",
        "\n",
        "# Freeze all expert parameters\n",
        "for expert in self.experts:\n",
        "\n",
        "          for param in expert.parameters():\n",
        "             param.requires_grad = False\n",
        "\n",
        "        if self.top_k > 1:\n",
        "            self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "\n",
        "        # Routing computations in fp32\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "           queries_raw = self.query_network(x_flat.float())\n",
        "           queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "\n",
        "          # Dot product scores.\n",
        "          # Low magnitude (decayed) keys will naturally have low scores.\n",
        "          scores = torch.matmul(queries, self.key_store.t())\n",
        "\n",
        "          top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "          gating_weights = F.softmax(top_k_scores / float(temperature), dim=-1)\n",
        "\n",
        "     # Expert dispatch\n",
        "     flat_top_k_indices = top_k_indices.view(-1)\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "        flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "        permuted_inputs = flat_inputs[permutation_map]\n",
        "        split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "        output_chunks = []\n",
        "        for i, expert_id in enumerate(unique_expert_ids):\n",
        "           output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "        concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "        inverse_permutation_map = torch.argsort(permutation_map)\n",
        "        expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "        gating_weights_for_mul = gating_weights.to(expert_outputs.dtype)\n",
        "        weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "                  gating_weights_for_mul.unsqueeze(-1)).sum(dim=1)\n",
        "\n",
        "  final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "  return x + final_output, queries, top_k_indices, gating_weights\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_keys(self, queries, top_k_indices, gating_weights):\n",
        "  self.steps += 1\n",
        "\n",
        "  num_tokens = top_k_indices.shape[0]\n",
        "  if num_tokens == 0: return\n",
        "\n",
        "  # 1. Update Usage Stats\n",
        "  flat_idx = top_k_indices.reshape(-1)\n",
        "  unique_indices, counts = torch.unique(flat_idx, return_counts=True)\n",
        "  counts_f = counts.to(torch.float32)\n",
        "\n",
        "  self.usage_count.mul_(self.ema_decay)\n",
        "  self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts_f)\n",
        "\n",
        "  if gating_weights is not None:\n",
        "      flat_w = gating_weights.reshape(-1).to(torch.float32)\n",
        "      mass_sums = torch.zeros_like(self.usage_mass)\n",
        "      mass_sums.index_add_(0, flat_idx, flat_w)\n",
        "      self.usage_mass.mul_(self.ema_decay)\n",
        "      self.usage_mass.add_((1.0 - self.ema_decay) * mass_sums)\n",
        "\n",
        "  inertia_source = self.usage_count\n",
        "\n",
        "  # 2. Calculate Updates (Attraction + Peer Pull)\n",
        "  if self.ablation_flags.get('use_alpha', True):\n",
        "      for i in range(self.top_k):\n",
        "         indices = top_k_indices[:, i]\n",
        "         inertia = 1.0 + inertia_source[indices]\n",
        "         alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "\n",
        "       update_vec = queries - self.key_store[indices]\n",
        "       self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "  if self.top_k > 1 and self.ablation_flags.get('use_beta', True):\n",
        "      indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "      indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "\n",
        "    keys_i, keys_j = self.key_store[indices_i], self.key_store[indices_j]\n",
        "\n",
        "  inertia_i = (1.0 + inertia_source[indices_i]).unsqueeze(-1)\n",
        "  inertia_j = (1.0 + inertia_source[indices_j]).unsqueeze(-1)\n",
        "\n",
        "  beta_effective = self.beta / torch.max(inertia_i, inertia_j)\n",
        "\n",
        "  update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "  update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "\n",
        "  self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "  self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "# 3. THE CLAMP FIX (v7)\n",
        "# Instead of F.normalize() which snaps norm to 1.0, we only clamp.\n",
        "# This allows keys to have norm < 1.0 (preserving decay).\n",
        "# We only shrink keys that have accidentally grown > 1.0 due to updates.\n",
        "current_norms = self.key_store.norm(p=2, dim=-1, keepdim=True)\n",
        "# Avoid division by zero, though unlikely with > 1.0 check\n",
        "clamp_mask = (current_norms > 1.0)\n",
        "# Only modify keys that are too large\n",
        "if clamp_mask.any():\n",
        "     self.key_store = torch.where(\n",
        "       clamp_mask,\n",
        "       self.key_store / current_norms,\n",
        "       self.key_store\n",
        "     )\n",
        "\n",
        "# 4. Apply Decay\n",
        "if self.steps > self.warmup_steps and self.ablation_flags.get('use_delta', True):\n",
        "    active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "    if active_usage_counts.numel() > 0:\n",
        "        dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "        low_usage_mask = self.usage_count < dynamic_theta\n",
        "        # Permanent shrinkage!\n",
        "        self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "# 5. The Phoenix Mechanism (Respawn)\n",
        "# Recalculate norms after decay\n",
        "key_norms = self.key_store.norm(p=2, dim=-1)\n",
        "dead_mask = key_norms < self.respawn_threshold\n",
        "num_dead = dead_mask.sum().item()\n",
        "\n",
        "self.dead_expert_count.fill_(num_dead) # Track for logging\n",
        "\n",
        "    if num_dead > 0:\n",
        "        # Pick random queries from the current batch to seed the new experts\n",
        "        flat_queries = queries.view(-1, self.d_model)\n",
        "\n",
        "       if flat_queries.size(0) >= num_dead:\n",
        "           rand_indices = torch.randperm(flat_queries.size(0),\n",
        "device=queries.device)[:num_dead]\n",
        "           new_keys = flat_queries[rand_indices].clone()\n",
        "           # Normalize new keys to 1.0 (Full Brightness)\n",
        "           new_keys = F.normalize(new_keys + torch.randn_like(new_keys) * 0.01, p=2, dim=-1)\n",
        "\n",
        "         # Overwrite dead keys\n",
        "         self.key_store[dead_mask] = new_keys\n",
        "\n",
        "         # Reset usage stats for respawned experts (fresh start)\n",
        "         self.usage_count[dead_mask] = 0.0\n",
        "         self.usage_mass[dead_mask] = 0.0\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_config):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = SOMELayer(d_model, some_config)\n",
        "\n",
        "   def forward(self, x, attention_mask=None, temperature=1.0):\n",
        "     seq_len = x.size(1)\n",
        "     causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device),\n",
        "diagonal=1).bool()\n",
        "\n",
        "    key_padding_mask = None\n",
        "    if attention_mask is not None:\n",
        "        key_padding_mask = (attention_mask == 0)\n",
        "\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=causal_mask,\n",
        "key_padding_mask=key_padding_mask)\n",
        "    x = self.norm1(x + attn_output)\n",
        "    some_output, queries, top_k_indices, gating_weights = self.some_layer(x,\n",
        "temperature=temperature)\n",
        "    x = self.norm2(some_output)\n",
        "    return x, queries, top_k_indices, gating_weights\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, max_len=5000):\n",
        "    super().__init__()\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "    pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, model_config, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(model_config['vocab_size'], model_config['d_model'])\n",
        "     self.pos_encoder = PositionalEncoding(model_config['d_model'], model_config['seq_len'])\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(model_config['d_model'], model_config['num_heads'],\n",
        "some_config)\n",
        "        for _ in range(model_config['num_layers'])\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(model_config['d_model'], model_config['vocab_size'])\n",
        "     self.d_model = model_config['d_model']\n",
        "\n",
        "  def forward(self, x, attention_mask=None, temperature=1.0):\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)\n",
        "\n",
        "    all_queries, all_indices, all_gates = [], [], []\n",
        "    for layer in self.layers:\n",
        "       x, queries, top_k_indices, gating_weights = layer(x, attention_mask=attention_mask,\n",
        "temperature=temperature)\n",
        "       all_queries.append(queries)\n",
        "       all_indices.append(top_k_indices)\n",
        "       all_gates.append(gating_weights)\n",
        "\n",
        "    return self.fc_out(x), all_queries, all_indices, all_gates\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices, all_gates, token_mask=None):\n",
        "    if token_mask is not None:\n",
        "        if token_mask.dim() == 2:\n",
        "            token_mask = token_mask.reshape(-1)\n",
        "\n",
        "         token_mask = token_mask.to(dtype=torch.bool, device=all_indices[0].device)\n",
        "\n",
        "      for layer, q, idx, g in zip(self.layers, all_queries, all_indices, all_gates):\n",
        "         if token_mask is not None:\n",
        "             if q is not None: q = q[token_mask]\n",
        "             if idx is not None: idx = idx[token_mask]\n",
        "             if g is not None: g = g[token_mask]\n",
        "\n",
        "         layer.some_layer.update_keys(q, idx, g)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Data Preparation & Training (Standard)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data, pad_token_id: int, eos_token_id: int = None):\n",
        "     self.data = tokenized_data\n",
        "     self.pad_token_id = pad_token_id\n",
        "     self.eos_token_id = eos_token_id\n",
        "\n",
        "   def __len__(self):\n",
        "     return len(self.data)\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "     item = self.data[idx]\n",
        "     input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
        "\n",
        "      if \"attention_mask\" in item:\n",
        "          attention_mask = torch.tensor(item[\"attention_mask\"], dtype=torch.long)\n",
        "      else:\n",
        "          attention_mask = (input_ids != self.pad_token_id).long()\n",
        "\n",
        "      targets = input_ids.clone()\n",
        "      targets[:-1] = input_ids[1:]\n",
        "      targets[-1] = -100\n",
        "\n",
        "      if self.pad_token_id is not None:\n",
        "          targets[targets == self.pad_token_id] = -100\n",
        "      if self.eos_token_id is not None:\n",
        "          eos_cum = (input_ids == self.eos_token_id).cumsum(dim=0)\n",
        "          targets[eos_cum > 0] = -100\n",
        "\n",
        "      return input_ids, targets, attention_mask\n",
        "\n",
        "def prepare_data(config):\n",
        "  print(\"\\n--- Part 2: Data Preparation & Configuration ---\")\n",
        "  tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "\n",
        "  if not os.path.exists(tokenizer_path):\n",
        "      print(\"Training custom tokenizer...\")\n",
        "      dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "      def get_training_corpus():\n",
        "         for i in range(0, len(dataset), 1000):\n",
        "            yield dataset[i : i + 1000][\"text\"]\n",
        "\n",
        "     tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "     tokenizer_model.pre_tokenizer = Whitespace()\n",
        "     trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"],\n",
        "vocab_size=config['model']['vocab_size'])\n",
        "     tokenizer_model.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "     tokenizer_model.save(tokenizer_path)\n",
        "     tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "  else:\n",
        "     print(\"Tokenizer already exists. Loading from file.\")\n",
        "     tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "  config['model']['vocab_size'] = tokenizer.vocab_size\n",
        "  print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "  print(\"\\nTokenizing dataset...\")\n",
        "  full_dataset = load_dataset(\"roneneldan/TinyStories\", streaming=False)\n",
        "  train_subset = full_dataset['train'].select(range(config['data']['train_subset_size']))\n",
        "  val_subset = full_dataset['validation'].select(range(config['data']['val_subset_size']))\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    text_with_eos = [s + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "    return tokenizer(text_with_eos, truncation=True, padding=\"max_length\",\n",
        "max_length=config['model']['seq_len'], return_tensors=\"pt\")\n",
        "\n",
        "  tokenized_train = train_subset.map(tokenize_function, batched=True,\n",
        "remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
        "  tokenized_val = val_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "\n",
        "  train_dataset = LanguageModelDataset(tokenized_train,\n",
        "pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "  validation_dataset = LanguageModelDataset(tokenized_val,\n",
        "pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], shuffle=True,\n",
        "drop_last=True, num_workers=2, pin_memory=True)\n",
        "  validation_loader = DataLoader(validation_dataset, batch_size=config['data']['batch_size'],\n",
        "drop_last=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "  return train_loader, validation_loader, tokenizer\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp, vocab_size):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "   for input_ids, targets, attention_mask in progress_bar:\n",
        "      input_ids, targets, attention_mask = input_ids.to(device, non_blocking=True),\n",
        "targets.to(device, non_blocking=True), attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "       logits, queries, indices, gates = model(input_ids, attention_mask=attention_mask,\n",
        "temperature=current_temp)\n",
        "       loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "     scheduler.step()\n",
        "\n",
        "     model_state = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
        "     model_state.update_all_keys(queries, indices, gates, token_mask=(targets.view(-1) !=\n",
        "-100))\n",
        "\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, vocab_size, eval_temp):\n",
        "   model.eval()\n",
        "   total_loss = 0\n",
        "   with torch.no_grad():\n",
        "      for input_ids, targets, attention_mask in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "         input_ids, targets, attention_mask = input_ids.to(device), targets.to(device),\n",
        "attention_mask.to(device)\n",
        "         with torch.cuda.amp.autocast():\n",
        "            logits, _, _, _ = model(input_ids, attention_mask=attention_mask,\n",
        "temperature=eval_temp)\n",
        "            loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
        "         total_loss += loss.item()\n",
        "   return total_loss / len(dataloader)\n",
        "\n",
        "def main(config):\n",
        "  print(f\"\\n--- Starting Experiment: {config['run_name']} ---\")\n",
        "  train_loader, val_loader, tokenizer = prepare_data(config)\n",
        "\n",
        "  print(\"\\n--- Part 3: Model Definition ---\")\n",
        "  model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "  model = torch.compile(model)\n",
        "\n",
        "   optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "lr=config['training']['learning_rate'], weight_decay=0.1)\n",
        "   criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "   scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)\n",
        "* config['training']['num_epochs'])\n",
        "\n",
        "  print(\"\\n--- Part 4: Training ---\")\n",
        "  for epoch in range(config['training']['num_epochs']):\n",
        "     train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler,\n",
        "config['training']['training_temp'], config['model']['vocab_size'])\n",
        "     val_loss = evaluate_epoch(model, val_loader, criterion, config['model']['vocab_size'],\n",
        "config['training']['eval_temp'])\n",
        "\n",
        "     model_state = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "     mid_layer = model_state.layers[config['model']['num_layers'] // 2].some_layer\n",
        "     dead_count = mid_layer.dead_expert_count.item()\n",
        "\n",
        "     gini = calculate_gini(mid_layer.usage_count)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f},\n",
        "PPL={math.exp(val_loss):.2f}\")\n",
        "    print(f\" Middle Layer: Gini={gini:.3f}, Phoenix Respawns (Last Step)={dead_count}\")\n",
        "\n",
        "     torch.save(model_state.state_dict(), f\"best_model_{config['run_name']}.pth\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Config & Run\n",
        "# ------------------------------------------------------------------------------\n",
        "config = {\n",
        "   \"run_name\": \"v7_ClampFix_Phoenix\",\n",
        "   \"data\": { \"train_subset_size\": 20000, \"val_subset_size\": 2000, \"batch_size\": 32 },\n",
        "   \"model\": { \"vocab_size\": 8192, \"d_model\": 512, \"num_heads\": 8, \"num_layers\": 8, \"seq_len\":\n",
        "768 },\n",
        "   \"some_layer\": {\n",
        "      \"num_experts\": 256,\n",
        "      \"d_ffn\": 1024,\n",
        "      \"top_k\": 4,\n",
        "      \"init_method\": \"sparse\",\n",
        "      \"alpha\": 0.015,\n",
        "      \"beta\": 0.001,\n",
        "      \"delta\": 0.005, # Decay\n",
        "      \"respawn_threshold\": 0.1, # Phoenix Threshold\n",
        "      \"theta_percentile\": 0.05,\n",
        "      \"warmup_steps\": 200,\n",
        "      \"ema_decay\": 0.995,\n",
        "      \"router_type\": \"mlp\", \"router_mlp_mult\": 2.0,\n",
        "      \"ablation_flags\": {\"use_alpha\": True, \"use_beta\": True, \"use_delta\": True}\n",
        "   },\n",
        "   \"training\": { \"num_epochs\": 4, \"learning_rate\": 6e-4, \"training_temp\": 1.0, \"eval_temp\": 1.0 }\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(config)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}