{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n",
        "========================================================================\n",
        "============\n",
        "# SoME Transformer V4.3 - Production-Optimized & All Bugs Fixed\n",
        "# Implements:\n",
        "# 1. All previous optimizations.\n",
        "# 2. Definitive fix for DataLoader TypeError.\n",
        "# 3. Definitive fix for index_add_ RuntimeError.\n",
        "# 4. Using your specified \"shrunk\" A100-Safe configuration.\n",
        "#\n",
        "========================================================================\n",
        "============\n",
        "\n",
        "# Part 1: Setup and Dependencies\n",
        "# ===============================\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs for a free performance boost\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Part 2: Data Preparation (Fixed)\n",
        "# ========================================\n",
        "SEQ_LEN = 512\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = 8192\n",
        "\n",
        "print(\"--- Loading/Training Custom Tokenizer ---\")\n",
        "tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "if not os.path.exists(tokenizer_path):\n",
        "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "    def get_training_corpus():\n",
        "       for i in range(0, len(dataset), 1000):\n",
        "          yield dataset[i : i + 1000][\"text\"]\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"], vocab_size=VOCAB_SIZE)\n",
        "    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "    tokenizer.save(tokenizer_path)\n",
        "else:\n",
        "    print(\"Tokenizer already exists. Loading from file.\")\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "print(\"\\n--- Tokenizing Dataset ---\")\n",
        "full_dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "train_subset_size = 40000\n",
        "val_subset_size = 10000\n",
        "train_subset = full_dataset['train'].select(range(train_subset_size))\n",
        "val_subset = full_dataset['validation'].select(range(val_subset_size))\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(\n",
        "     examples[\"text\"], truncation=True, padding=\"max_length\",\n",
        "     max_length=SEQ_LEN\n",
        "  )\n",
        "\n",
        "tokenized_train = train_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "\n",
        "tokenized_val = val_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data):\n",
        "     self.data = tokenized_data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    inputs = torch.tensor(item['input_ids'])\n",
        "    targets = torch.roll(inputs, shifts=-1, dims=0)\n",
        "    targets[-1] = -100\n",
        "    return inputs, targets\n",
        "\n",
        "train_dataset = LanguageModelDataset(tokenized_train)\n",
        "validation_dataset = LanguageModelDataset(tokenized_val)\n",
        "\n",
        "CPU_COUNT = os.cpu_count()\n",
        "NUM_WORKERS = max(2, CPU_COUNT // 2 if CPU_COUNT else 2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, drop_last=True,\n",
        "num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train dataset size (subset): {len(train_dataset)}\")\n",
        "print(f\"Using {NUM_WORKERS} workers for DataLoader.\")\n",
        "\n",
        "\n",
        "# Part 3: Model Definition (Optimized & Fixed)\n",
        "# =============================================\n",
        "class Expert(nn.Module):\n",
        "   def __init__(self, d_model, d_ffn):\n",
        "     super().__init__()\n",
        "     self.w_down = nn.Linear(d_model, d_ffn)\n",
        "     self.activation = nn.GELU()\n",
        "     self.w_up = nn.Linear(d_ffn, d_model)\n",
        "   def forward(self, x):\n",
        "     return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_experts, d_ffn, top_k, alpha=0.01, beta=0.001, delta=0.001,\n",
        "theta_percentile=0.05, warmup_steps=1000):\n",
        "      super().__init__()\n",
        "      self.d_model, self.num_experts, self.d_ffn, self.top_k = d_model, num_experts, d_ffn, top_k\n",
        "      self.alpha, self.beta, self.delta = alpha, beta, delta\n",
        "      self.theta_percentile = theta_percentile\n",
        "      self.warmup_steps = warmup_steps\n",
        "      self.query_network = nn.Linear(d_model, d_model)\n",
        "      keys = torch.randn(num_experts, d_model)\n",
        "      self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "      self.register_buffer(\"usage_count\", torch.zeros(num_experts))\n",
        "      self.register_buffer(\"steps\", torch.tensor([0], dtype=torch.long))\n",
        "      self.experts = nn.ModuleList([Expert(d_model, d_ffn) for _ in range(num_experts)])\n",
        "      for expert in self.experts:\n",
        "          for param in expert.parameters():\n",
        "             param.requires_grad = False\n",
        "      if self.top_k > 1:\n",
        "          self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "    queries = self.query_network(x_flat)\n",
        "    scores = torch.matmul(queries, self.key_store.t())\n",
        "    top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "    gating_weights = F.softmax(top_k_scores, dim=-1)\n",
        "\n",
        "     flat_top_k_indices = top_k_indices.view(-1)\n",
        "\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "     flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "     permuted_inputs = flat_inputs[permutation_map]\n",
        "\n",
        "     split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "     output_chunks = []\n",
        "     for i, expert_id in enumerate(unique_expert_ids):\n",
        "        output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "     concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "\n",
        "    inverse_permutation_map = torch.argsort(permutation_map)\n",
        "    expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "     weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "gating_weights.unsqueeze(-1)).sum(dim=1)\n",
        "     final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    return x + final_output, queries, top_k_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_keys(self, queries, top_k_indices):\n",
        "    self.steps += 1\n",
        "\n",
        "    unique_indices, counts = torch.unique(top_k_indices, return_counts=True)\n",
        "\n",
        "    # DEFINITIVE FIX for RuntimeError: Cast `counts` to float before adding.\n",
        "    self.usage_count.index_add_(0, unique_indices, counts.float())\n",
        "\n",
        "    for i in range(self.top_k):\n",
        "       indices = top_k_indices[:, i]\n",
        "       inertia = 1.0 + self.usage_count[indices]\n",
        "       alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "       update_vec = queries - self.key_store[indices]\n",
        "       self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "    if self.top_k > 1:\n",
        "        indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "        indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "        keys_i = self.key_store[indices_i]\n",
        "        keys_j = self.key_store[indices_j]\n",
        "        inertia_i = (1.0 + self.usage_count[indices_i]).unsqueeze(-1)\n",
        "        inertia_j = (1.0 + self.usage_count[indices_j]).unsqueeze(-1)\n",
        "        beta_effective = self.beta / torch.min(inertia_i, inertia_j)\n",
        "        update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "        update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "        self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "        self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "    self.key_store.data = F.normalize(self.key_store.data, p=2, dim=-1)\n",
        "\n",
        "    if self.steps > self.warmup_steps:\n",
        "        active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "        if active_usage_counts.numel() > 0:\n",
        "            dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "\n",
        "          low_usage_mask = self.usage_count < dynamic_theta\n",
        "          self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_layer):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = some_layer\n",
        "   def forward(self, x):\n",
        "     attn_output, _ = self.attention(x, x, x)\n",
        "     x = self.norm1(x + attn_output)\n",
        "     some_output, queries, top_k_indices = self.some_layer(x)\n",
        "     x = self.norm2(some_output)\n",
        "     return x, queries, top_k_indices\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "     super().__init__()\n",
        "     pe = torch.zeros(max_len, d_model)\n",
        "     position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "     pe[:, 0::2] = torch.sin(position * div_term)\n",
        "     pe[:, 1::2] = torch.cos(position * div_term)\n",
        "     pe = pe.unsqueeze(0)\n",
        "     self.register_buffer('pe', pe)\n",
        "   def forward(self, x):\n",
        "     return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, vocab_size, d_model, num_heads, num_layers, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "     self.pos_encoder = PositionalEncoding(d_model, max_len=SEQ_LEN)\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(d_model, num_heads, SOMELayer(d_model=d_model,\n",
        "**some_config))\n",
        "        for _ in range(num_layers)\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
        "\n",
        "     x = self.pos_encoder(x)\n",
        "     all_queries, all_indices = [], []\n",
        "     for layer in self.layers:\n",
        "        x, queries, top_k_indices = layer(x)\n",
        "        all_queries.append(queries)\n",
        "        all_indices.append(top_k_indices)\n",
        "     return self.fc_out(x), all_queries, all_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices):\n",
        "    for i, layer_block in enumerate(self.layers):\n",
        "       queries = all_queries[i].view(-1, layer_block.some_layer.d_model)\n",
        "       indices = all_indices[i].view(-1, layer_block.some_layer.top_k)\n",
        "       layer_block.some_layer.update_keys(queries, indices)\n",
        "\n",
        "# Part 4: Training, Evaluation, and Metrics\n",
        "# ==========================================================\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0: return 0.0\n",
        "  probs = usage_counts / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "  for inputs, targets in progress_bar:\n",
        "      inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "      with torch.amp.autocast(\"cuda\"):\n",
        "         logits, queries, indices = model(inputs)\n",
        "         loss = criterion(logits.view(-1, tokenizer.vocab_size), targets.view(-1))\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "     scheduler.step()\n",
        "\n",
        "     model.update_all_keys(queries, indices)\n",
        "\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "  with torch.no_grad():\n",
        "     for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "           logits, _, _ = model(inputs)\n",
        "           loss = criterion(logits.view(-1, tokenizer.vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, epochs):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "  plt.savefig('loss_curve_a100_safe.png')\n",
        "  print(\"\\nLoss curve plot saved to loss_curve_a100_safe.png\")\n",
        "\n",
        "# Part 5: Main Execution Block (Your \"Shrunk\" A100-Safe Config)\n",
        "\n",
        "# ===================================================\n",
        "# This is the shrunk configuration you were running that caused the OOM.\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8\n",
        "some_config = {\n",
        "  \"num_experts\": 256,\n",
        "  \"d_ffn\": 1536,\n",
        "  \"top_k\": 8,\n",
        "  \"alpha\": 0.01,\n",
        "  \"beta\": 0.001,\n",
        "  \"delta\": 0.001,\n",
        "  \"theta_percentile\": 0.05,\n",
        "  \"warmup_steps\": 2000\n",
        "}\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 6e-4\n",
        "\n",
        "# --- Initialization ---\n",
        "model = SOMETransformer(\n",
        "   vocab_size=tokenizer.vocab_size, d_model=D_MODEL, num_heads=NUM_HEADS,\n",
        "   num_layers=NUM_LAYERS, some_config=some_config\n",
        ").to(device)\n",
        "\n",
        "if hasattr(torch, 'compile'):\n",
        "    print(\"\\nCompiling the model for faster training...\")\n",
        "    model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if\n",
        "p.requires_grad)/1e6:.2f}M\")\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "\n",
        "\n",
        "# --- Run Training and Metric Tracking ---\n",
        "train_losses, val_losses = [], []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "   print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "   train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler)\n",
        "   val_loss = evaluate_epoch(model, validation_loader, criterion)\n",
        "   perplexity = math.exp(val_loss)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  model_to_inspect = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "  usage_counts = model_to_inspect.layers[NUM_LAYERS // 2].some_layer.usage_count\n",
        "  gini_coeff = calculate_gini(usage_counts)\n",
        "  entropy_val = calculate_entropy(usage_counts)\n",
        "\n",
        "   print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Perplexity\n",
        "= {perplexity:.2f}\")\n",
        "   print(f\" \u2514\u2500 Middle Layer Expert Metrics: Gini = {gini_coeff:.3f}, Entropy = {entropy_val:.3f}\")\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save(model_to_inspect.state_dict(), \"best_some_transformer_a100_safe.pth\")\n",
        "      print(\"Model saved as best_some_transformer_a100_safe.pth\")\n",
        "\n",
        "print(\"\\n--- A100-Safe Training Complete ---\")\n",
        "plot_losses(train_losses, val_losses, NUM_EPOCHS)\n",
        "\n",
        "____________________________________\n",
        "\n",
        " Using device: cuda\n",
        "A100 GPU detected. Enabling TF32.\n",
        "--- Loading/Training Custom Tokenizer ---\n",
        "README.md: 1.06k/? [00:00<00:00, 106kB/s]data/train-00000-of-00004-2d5a1467fff108(\u2026):\n",
        "  100% 249M/249M [00:01<00:00, 260MB/s]data/train-00001-of-00004-5852b56a2bd28f(\u2026):\n",
        "100% 248M/248M [00:01<00:00, 209MB/s]data/train-00002-of-00004-a26307300439e9(\u2026):\n",
        "100% 246M/246M [00:01<00:00, 235MB/s]data/train-00003-of-00004-d243063613e5a0(\u2026):\n",
        "100% 248M/248M [00:01<00:00, 230MB/s]data/validation-00000-of-00001-869c898b5(\u2026): 1\n",
        "00% 9.99M/9.99M [00:00<00:00, 23.1MB/s]Generating train split: 100% 2119719/2119719\n",
        "[00:06<00:00, 355343.88 examples/s]Generating validation split: 100% 21990/21990 [00:00\n",
        "<00:00, 290010.90 examples/s]Custom tokenizer loaded with vocab size: 8192\n",
        "\n",
        "--- Tokenizing Dataset ---\n",
        "\n",
        "Map (num_proc=12): 100% 40000/40000 [00:04<00:00, 14113.17 examples/s]Map (num_pr\n",
        "oc=12): 100% 10000/10000 [00:01<00:00, 825.39 examples/s]Train dataset size (subset):\n",
        "40000\n",
        "Using 6 workers for DataLoader.\n",
        "\n",
        "Compiling the model for faster training...\n",
        "\n",
        "Total parameters: 3244.34M\n",
        "Trainable parameters: 18.92M\n",
        "Total training steps: 6250\n",
        "\n",
        "--- Epoch 1/10 ---\n",
        "Training: 0%|           | 0/625 [00:00<?,\n",
        "?it/s]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning:\n",
        "Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you\n",
        "should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`. Failure to\n",
        "do this will result in PyTorch skipping the first value of the learning rate schedule. See more\n",
        "details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "  warnings.warn(\n",
        "Epoch 1: Train Loss = 1.5334, Val Loss = 1.1589, Val Perplexity = 3.19\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.878, Entropy = 5.178\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 2/10 ---\n",
        "Epoch 2: Train Loss = 1.1551, Val Loss = 0.9970, Val Perplexity = 2.71\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.874, Entropy = 5.221\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 3/10 ---\n",
        "Epoch 3: Train Loss = 0.9901, Val Loss = 0.8513, Val Perplexity = 2.34\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.871, Entropy = 5.245\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 4/10 ---\n",
        "Epoch 4: Train Loss = 0.8322, Val Loss = 0.7234, Val Perplexity = 2.06\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.869, Entropy = 5.262\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 5/10 ---\n",
        "Epoch 5: Train Loss = 0.7033, Val Loss = 0.6187, Val Perplexity = 1.86\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.867, Entropy = 5.274\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 6/10 ---\n",
        "\n",
        "Epoch 6: Train Loss = 0.5965, Val Loss = 0.5393, Val Perplexity = 1.71\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.866, Entropy = 5.281\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 7/10 ---\n",
        "Epoch 7: Train Loss = 0.5132, Val Loss = 0.4757, Val Perplexity = 1.61\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.865, Entropy = 5.286\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 8/10 ---\n",
        "Epoch 8: Train Loss = 0.4517, Val Loss = 0.4365, Val Perplexity = 1.55\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.865, Entropy = 5.290\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 9/10 ---\n",
        "Epoch 9: Train Loss = 0.4108, Val Loss = 0.4113, Val Perplexity = 1.51\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.864, Entropy = 5.293\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- Epoch 10/10 ---\n",
        "Epoch 10: Train Loss = 0.3904, Val Loss = 0.4049, Val Perplexity = 1.50\n",
        "  \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.864, Entropy = 5.296\n",
        "Model saved as best_some_transformer_a100_safe.pth\n",
        "\n",
        "--- A100-Safe Training Complete ---\n",
        "\n",
        "Loss curve plot saved to loss_curve_a100_safe.png\n",
        "\n",
        "____________________________________\n",
        "#\n",
        "========================================================================\n",
        "============\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Advanced Visualization of Expert \"Knowledge Galaxies\"\n",
        "#\n",
        "========================================================================\n",
        "============\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ensure the model class definitions from the previous cell are available\n",
        "# (SOMETransformer, SOMETransformerBlock, SOMELayer, Expert, PositionalEncoding)\n",
        "\n",
        "# --- Step 1: Re-instantiate the model with the exact trained architecture ---\n",
        "# This uses the configuration variables from the previous cell to prevent errors.\n",
        "print(\"--- Re-instantiating model for visualization ---\")\n",
        "model_viz = SOMETransformer(\n",
        "   vocab_size=tokenizer.vocab_size,\n",
        "   d_model=D_MODEL,\n",
        "\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    some_config=some_config\n",
        ")\n",
        "\n",
        "# --- Step 2: Load the saved weights ---\n",
        "model_path = \"best_some_transformer_a100_safe.pth\"\n",
        "print(f\"--- Loading weights from {model_path} ---\")\n",
        "try:\n",
        "   # We load onto the CPU for analysis to free up GPU VRAM\n",
        "   model_viz.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
        "   print(\"Successfully loaded trained model weights.\")\n",
        "except Exception as e:\n",
        "   print(f\"Error loading model weights: {e}\")\n",
        "   print(\"Please ensure the model file exists and the architecture matches the saved weights.\")\n",
        "\n",
        "model_viz.eval()\n",
        "\n",
        "# --- Step 3: Define a reusable plotting function ---\n",
        "def plot_galaxy(layer_index, perplexity=30, n_iter=1000):\n",
        "   \"\"\"\n",
        "   Extracts keys and usage from a specific layer, runs t-SNE, and plots the result.\n",
        "   \"\"\"\n",
        "   print(f\"\\n--- Visualizing Layer {layer_index} ---\")\n",
        "\n",
        "   # Extract data from the specified layer\n",
        "   try:\n",
        "      layer_to_inspect = model_viz.layers[layer_index].some_layer\n",
        "      keys = layer_to_inspect.key_store.detach().cpu().numpy()\n",
        "      usage = layer_to_inspect.usage_count.detach().cpu().numpy()\n",
        "   except IndexError:\n",
        "      print(f\"Error: Layer index {layer_index} is out of bounds for a model with {NUM_LAYERS}\n",
        "layers.\")\n",
        "      return\n",
        "\n",
        "    print(f\"Extracted {keys.shape[0]} keys. Total usage: {int(usage.sum())}\")\n",
        "\n",
        "  # Handle the case where a layer might have zero usage if training was very short\n",
        "  if usage.sum() == 0:\n",
        "      print(f\"Warning: Layer {layer_index} has zero expert usage. Cannot normalize size or color.\n",
        "Plotting uniformly.\")\n",
        "      usage_normalized = np.zeros_like(usage)\n",
        "  else:\n",
        "      usage_normalized = usage / usage.sum()\n",
        "\n",
        "   # Perform t-SNE dimensionality reduction\n",
        "   print(f\"Running t-SNE with perplexity={perplexity}... (this may take a moment)\")\n",
        "   tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, n_iter=n_iter,\n",
        "init='pca', learning_rate='auto')\n",
        "   keys_2d = tsne.fit_transform(keys)\n",
        "   print(\"t-SNE complete.\")\n",
        "\n",
        "  # Create the plot\n",
        "  fig, ax = plt.subplots(figsize=(16, 12))\n",
        "  scatter = ax.scatter(\n",
        "     keys_2d[:, 0],\n",
        "     keys_2d[:, 1],\n",
        "     c=usage,\n",
        "     s=20 + usage_normalized * 10000, # Increased multiplier for better size variation\n",
        "     cmap='viridis',\n",
        "     alpha=0.8,\n",
        "     edgecolor='k',\n",
        "     linewidth=0.5\n",
        "  )\n",
        "\n",
        "  cbar = fig.colorbar(scatter, ax=ax, pad=0.01)\n",
        "  cbar.set_label('Expert Activation Frequency (Usage Count)', fontsize=14)\n",
        "\n",
        "  ax.set_title(f't-SNE Visualization of SoME Expert Key Space for Layer {layer_index}',\n",
        "fontsize=18, pad=20)\n",
        "  ax.set_xlabel('t-SNE Dimension 1', fontsize=14)\n",
        "  ax.set_ylabel('t-SNE Dimension 2', fontsize=14)\n",
        "\n",
        "  info_text = (\n",
        "     \"How to Read This Plot:\\n\"\n",
        "     \"\u2022 Each circle represents one of the {num_experts} experts.\\n\"\n",
        "     \"\u2022 Proximity suggests conceptual similarity learned by the router.\\n\"\n",
        "     \"\u2022 Bright, large circles are high-usage 'generalist' experts.\\n\"\n",
        "     \"\u2022 Dark, small circles are low-usage 'specialist' or unused experts.\\n\"\n",
        "     \"Clusters of points are emerging 'Knowledge Galaxies'.\"\n",
        "  ).format(num_experts=keys.shape[0])\n",
        "\n",
        "  ax.text(0.98, 0.02, info_text, transform=ax.transAxes, fontsize=12,\n",
        "       verticalalignment='bottom', horizontalalignment='right',\n",
        "       bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.8))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f\"expert_galaxy_layer_{layer_index}.png\", dpi=300)\n",
        "\n",
        "  print(f\"Visualization for layer {layer_index} saved as 'expert_galaxy_layer_{layer_index}.png'\")\n",
        "  plt.show()\n",
        "\n",
        "# --- Step 4: Generate plots for key layers ---\n",
        "# We visualize the first, middle, and last layers to see how organization evolves.\n",
        "layers_to_plot = [0, NUM_LAYERS // 2, NUM_LAYERS - 1]\n",
        "for layer_idx in layers_to_plot:\n",
        "   plot_galaxy(layer_index=layer_idx)\n",
        "____________________________________\n",
        "--- Re-instantiating model for visualization ---\n",
        "--- Loading weights from best_some_transformer_a100_safe.pth ---\n",
        "Successfully loaded trained model weights.\n",
        "\n",
        "--- Visualizing Layer 0 ---\n",
        "Extracted 256 keys. Total usage: 1638400128\n",
        "Running t-SNE with perplexity=30... (this may take a moment)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter'\n",
        "was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
        " warnings.warn(\n",
        "\n",
        "t-SNE complete.\n",
        "Visualization for layer 0 saved as 'expert_galaxy_layer_0.png'\n",
        "\n",
        "--- Visualizing Layer 4 ---\n",
        "Extracted 256 keys. Total usage: 1638399488\n",
        "Running t-SNE with perplexity=30... (this may take a moment)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter'\n",
        "was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
        " warnings.warn(\n",
        "\n",
        "t-SNE complete.\n",
        "Visualization for layer 4 saved as 'expert_galaxy_layer_4.png'\n",
        "\n",
        "--- Visualizing Layer 7 ---\n",
        "Extracted 256 keys. Total usage: 1638399488\n",
        "Running t-SNE with perplexity=30... (this may take a moment)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter'\n",
        "was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
        " warnings.warn(\n",
        "\n",
        "t-SNE complete.\n",
        "Visualization for layer 7 saved as 'expert_galaxy_layer_7.png'\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}