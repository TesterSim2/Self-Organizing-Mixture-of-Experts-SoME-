{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SoME Colab Code v2.1\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "print(\"--- Part 1: Setup and Dependencies ---\")\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs for a free performance boost\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Enable benchmark mode for cuDNN\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Model Definition, Training, and Evaluation\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "# Part 2: Data Preparation & Configuration\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n--- Part 2: Data Preparation & Configuration ---\")\n",
        "\n",
        "# -- SoME V2.1 \"Honest Speedrun\" Configuration --\n",
        "SEQ_LEN = 512\n",
        "BATCH_SIZE = 256 # Increased for A100 throughput\n",
        "VOCAB_SIZE = 8192\n",
        "train_subset_size = 20000 # Reduced for speed\n",
        "val_subset_size = 4000 # Reduced for speed\n",
        "\n",
        "# -- Loading/Training Custom Tokenizer ---\n",
        "tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "if not os.path.exists(tokenizer_path):\n",
        "    print(\"Training custom tokenizer...\")\n",
        "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "    def get_training_corpus():\n",
        "       for i in range(0, len(dataset), 1000):\n",
        "          yield dataset[i : i + 1000][\"text\"]\n",
        "    tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer_model.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"], vocab_size=VOCAB_SIZE)\n",
        "    tokenizer_model.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "    tokenizer_model.save(tokenizer_path)\n",
        "else:\n",
        "    print(\"Tokenizer already exists. Loading from file.\")\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# --- Tokenizing Dataset ---\n",
        "print(\"\\nTokenizing dataset...\")\n",
        "full_dataset = load_dataset(\"roneneldan/TinyStories\", streaming=False) # Use non-streaming\n",
        "for select\n",
        "\n",
        "train_subset = full_dataset['train'].select(range(train_subset_size))\n",
        "val_subset = full_dataset['validation'].select(range(val_subset_size))\n",
        "\n",
        "# V2 FIX: EOS TOKEN HANDLING\n",
        "def tokenize_function(examples):\n",
        "  text_with_eos = [s + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "  return tokenizer(\n",
        "     text_with_eos,\n",
        "     truncation=True,\n",
        "     padding=\"max_length\",\n",
        "     max_length=SEQ_LEN,\n",
        "     return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "tokenized_train = train_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "tokenized_val = val_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data):\n",
        "     self.data = tokenized_data\n",
        "   def __len__(self):\n",
        "     return len(self.data)\n",
        "   def __getitem__(self, idx):\n",
        "     item = self.data[idx]\n",
        "     inputs = torch.tensor(item['input_ids'])\n",
        "     # V2.2 FIX: Correct Target Creation\n",
        "     targets = inputs.clone()\n",
        "     targets[:-1] = inputs[1:]\n",
        "     targets[-1] = -100 # Ignore loss for the last token prediction\n",
        "     return inputs, targets\n",
        "\n",
        "train_dataset = LanguageModelDataset(tokenized_train)\n",
        "validation_dataset = LanguageModelDataset(tokenized_val)\n",
        "\n",
        "CPU_COUNT = os.cpu_count()\n",
        "NUM_WORKERS = max(2, CPU_COUNT // 2 if CPU_COUNT else 2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, drop_last=True,\n",
        "num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train dataset size (subset): {len(train_dataset)}\")\n",
        "\n",
        "print(f\"Using {NUM_WORKERS} workers for DataLoader.\")\n",
        "\n",
        "\n",
        "# Part 3: Model Definition (Optimized & Fixed)\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n--- Part 3: Model Definition ---\")\n",
        "\n",
        "class Expert(nn.Module):\n",
        "   \"\"\"An expert module with configurable random weight initialization.\"\"\"\n",
        "   def __init__(self, d_model, d_ffn, init_method='default'):\n",
        "      super().__init__()\n",
        "      self.w_down = nn.Linear(d_model, d_ffn)\n",
        "      self.activation = nn.GELU()\n",
        "      self.w_up = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "      # === NEW: STRUCTURED INITIALIZATION LOGIC ===\n",
        "      if init_method == 'orthogonal':\n",
        "          nn.init.orthogonal_(self.w_down.weight)\n",
        "          nn.init.orthogonal_(self.w_up.weight)\n",
        "      elif init_method == 'sparse':\n",
        "          nn.init.sparse_(self.w_down.weight, sparsity=0.5)\n",
        "          nn.init.sparse_(self.w_up.weight, sparsity=0.5)\n",
        "      elif init_method != 'default':\n",
        "          raise ValueError(f\"Unknown initialization method: {init_method}\")\n",
        "\n",
        "      nn.init.zeros_(self.w_down.bias)\n",
        "      nn.init.zeros_(self.w_up.bias)\n",
        "\n",
        "   def forward(self, x):\n",
        "     return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, num_experts, d_ffn, top_k, alpha, beta, delta, theta_percentile,\n",
        "warmup_steps, ema_decay, init_method='default'):\n",
        "     super().__init__()\n",
        "     self.d_model, self.num_experts, self.d_ffn, self.top_k = d_model, num_experts, d_ffn, top_k\n",
        "     self.alpha, self.beta, self.delta = alpha, beta, delta\n",
        "     self.theta_percentile = theta_percentile\n",
        "     self.warmup_steps = warmup_steps\n",
        "     self.ema_decay = ema_decay\n",
        "     self.query_network = nn.Linear(d_model, d_model)\n",
        "     keys = torch.randn(num_experts, d_model)\n",
        "     self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "     self.register_buffer(\"usage_count\", torch.zeros(num_experts))\n",
        "\n",
        "        self.register_buffer(\"steps\", torch.tensor([0], dtype=torch.long))\n",
        "\n",
        "    # === MODIFIED: Pass init_method to each Expert ===\n",
        "    self.experts = nn.ModuleList([Expert(d_model, d_ffn, init_method=init_method) for _ in\n",
        "range(num_experts)])\n",
        "\n",
        "        for expert in self.experts:\n",
        "            for param in expert.parameters():\n",
        "               param.requires_grad = False\n",
        "        if self.top_k > 1:\n",
        "            self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "\n",
        "   def forward(self, x, temperature=1.0):\n",
        "     batch_size, seq_len, _ = x.shape\n",
        "     x_flat = x.view(-1, self.d_model)\n",
        "     queries_raw = self.query_network(x_flat)\n",
        "     queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "     scores = torch.matmul(queries, self.key_store.t())\n",
        "     top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "     gating_weights = F.softmax(top_k_scores / temperature, dim=-1)\n",
        "     flat_top_k_indices = top_k_indices.view(-1)\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "     flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "     permuted_inputs = flat_inputs[permutation_map]\n",
        "     split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "     output_chunks = []\n",
        "     for i, expert_id in enumerate(unique_expert_ids):\n",
        "        output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "     concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "     inverse_permutation_map = torch.argsort(permutation_map)\n",
        "     expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "     weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "gating_weights.unsqueeze(-1)).sum(dim=1)\n",
        "     final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "     return x + final_output, queries, top_k_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_keys(self, queries, top_k_indices):\n",
        "    self.steps += 1\n",
        "    unique_indices, counts = torch.unique(top_k_indices, return_counts=True)\n",
        "    self.usage_count.mul_(self.ema_decay)\n",
        "\n",
        "    self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts.float())\n",
        "\n",
        "    for i in range(self.top_k):\n",
        "       indices = top_k_indices[:, i]\n",
        "       inertia = 1.0 + self.usage_count[indices]\n",
        "       alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "       update_vec = queries - self.key_store[indices]\n",
        "       self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "    if self.top_k > 1:\n",
        "        indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "        indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "        keys_i, keys_j = self.key_store[indices_i], self.key_store[indices_j]\n",
        "        inertia_i = (1.0 + self.usage_count[indices_i]).unsqueeze(-1)\n",
        "        inertia_j = (1.0 + self.usage_count[indices_j]).unsqueeze(-1)\n",
        "        beta_effective = self.beta / torch.min(inertia_i, inertia_j)\n",
        "        update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "        update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "        self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "        self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "    self.key_store.data = F.normalize(self.key_store.data, p=2, dim=-1)\n",
        "\n",
        "    if self.steps > self.warmup_steps:\n",
        "        active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "        if active_usage_counts.numel() > 0:\n",
        "            dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "            low_usage_mask = self.usage_count < dynamic_theta\n",
        "            self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_layer):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = some_layer\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    seq_len = x.size(1)\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device) * float('-inf'), diagonal=1)\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "    x = self.norm1(x + attn_output)\n",
        "    some_output, queries, top_k_indices = self.some_layer(x, temperature=temperature)\n",
        "\n",
        "      x = self.norm2(some_output)\n",
        "      return x, queries, top_k_indices\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "     super().__init__()\n",
        "     position = torch.arange(max_len).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "     pe = torch.zeros(max_len, 1, d_model)\n",
        "     pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "     pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "     self.register_buffer('pe', pe.transpose(0, 1))\n",
        "   def forward(self, x):\n",
        "     return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, vocab_size, d_model, num_heads, num_layers, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "     self.pos_encoder = PositionalEncoding(d_model, max_len=SEQ_LEN)\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(d_model, num_heads, SOMELayer(d_model=d_model,\n",
        "**some_config))\n",
        "        for _ in range(num_layers)\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "   def forward(self, x, temperature=1.0):\n",
        "     x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
        "     x = self.pos_encoder(x)\n",
        "     all_queries, all_indices = [], []\n",
        "     for layer in self.layers:\n",
        "        x, queries, top_k_indices = layer(x, temperature=temperature)\n",
        "        all_queries.append(queries)\n",
        "        all_indices.append(top_k_indices)\n",
        "     return self.fc_out(x), all_queries, all_indices\n",
        "   @torch.no_grad()\n",
        "   def update_all_keys(self, all_queries, all_indices):\n",
        "     for i, layer_block in enumerate(self.layers):\n",
        "        queries = all_queries[i].view(-1, layer_block.some_layer.d_model)\n",
        "        indices = all_indices[i].view(-1, layer_block.some_layer.top_k)\n",
        "        layer_block.some_layer.update_keys(queries, indices)\n",
        "\n",
        "# Part 4: Training, Evaluation, and Metrics\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Part 4: Training, Evaluation, and Metrics ---\")\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0: return 0.0\n",
        "  probs = usage_counts / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "  for inputs, targets in progress_bar:\n",
        "      inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "      with torch.amp.autocast(\"cuda\"):\n",
        "         logits, queries, indices = model(inputs, temperature=current_temp)\n",
        "         loss = criterion(logits.view(-1, tokenizer.vocab_size), targets.view(-1))\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.unscale_(optimizer)\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      scheduler.step()\n",
        "      model.update_all_keys(queries, indices)\n",
        "      total_loss += loss.item()\n",
        "      progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "           logits, _, _ = model(inputs, temperature=0.5) # Sharpen during eval\n",
        "           loss = criterion(logits.view(-1, tokenizer.vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, epochs, init_method):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  plt.title(f'Training and Validation Loss (V2.1 Speedrun - {init_method.capitalize()} Init)')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "  filename = f'loss_curve_v2.1_speedrun_{init_method}.png'\n",
        "  plt.savefig(filename)\n",
        "  print(f\"\\nLoss curve plot saved to {filename}\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Part 5: Main Execution Block\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n--- Part 5: Main Execution Block ---\")\n",
        "\n",
        "# -- Model Dimensions --\n",
        "D_MODEL = 384\n",
        "NUM_HEADS = 6\n",
        "NUM_LAYERS = 6\n",
        "\n",
        "# -- SoME V2.1 Hyperparameters --\n",
        "some_config = {\n",
        "   \"num_experts\": 128,\n",
        "   \"d_ffn\": 1024,\n",
        "   \"top_k\": 4,\n",
        "   \"alpha\": 0.015,\n",
        "   \"beta\": 0.001,\n",
        "   \"delta\": 0.001,\n",
        "\n",
        "    \"theta_percentile\": 0.05,\n",
        "    \"warmup_steps\": 400,\n",
        "    \"ema_decay\": 0.995,\n",
        "    # === CONTROL YOUR EXPERIMENT HERE ===\n",
        "    # Options: 'default', 'orthogonal', 'sparse'\n",
        "    \"init_method\": \"orthogonal\"\n",
        "}\n",
        "\n",
        "# -- Training Schedule --\n",
        "NUM_EPOCHS = 6\n",
        "LEARNING_RATE = 8e-4\n",
        "TRAINING_TEMP = 1.0\n",
        "\n",
        "# -- Initialization --\n",
        "model_save_path = f\"best_some_transformer_v2.1_{some_config['init_method']}.pth\"\n",
        "model = SOMETransformer(\n",
        "   vocab_size=tokenizer.vocab_size, d_model=D_MODEL, num_heads=NUM_HEADS,\n",
        "   num_layers=NUM_LAYERS, some_config=some_config\n",
        ").to(device)\n",
        "\n",
        "if hasattr(torch, 'compile'):\n",
        "    print(\"\\nCompiling the model for faster training...\")\n",
        "    model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "                    lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if\n",
        "p.requires_grad)/1e6:.2f}M\")\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "print(f\"Using expert initialization method: {some_config['init_method']}\")\n",
        "\n",
        "\n",
        "# -- Run Training and Metric Tracking --\n",
        "train_losses, val_losses = [], []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "   print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "  train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler,\n",
        "current_temp=TRAINING_TEMP)\n",
        "  val_loss = evaluate_epoch(model, validation_loader, criterion)\n",
        "  perplexity = math.exp(val_loss)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  model_to_inspect = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "  usage_counts = model_to_inspect.layers[NUM_LAYERS // 2].some_layer.usage_count\n",
        "  gini_coeff = calculate_gini(usage_counts)\n",
        "  entropy_val = calculate_entropy(usage_counts)\n",
        "\n",
        "   print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Perplexity\n",
        "= {perplexity:.2f}\")\n",
        "   print(f\" Middle Layer Expert Metrics: Gini = {gini_coeff:.3f}, Entropy = {entropy_val:.3f}\")\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save(model_to_inspect.state_dict(), model_save_path)\n",
        "      print(f\"Model saved as {model_save_path}\")\n",
        "\n",
        "print(f\"\\n--- V2.1 'Honest Speedrun' Training Complete ---\")\n",
        "plot_losses(train_losses, val_losses, NUM_EPOCHS, some_config['init_method'])\n",
        "____________________________________\n",
        "--- Part 2: Data Preparation & Configuration ---\n",
        "Training custom tokenizer...\n",
        "\n",
        "README.md:\n",
        " 1.06k/? [00:00<00:00, 114kB/s]\n",
        "data/train-00000-of-00004-2d5a1467fff108(\u2026): 100%\n",
        " 249M/249M [00:02<00:00, 251MB/s]\n",
        "data/train-00001-of-00004-5852b56a2bd28f(\u2026): 100%\n",
        " 248M/248M [00:01<00:00, 310MB/s]\n",
        "data/train-00002-of-00004-a26307300439e9(\u2026): 100%\n",
        " 246M/246M [00:01<00:00, 132MB/s]\n",
        "data/train-00003-of-00004-d243063613e5a0(\u2026): 100%\n",
        " 248M/248M [00:01<00:00, 122MB/s]\n",
        "data/validation-00000-of-00001-869c898b5(\u2026): 100%\n",
        " 9.99M/9.99M [00:00<00:00, 20.8MB/s]\n",
        "Generating train split: 100%\n",
        " 2119719/2119719 [00:06<00:00, 339467.91 examples/s]\n",
        "Generating validation split: 100%\n",
        " 21990/21990 [00:00<00:00, 316045.23 examples/s]\n",
        "\n",
        "Custom tokenizer loaded with vocab size: 8192\n",
        "\n",
        "Tokenizing dataset...\n",
        "\n",
        "Map (num_proc=12): 100%\n",
        " 20000/20000 [00:03<00:00, 6167.38 examples/s]\n",
        "Map (num_proc=12): 100%\n",
        " 4000/4000 [00:00<00:00, 434.57 examples/s]\n",
        "Train dataset size (subset): 20000\n",
        "Using 6 workers for DataLoader.\n",
        "\n",
        "--- Part 3: Model Definition ---\n",
        "\n",
        "--- Part 4: Training, Evaluation, and Metrics ---\n",
        "\n",
        "--- Part 5: Main Execution Block ---\n",
        "\n",
        "Compiling the model for faster training...\n",
        "\n",
        "Total parameters: 615.81M\n",
        "Trainable parameters: 10.74M\n",
        "Total training steps: 468\n",
        "Using expert initialization method: orthogonal\n",
        "\n",
        "--- Epoch 1/6 ---\n",
        "\n",
        "Training: 0%|           | 0/78 [00:00<?,\n",
        "?it/s]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning:\n",
        "Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you\n",
        "should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`. Failure to\n",
        "do this will result in PyTorch skipping the first value of the learning rate schedule. See more\n",
        "details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        " warnings.warn(\n",
        "\n",
        "Epoch 1: Train Loss = 2.3959, Val Loss = 1.5460, Val Perplexity = 4.69\n",
        " Middle Layer Expert Metrics: Gini = 0.755, Entropy = 4.770\n",
        "Model saved as best_some_transformer_v2.1_orthogonal.pth\n",
        "\n",
        "--- Epoch 2/6 ---\n",
        "\n",
        "Epoch 2: Train Loss = 1.5637, Val Loss = 1.3744, Val Perplexity = 3.95\n",
        " Middle Layer Expert Metrics: Gini = 0.750, Entropy = 4.748\n",
        "Model saved as best_some_transformer_v2.1_orthogonal.pth\n",
        "\n",
        "--- Epoch 3/6 ---\n",
        "\n",
        "Epoch 3: Train Loss = 1.4259, Val Loss = 1.2912, Val Perplexity = 3.64\n",
        " Middle Layer Expert Metrics: Gini = 0.750, Entropy = 4.736\n",
        "Model saved as best_some_transformer_v2.1_orthogonal.pth\n",
        "\n",
        "--- Epoch 4/6 ---\n",
        "\n",
        "Epoch 4: Train Loss = 1.3462, Val Loss = 1.2458, Val Perplexity = 3.48\n",
        " Middle Layer Expert Metrics: Gini = 0.755, Entropy = 4.716\n",
        "Model saved as best_some_transformer_v2.1_orthogonal.pth\n",
        "\n",
        "--- Epoch 5/6 ---\n",
        "\n",
        "Epoch 5: Train Loss = 1.3038, Val Loss = 1.2266, Val Perplexity = 3.41\n",
        " Middle Layer Expert Metrics: Gini = 0.760, Entropy = 4.700\n",
        "Model saved as best_some_transformer_v2.1_orthogonal.pth\n",
        "\n",
        "--- Epoch 6/6 ---\n",
        "\n",
        "Epoch 6: Train Loss = 1.2878, Val Loss = 1.2241, Val Perplexity = 3.40\n",
        " Middle Layer Expert Metrics: Gini = 0.763, Entropy = 4.690\n",
        "Model saved as best_some_transformer_v2.1_orthogonal.pth\n",
        "\n",
        "--- V2.1 'Honest Speedrun' Training Complete ---\n",
        "\n",
        "Loss curve plot saved to loss_curve_v2.1_speedrun_orthogonal.png\n",
        "\n",
        "____________________________________\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Interactive Inference\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "print(\"\\n--- Part 6: Interactive Inference ---\")\n",
        "\n",
        "def generate(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, top_k=50,\n",
        "device=\"cuda\"):\n",
        "  \"\"\"Generates text from a trained SoME model using temperature and top-k sampling.\"\"\"\n",
        "  model.eval()\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "  generated_ids = input_ids\n",
        "\n",
        "  print(f\"\\n--- Prompt ---\\n{prompt}\", end=\"\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for _ in range(max_new_tokens):\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "           outputs, _, _ = model(generated_ids)\n",
        "\n",
        "        next_token_logits = outputs[:, -1, :]\n",
        "\n",
        "        # Apply temperature\n",
        "        if temperature > 0:\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
        "        probs = F.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "        # Sample the next token\n",
        "        next_token_relative_id = torch.multinomial(probs, num_samples=1)\n",
        "        next_token_id = torch.gather(top_k_indices, -1, next_token_relative_id)\n",
        "\n",
        "        if next_token_id.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
        "\n",
        "        new_token = tokenizer.decode(next_token_id[0])\n",
        "        print(new_token, end=\"\", flush=True)\n",
        "\n",
        "  print(\"\\n--- End of Generation ---\")\n",
        "\n",
        "\n",
        "# --- Setup Inference Model ---\n",
        "print(\"\\nSetting up model for inference...\")\n",
        "\n",
        "# Use the same configuration as the training cell\n",
        "inference_config = some_config\n",
        "model_path_to_load = model_save_path\n",
        "\n",
        "if os.path.exists(model_path_to_load):\n",
        "    inference_model = SOMETransformer(\n",
        "       vocab_size=tokenizer.vocab_size, d_model=D_MODEL, num_heads=NUM_HEADS,\n",
        "       num_layers=NUM_LAYERS, some_config=inference_config\n",
        "    ).to(device)\n",
        "\n",
        "  inference_model.load_state_dict(torch.load(model_path_to_load))\n",
        "  print(f\"Loaded weights from {model_path_to_load}\")\n",
        "\n",
        "  # Note: torch.compile can sometimes have issues with dynamic shapes in generation\n",
        "  # It's often safer to run inference without it unless tested thoroughly.\n",
        "\n",
        "  # --- Talk to the Model! ---\n",
        "  prompts = [\n",
        "     \"Once upon a time, there was a brave knight who\",\n",
        "     \"The secret to making the best pizza is\",\n",
        "     \"A lonely robot sat on a hill watching the stars. It wondered,\"\n",
        "  ]\n",
        "\n",
        "    for p in prompts:\n",
        "       generate(inference_model, tokenizer, p, max_new_tokens=80, temperature=0.8, top_k=50)\n",
        "       print(\"-\" * 30)\n",
        "else:\n",
        "    print(f\"Could not find a saved model at '{model_path_to_load}'. Please run the training cell\n",
        "first.\")\n",
        "____________________________________\n",
        "\n",
        "--- Part 6: Interactive Inference ---\n",
        "\n",
        "Setting up model for inference...\n",
        "Loaded weights from best_some_transformer_v2.1_orthogonal.pth\n",
        "\n",
        "--- Prompt ---\n",
        "Once upon a time, there was a brave knight\n",
        "whowasverynervous.Hewantedtogototheworldaroundthemarket.Helookedforabig,it,buthecouldn't\n",
        "findtodo.Sohesawthatit'snotlookingforthebest.Hedidn'tknowwhattodo.Hetriedtohelpbeforeitupand\n",
        "makeitveryspecial.Suddenly,aholewasakindofotherchildren.Hewas\n",
        "--- End of Generation ---\n",
        "------------------------------\n",
        "\n",
        "--- Prompt ---\n",
        "The secret to making the best pizza\n",
        "isthat.\"Iwanttobuyitcan.\"So,theyheardabigcar.Thetruckwasscared.Hetoldittohismum.Thecarwast\n",
        "hecar.Hewassohewantedtogetitahat.Itwasabigandbrightnewfriend.Thecarwashappyandhappy.Th\n",
        "eyworkedhardandplayed.Theyallthecarandtheanimals.Itwasthe\n",
        "--- End of Generation ---\n",
        "------------------------------\n",
        "\n",
        "--- Prompt ---\n",
        "A lonely robot sat on a hill watching the stars. It\n",
        "wondered,andalittlegirlnamed\"Thatboy!\"Theboywasverycurious.\"Whatcanweplayinthegarden?\"\"\n",
        "CanIhaveaplan?\"Hismomlookedforabig,butshewantedtoseetheboy.\"Idon'tworry,sweetheart.Iwant\n",
        "topickourthingsinthewoods.Maybeoneday,alittlegirlnamedSue.Shehadalotof\n",
        "\n",
        "--- End of Generation ---\n",
        "------------------------------\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}