{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n",
        "========================================================================\n",
        "============\n",
        "# SoME Transformer V5.2 - Final V2 Baseline with All Phase 1 Fixes\n",
        "# Implements:\n",
        "# 1. DEFINITIVE CAUSAL MASKING: Explicitly creates and registers a causal mask.\n",
        "# 2. CORRECT TARGET CREATION: Standard next-token prediction target shifting. (V2.2\n",
        "FIX)\n",
        "# 3. CORRECT LR SCHEDULER ORDER: The scheduler is now called after the optimizer.\n",
        "(V2.2 FIX)\n",
        "# 4. All other V2 Baseline improvements (EOS handling, Cosine Routing, EMA Inertia).\n",
        "#\n",
        "========================================================================\n",
        "============\n",
        "\n",
        "# Part 1: Setup and Dependencies\n",
        "# ===============================\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs for a free performance boost\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Part 2: Data Preparation (Fixed)\n",
        "# ========================================\n",
        "SEQ_LEN = 512\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = 8192\n",
        "\n",
        "print(\"--- Loading/Training Custom Tokenizer ---\")\n",
        "tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "if not os.path.exists(tokenizer_path):\n",
        "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "    def get_training_corpus():\n",
        "       for i in range(0, len(dataset), 1000):\n",
        "          yield dataset[i : i + 1000][\"text\"]\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"], vocab_size=VOCAB_SIZE)\n",
        "    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "    tokenizer.save(tokenizer_path)\n",
        "else:\n",
        "    print(\"Tokenizer already exists. Loading from file.\")\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "print(\"\\n--- Tokenizing Dataset ---\")\n",
        "full_dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "train_subset_size = 40000\n",
        "val_subset_size = 10000\n",
        "train_subset = full_dataset['train'].select(range(train_subset_size))\n",
        "val_subset = full_dataset['validation'].select(range(val_subset_size))\n",
        "\n",
        "## V2 FIX: EOS TOKEN HANDLING\n",
        "def tokenize_function(examples):\n",
        "  text_with_eos = [s + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "  return tokenizer(\n",
        "     text_with_eos,\n",
        "     truncation=True,\n",
        "     padding=\"max_length\",\n",
        "\n",
        "      max_length=SEQ_LEN,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "tokenized_train = train_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "tokenized_val = val_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data):\n",
        "     self.data = tokenized_data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    inputs = torch.tensor(item['input_ids'])\n",
        "\n",
        "      ## V2.2 FIX: Correct Target Creation\n",
        "      # The target for token at position `i` should be the token at position `i+1`.\n",
        "      targets = inputs.clone()\n",
        "      targets[:-1] = inputs[1:]\n",
        "      targets[-1] = -100 # Ignore the loss for the last token prediction.\n",
        "      return inputs, targets\n",
        "\n",
        "train_dataset = LanguageModelDataset(tokenized_train)\n",
        "validation_dataset = LanguageModelDataset(tokenized_val)\n",
        "\n",
        "CPU_COUNT = os.cpu_count()\n",
        "NUM_WORKERS = max(2, CPU_COUNT // 2 if CPU_COUNT else 2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, drop_last=True,\n",
        "num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train dataset size (subset): {len(train_dataset)}\")\n",
        "print(f\"Using {NUM_WORKERS} workers for DataLoader.\")\n",
        "\n",
        "\n",
        "# Part 3: Model Definition (Optimized & Fixed)\n",
        "# =============================================\n",
        "class Expert(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.w_down = nn.Linear(d_model, d_ffn)\n",
        "    self.activation = nn.GELU()\n",
        "    self.w_up = nn.Linear(d_ffn, d_model)\n",
        "  def forward(self, x):\n",
        "    return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, num_experts, d_ffn, top_k, alpha=0.01, beta=0.001, delta=0.001,\n",
        "theta_percentile=0.05, warmup_steps=1000, ema_decay=0.99):\n",
        "      super().__init__()\n",
        "      self.d_model, self.num_experts, self.d_ffn, self.top_k = d_model, num_experts, d_ffn, top_k\n",
        "      self.alpha, self.beta, self.delta = alpha, beta, delta\n",
        "      self.theta_percentile = theta_percentile\n",
        "      self.warmup_steps = warmup_steps\n",
        "      self.query_network = nn.Linear(d_model, d_model)\n",
        "      keys = torch.randn(num_experts, d_model)\n",
        "      self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "      self.register_buffer(\"usage_count\", torch.zeros(num_experts))\n",
        "      self.register_buffer(\"steps\", torch.tensor([0], dtype=torch.long))\n",
        "      self.experts = nn.ModuleList([Expert(d_model, d_ffn) for _ in range(num_experts)])\n",
        "      for expert in self.experts:\n",
        "          for param in expert.parameters():\n",
        "             param.requires_grad = False\n",
        "      if self.top_k > 1:\n",
        "          self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "      self.ema_decay = ema_decay\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "    queries_raw = self.query_network(x_flat)\n",
        "    queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "\n",
        "     scores = torch.matmul(queries, self.key_store.t())\n",
        "     top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "     gating_weights = F.softmax(top_k_scores / temperature, dim=-1)\n",
        "\n",
        "     flat_top_k_indices = top_k_indices.view(-1)\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "    flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "    permuted_inputs = flat_inputs[permutation_map]\n",
        "    split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "    output_chunks = []\n",
        "    for i, expert_id in enumerate(unique_expert_ids):\n",
        "       output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "    concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "    inverse_permutation_map = torch.argsort(permutation_map)\n",
        "    expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "     weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "gating_weights.unsqueeze(-1)).sum(dim=1)\n",
        "     final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    return x + final_output, queries, top_k_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_keys(self, queries, top_k_indices):\n",
        "    self.steps += 1\n",
        "    unique_indices, counts = torch.unique(top_k_indices, return_counts=True)\n",
        "    self.usage_count.mul_(self.ema_decay)\n",
        "    self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts.float())\n",
        "\n",
        "    for i in range(self.top_k):\n",
        "       indices = top_k_indices[:, i]\n",
        "       inertia = 1.0 + self.usage_count[indices]\n",
        "       alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "       update_vec = queries - self.key_store[indices]\n",
        "       self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "    if self.top_k > 1:\n",
        "        indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "        indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "        keys_i = self.key_store[indices_i]\n",
        "        keys_j = self.key_store[indices_j]\n",
        "        inertia_i = (1.0 + self.usage_count[indices_i]).unsqueeze(-1)\n",
        "        inertia_j = (1.0 + self.usage_count[indices_j]).unsqueeze(-1)\n",
        "        beta_effective = self.beta / torch.min(inertia_i, inertia_j)\n",
        "        update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "        update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "        self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "\n",
        "       self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "     self.key_store.data = F.normalize(self.key_store.data, p=2, dim=-1)\n",
        "\n",
        "     if self.steps > self.warmup_steps:\n",
        "         active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "         if active_usage_counts.numel() > 0:\n",
        "             dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "             low_usage_mask = self.usage_count < dynamic_theta\n",
        "             self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_layer):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = some_layer\n",
        "     ## V2.1 FIX: Create and register a permanent causal mask.\n",
        "     mask = torch.triu(torch.ones(SEQ_LEN, SEQ_LEN) * float('-inf'), diagonal=1)\n",
        "     self.register_buffer('mask', mask)\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    ## V2.1 FIX: Pass the registered causal mask to the attention call.\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=self.mask)\n",
        "\n",
        "     x = self.norm1(x + attn_output)\n",
        "     some_output, queries, top_k_indices = self.some_layer(x, temperature=temperature)\n",
        "     x = self.norm2(some_output)\n",
        "     return x, queries, top_k_indices\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "     super().__init__()\n",
        "     pe = torch.zeros(max_len, d_model)\n",
        "     position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "     pe[:, 0::2] = torch.sin(position * div_term)\n",
        "     pe[:, 1::2] = torch.cos(position * div_term)\n",
        "     pe = pe.unsqueeze(0)\n",
        "     self.register_buffer('pe', pe)\n",
        "   def forward(self, x):\n",
        "     return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, vocab_size, d_model, num_heads, num_layers, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "     self.pos_encoder = PositionalEncoding(d_model, max_len=SEQ_LEN)\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(d_model, num_heads, SOMELayer(d_model=d_model,\n",
        "**some_config))\n",
        "        for _ in range(num_layers)\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
        "    x = self.pos_encoder(x)\n",
        "    all_queries, all_indices = [], []\n",
        "    for layer in self.layers:\n",
        "       x, queries, top_k_indices = layer(x, temperature=temperature)\n",
        "       all_queries.append(queries)\n",
        "       all_indices.append(top_k_indices)\n",
        "    return self.fc_out(x), all_queries, all_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices):\n",
        "    for i, layer_block in enumerate(self.layers):\n",
        "       queries = all_queries[i].view(-1, layer_block.some_layer.d_model)\n",
        "       indices = all_indices[i].view(-1, layer_block.some_layer.top_k)\n",
        "       layer_block.some_layer.update_keys(queries, indices)\n",
        "\n",
        "# Part 4: Training, Evaluation, and Metrics\n",
        "# ==========================================================\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0: return 0.0\n",
        "\n",
        "  probs = usage_counts / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "  for inputs, targets in progress_bar:\n",
        "      inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "      with torch.amp.autocast(\"cuda\"):\n",
        "         logits, queries, indices = model(inputs, temperature=current_temp)\n",
        "         loss = criterion(logits.view(-1, tokenizer.vocab_size), targets.view(-1))\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "\n",
        "     ## V2.2 FIX: Correct LR Scheduler Order\n",
        "     scheduler.step()\n",
        "\n",
        "     model.update_all_keys(queries, indices)\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "  with torch.no_grad():\n",
        "     for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "           logits, _, _ = model(inputs, temperature=0.5)\n",
        "           loss = criterion(logits.view(-1, tokenizer.vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, epochs):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  plt.title('Training and Validation Loss (V2 Baseline)')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "  plt.savefig('loss_curve_v2_baseline.png')\n",
        "  print(\"\\nLoss curve plot saved to loss_curve_v2_baseline.png\")\n",
        "\n",
        "# Part 5: Main Execution Block (V2 Baseline Config)\n",
        "# ===================================================\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8\n",
        "some_config = {\n",
        "  \"num_experts\": 256,\n",
        "  \"d_ffn\": 1536,\n",
        "  \"top_k\": 8,\n",
        "  \"alpha\": 0.01,\n",
        "  \"beta\": 0.001,\n",
        "  \"delta\": 0.001,\n",
        "  \"theta_percentile\": 0.05,\n",
        "  \"warmup_steps\": 2000,\n",
        "  \"ema_decay\": 0.99\n",
        "}\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 6e-4\n",
        "TRAINING_TEMP = 0.8\n",
        "\n",
        "# --- Initialization ---\n",
        "model = SOMETransformer(\n",
        "   vocab_size=tokenizer.vocab_size, d_model=D_MODEL, num_heads=NUM_HEADS,\n",
        "   num_layers=NUM_LAYERS, some_config=some_config\n",
        ").to(device)\n",
        "\n",
        "if hasattr(torch, 'compile'):\n",
        "    print(\"\\nCompiling the model for faster training...\")\n",
        "    model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if\n",
        "p.requires_grad)/1e6:.2f}M\")\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "\n",
        "# --- Run Training and Metric Tracking ---\n",
        "train_losses, val_losses = [], []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "   print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "   train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler,\n",
        "current_temp=TRAINING_TEMP)\n",
        "   val_loss = evaluate_epoch(model, validation_loader, criterion)\n",
        "   perplexity = math.exp(val_loss)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  model_to_inspect = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "  usage_counts = model_to_inspect.layers[NUM_LAYERS // 2].some_layer.usage_count\n",
        "  gini_coeff = calculate_gini(usage_counts)\n",
        "  entropy_val = calculate_entropy(usage_counts)\n",
        "\n",
        "   print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Perplexity\n",
        "= {perplexity:.2f}\")\n",
        "   print(f\" \u2514\u2500 Middle Layer Expert Metrics: Gini = {gini_coeff:.3f}, Entropy = {entropy_val:.3f}\")\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save(model_to_inspect.state_dict(), \"best_some_transformer_v2_baseline.pth\")\n",
        "      print(\"Model saved as best_some_transformer_v2_baseline.pth\")\n",
        "\n",
        "print(\"\\n--- V2 Baseline Training Complete ---\")\n",
        "plot_losses(train_losses, val_losses, NUM_EPOCHS)\n",
        "\n",
        "____________________________________\n",
        "\n",
        "Using device: cuda\n",
        "A100 GPU detected. Enabling TF32.\n",
        "--- Loading/Training Custom Tokenizer ---\n",
        "Tokenizer already exists. Loading from file.\n",
        "Custom tokenizer loaded with vocab size: 8192\n",
        "\n",
        "--- Tokenizing Dataset ---\n",
        "Train dataset size (subset): 40000\n",
        "Using 6 workers for DataLoader.\n",
        "\n",
        "Compiling the model for faster training...\n",
        "\n",
        "Total parameters: 3244.34M\n",
        "Trainable parameters: 18.92M\n",
        "Total training steps: 6250\n",
        "\n",
        "--- Epoch 1/10 ---\n",
        "\n",
        "Training: 0%|           | 0/625 [00:00<?,\n",
        "?it/s]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning:\n",
        "Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you\n",
        "should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`. Failure to\n",
        "do this will result in PyTorch skipping the first value of the learning rate schedule. See more\n",
        "details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        " warnings.warn(\n",
        "\n",
        "Epoch 1: Train Loss = 1.4702, Val Loss = 1.0995, Val Perplexity = 3.00\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.740, Entropy = 5.780\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 2/10 ---\n",
        "\n",
        "Epoch 2: Train Loss = 1.0929, Val Loss = 0.9834, Val Perplexity = 2.67\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.714, Entropy = 5.841\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 3/10 ---\n",
        "\n",
        "Epoch 3: Train Loss = 0.9882, Val Loss = 0.9299, Val Perplexity = 2.53\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.706, Entropy = 5.868\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 4/10 ---\n",
        "\n",
        "Epoch 4: Train Loss = 0.9241, Val Loss = 0.8964, Val Perplexity = 2.45\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.703, Entropy = 5.889\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 5/10 ---\n",
        "\n",
        "Epoch 5: Train Loss = 0.8757, Val Loss = 0.8763, Val Perplexity = 2.40\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.699, Entropy = 5.880\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 6/10 ---\n",
        "\n",
        "Epoch 6: Train Loss = 0.8360, Val Loss = 0.8622, Val Perplexity = 2.37\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.698, Entropy = 5.887\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 7/10 ---\n",
        "\n",
        "Epoch 7: Train Loss = 0.8027, Val Loss = 0.8524, Val Perplexity = 2.35\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.697, Entropy = 5.878\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 8/10 ---\n",
        "\n",
        "Epoch 8: Train Loss = 0.7762, Val Loss = 0.8482, Val Perplexity = 2.34\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.689, Entropy = 5.907\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 9/10 ---\n",
        "\n",
        "Epoch 9: Train Loss = 0.7576, Val Loss = 0.8470, Val Perplexity = 2.33\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.690, Entropy = 5.898\n",
        "Model saved as best_some_transformer_v2_baseline.pth\n",
        "\n",
        "--- Epoch 10/10 ---\n",
        "\n",
        "Epoch 10: Train Loss = 0.7479, Val Loss = 0.8471, Val Perplexity = 2.33\n",
        " \u2514\u2500 Middle Layer Expert Metrics: Gini = 0.691, Entropy = 5.893\n",
        "\n",
        "--- V2 Baseline Training Complete ---\n",
        "\n",
        "Loss curve plot saved to loss_curve_v2_baseline.png\n",
        "\n",
        "____________________________________\n",
        "#\n",
        "========================================================================\n",
        "============\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Advanced Visualization of Expert \"Knowledge Galaxies\"\n",
        "#\n",
        "========================================================================\n",
        "============\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ensure the model class definitions from the previous cell are available\n",
        "# (SOMETransformer, SOMETransformerBlock, SOMELayer, Expert, PositionalEncoding)\n",
        "\n",
        "# --- Step 1: Re-instantiate the model with the exact trained architecture ---\n",
        "# This uses the configuration variables from the previous cell to prevent errors.\n",
        "print(\"--- Re-instantiating model for visualization ---\")\n",
        "model_viz = SOMETransformer(\n",
        "   vocab_size=tokenizer.vocab_size,\n",
        "   d_model=D_MODEL,\n",
        "   num_heads=NUM_HEADS,\n",
        "\n",
        "    num_layers=NUM_LAYERS,\n",
        "    some_config=some_config\n",
        ")\n",
        "\n",
        "# --- Step 2: Load the saved weights ---\n",
        "## V2 CHANGE: Updated model path to load the new V2 baseline weights.\n",
        "model_path = \"best_some_transformer_v2_baseline.pth\"\n",
        "print(f\"--- Loading weights from {model_path} ---\")\n",
        "try:\n",
        "   # We load onto the CPU for analysis to free up GPU VRAM\n",
        "   model_viz.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
        "   print(\"Successfully loaded trained model weights.\")\n",
        "except Exception as e:\n",
        "   print(f\"Error loading model weights: {e}\")\n",
        "   print(\"Please ensure the model file exists and the architecture matches the saved weights.\")\n",
        "\n",
        "model_viz.eval()\n",
        "\n",
        "# --- Step 3: Define a reusable plotting function ---\n",
        "def plot_galaxy(layer_index, perplexity=30, n_iter=1000):\n",
        "   \"\"\"\n",
        "   Extracts keys and usage from a specific layer, runs t-SNE, and plots the result.\n",
        "   \"\"\"\n",
        "   print(f\"\\n--- Visualizing Layer {layer_index} ---\")\n",
        "\n",
        "   # Extract data from the specified layer\n",
        "   try:\n",
        "      layer_to_inspect = model_viz.layers[layer_index].some_layer\n",
        "      keys = layer_to_inspect.key_store.detach().cpu().numpy()\n",
        "      usage = layer_to_inspect.usage_count.detach().cpu().numpy()\n",
        "   except IndexError:\n",
        "      print(f\"Error: Layer index {layer_index} is out of bounds for a model with {NUM_LAYERS}\n",
        "layers.\")\n",
        "      return\n",
        "\n",
        "    print(f\"Extracted {keys.shape[0]} keys. Total usage (EMA-scaled): {usage.sum():.2f}\")\n",
        "\n",
        "  # Handle the case where a layer might have zero usage if training was very short\n",
        "  if usage.sum() == 0:\n",
        "      print(f\"Warning: Layer {layer_index} has zero expert usage. Cannot normalize size or color.\n",
        "Plotting uniformly.\")\n",
        "      usage_normalized = np.zeros_like(usage)\n",
        "  else:\n",
        "      # We can use the raw EMA value for color and a normalized version for size\n",
        "\n",
        "     usage_normalized_size = usage / usage.sum()\n",
        "\n",
        "   # Perform t-SNE dimensionality reduction\n",
        "   print(f\"Running t-SNE with perplexity={perplexity}... (this may take a moment)\")\n",
        "   tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, n_iter=n_iter,\n",
        "init='pca', learning_rate='auto')\n",
        "   keys_2d = tsne.fit_transform(keys)\n",
        "   print(\"t-SNE complete.\")\n",
        "\n",
        "  # Create the plot\n",
        "  fig, ax = plt.subplots(figsize=(16, 12))\n",
        "  scatter = ax.scatter(\n",
        "     keys_2d[:, 0],\n",
        "     keys_2d[:, 1],\n",
        "     c=usage,\n",
        "     s=20 + usage_normalized_size * 20000, # Increased multiplier for better size variation\n",
        "     cmap='viridis',\n",
        "     alpha=0.8,\n",
        "     edgecolor='k',\n",
        "     linewidth=0.5\n",
        "  )\n",
        "\n",
        "  cbar = fig.colorbar(scatter, ax=ax, pad=0.01)\n",
        "  cbar.set_label('Expert Activation Frequency (EMA Usage)', fontsize=14)\n",
        "\n",
        "  ax.set_title(f't-SNE Visualization of SoME Expert Key Space for Layer {layer_index}',\n",
        "fontsize=18, pad=20)\n",
        "  ax.set_xlabel('t-SNE Dimension 1', fontsize=14)\n",
        "  ax.set_ylabel('t-SNE Dimension 2', fontsize=14)\n",
        "\n",
        "  info_text = (\n",
        "     \"How to Read This Plot:\\n\"\n",
        "     \"\u2022 Each circle represents one of the {num_experts} experts.\\n\"\n",
        "     \"\u2022 Proximity suggests conceptual similarity learned by the router.\\n\"\n",
        "     \"\u2022 Bright, large circles are high-usage 'generalist' experts.\\n\"\n",
        "     \"\u2022 Dark, small circles are low-usage 'specialist' or unused experts.\\n\"\n",
        "     \"Clusters of points are emerging 'Knowledge Galaxies'.\"\n",
        "  ).format(num_experts=keys.shape[0])\n",
        "\n",
        "  ax.text(0.98, 0.02, info_text, transform=ax.transAxes, fontsize=12,\n",
        "       verticalalignment='bottom', horizontalalignment='right',\n",
        "       bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.8))\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "   plt.savefig(f\"expert_galaxy_layer_{layer_index}_v2.png\", dpi=300)\n",
        "   print(f\"Visualization for layer {layer_index} saved as\n",
        "'expert_galaxy_layer_{layer_index}_v2.png'\")\n",
        "   plt.show()\n",
        "\n",
        "# --- Step 4: Generate plots for key layers ---\n",
        "# We visualize the first, middle, and last layers to see how organization evolves.\n",
        "layers_to_plot = [0, NUM_LAYERS // 2, NUM_LAYERS - 1]\n",
        "for layer_idx in layers_to_plot:\n",
        "   plot_galaxy(layer_index=layer_idx)\n",
        "____________________________________\n",
        "--- Re-instantiating model for visualization ---\n",
        "--- Loading weights from best_some_transformer_v2_baseline.pth ---\n",
        "Successfully loaded trained model weights.\n",
        "\n",
        "--- Visualizing Layer 0 ---\n",
        "Extracted 256 keys. Total usage (EMA-scaled): 262144.25\n",
        "Running t-SNE with perplexity=30... (this may take a moment)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter'\n",
        "was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
        " warnings.warn(\n",
        "\n",
        "t-SNE complete.\n",
        "Visualization for layer 0 saved as 'expert_galaxy_layer_0_v2.png'\n",
        "\n",
        "--- Visualizing Layer 4 ---\n",
        "Extracted 256 keys. Total usage (EMA-scaled): 262144.25\n",
        "Running t-SNE with perplexity=30... (this may take a moment)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter'\n",
        "was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
        " warnings.warn(\n",
        "\n",
        "t-SNE complete.\n",
        "Visualization for layer 4 saved as 'expert_galaxy_layer_4_v2.png'\n",
        "\n",
        "--- Visualizing Layer 7 ---\n",
        "Extracted 256 keys. Total usage (EMA-scaled): 262144.28\n",
        "Running t-SNE with perplexity=30... (this may take a moment)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter'\n",
        "was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
        " warnings.warn(\n",
        "\n",
        "t-SNE complete.\n",
        "Visualization for layer 7 saved as 'expert_galaxy_layer_7_v2.png'\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}