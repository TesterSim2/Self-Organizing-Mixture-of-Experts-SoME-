{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SoME v4 Code\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "#\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "print(\"--- Part 1: Setup and Dependencies ---\")\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs for a free performance boost\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Enable benchmark mode for cuDNN\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Core SoME Framework (Corrected)\n",
        "# (Classes, Data Functions, Training Loop)\n",
        "#\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "# --- 1. Model Component Classes ---\n",
        "\n",
        "class Expert(nn.Module):\n",
        "   \"\"\"An expert module with configurable random weight initialization.\"\"\"\n",
        "   def __init__(self, d_model, d_ffn, init_method='default'):\n",
        "      super().__init__()\n",
        "      self.w_down = nn.Linear(d_model, d_ffn)\n",
        "      self.activation = nn.GELU()\n",
        "      self.w_up = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "     if init_method == 'orthogonal':\n",
        "         nn.init.orthogonal_(self.w_down.weight)\n",
        "         nn.init.orthogonal_(self.w_up.weight)\n",
        "     elif init_method == 'sparse':\n",
        "         nn.init.sparse_(self.w_down.weight, sparsity=0.5)\n",
        "         nn.init.sparse_(self.w_up.weight, sparsity=0.5)\n",
        "     elif init_method != 'default':\n",
        "         raise ValueError(f\"Unknown initialization method: {init_method}\")\n",
        "\n",
        "     nn.init.zeros_(self.w_down.bias)\n",
        "     nn.init.zeros_(self.w_up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, some_config):\n",
        "     super().__init__()\n",
        "     self.d_model = d_model\n",
        "     self.num_experts = some_config['num_experts']\n",
        "     self.d_ffn = some_config['d_ffn']\n",
        "     self.top_k = some_config['top_k']\n",
        "\n",
        "        # Heuristic update parameters\n",
        "        self.alpha = some_config['alpha']\n",
        "        self.beta = some_config['beta']\n",
        "        self.delta = some_config['delta']\n",
        "\n",
        "        # Key management parameters\n",
        "        self.theta_percentile = some_config['theta_percentile']\n",
        "        self.warmup_steps = some_config['warmup_steps']\n",
        "        self.ema_decay = some_config['ema_decay']\n",
        "\n",
        "     # --- Ablation flags ---\n",
        "     self.ablation_flags = some_config.get('ablation_flags', {'use_alpha': True, 'use_beta': True,\n",
        "'use_delta': True})\n",
        "     print(f\"SoME Layer Ablation Flags: {self.ablation_flags}\")\n",
        "\n",
        "        # --- Upgraded MLP Router ---\n",
        "        hidden_dim = d_model * 2\n",
        "        self.query_network = nn.Sequential(\n",
        "           nn.Linear(d_model, hidden_dim),\n",
        "           nn.GELU(),\n",
        "           nn.Linear(hidden_dim, d_model)\n",
        "        )\n",
        "\n",
        "        keys = torch.randn(self.num_experts, d_model)\n",
        "        self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "        self.register_buffer(\"usage_count\", torch.zeros(self.num_experts))\n",
        "        self.register_buffer(\"steps\", torch.tensor([0], dtype=torch.long))\n",
        "\n",
        "    self.experts = nn.ModuleList([Expert(d_model, self.d_ffn,\n",
        "                        init_method=some_config['init_method']) for _ in\n",
        "range(self.num_experts)])\n",
        "\n",
        "        # Freeze all expert parameters\n",
        "        for expert in self.experts:\n",
        "           for param in expert.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "        if self.top_k > 1:\n",
        "            self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "\n",
        "    queries_raw = self.query_network(x_flat)\n",
        "    queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "\n",
        "    scores = torch.matmul(queries, self.key_store.t())\n",
        "    top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "    gating_weights = F.softmax(top_k_scores / temperature, dim=-1)\n",
        "\n",
        "    flat_top_k_indices = top_k_indices.view(-1)\n",
        "\n",
        "     # Efficient expert processing using permutation\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "    flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "    permuted_inputs = flat_inputs[permutation_map]\n",
        "    split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "    output_chunks = []\n",
        "    for i, expert_id in enumerate(unique_expert_ids):\n",
        "       output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "    concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "    inverse_permutation_map = torch.argsort(permutation_map)\n",
        "    expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "    weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "                 gating_weights.unsqueeze(-1)).sum(dim=1)\n",
        "    final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    return x + final_output, queries, top_k_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_keys(self, queries, top_k_indices):\n",
        "    self.steps += 1\n",
        "    unique_indices, counts = torch.unique(top_k_indices, return_counts=True)\n",
        "    self.usage_count.mul_(self.ema_decay)\n",
        "    self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts.float())\n",
        "\n",
        "    # --- Heuristic 1: Alpha (Attraction) ---\n",
        "    if self.ablation_flags.get('use_alpha', True):\n",
        "        for i in range(self.top_k):\n",
        "           indices = top_k_indices[:, i]\n",
        "\n",
        "         inertia = 1.0 + self.usage_count[indices]\n",
        "         alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "         update_vec = queries - self.key_store[indices]\n",
        "         self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "    # --- Heuristic 2: Beta (Peer Pull / Clustering) ---\n",
        "    if self.top_k > 1 and self.ablation_flags.get('use_beta', True):\n",
        "        indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "        indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "\n",
        "       keys_i, keys_j = self.key_store[indices_i], self.key_store[indices_j]\n",
        "       inertia_i = (1.0 + self.usage_count[indices_i]).unsqueeze(-1)\n",
        "       inertia_j = (1.0 + self.usage_count[indices_j]).unsqueeze(-1)\n",
        "       beta_effective = self.beta / torch.min(inertia_i, inertia_j)\n",
        "\n",
        "       update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "       update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "       self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "       self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "    self.key_store.data = F.normalize(self.key_store.data, p=2, dim=-1)\n",
        "\n",
        "    # --- Heuristic 3: Delta (Decay / \"Use it or Lose it\") ---\n",
        "    if self.steps > self.warmup_steps and self.ablation_flags.get('use_delta', True):\n",
        "        active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "        if active_usage_counts.numel() > 0:\n",
        "            dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "            low_usage_mask = self.usage_count < dynamic_theta\n",
        "            self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_config):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = SOMELayer(d_model, some_config)\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    seq_len = x.size(1)\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device) * float('-inf'), diagonal=1)\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "    x = self.norm1(x + attn_output)\n",
        "\n",
        "    some_output, queries, top_k_indices = self.some_layer(x, temperature=temperature)\n",
        "    x = self.norm2(some_output)\n",
        "    return x, queries, top_k_indices\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "     super().__init__()\n",
        "     position = torch.arange(max_len).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "     pe = torch.zeros(1, max_len, d_model)\n",
        "     pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "     pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "     self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, model_config, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(model_config['vocab_size'], model_config['d_model'])\n",
        "     self.pos_encoder = PositionalEncoding(model_config['d_model'], model_config['seq_len'])\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(model_config['d_model'], model_config['num_heads'],\n",
        "some_config)\n",
        "        for _ in range(model_config['num_layers'])\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(model_config['d_model'], model_config['vocab_size'])\n",
        "     self.d_model = model_config['d_model']\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)\n",
        "    all_queries, all_indices = [], []\n",
        "    for layer in self.layers:\n",
        "       x, queries, top_k_indices = layer(x, temperature=temperature)\n",
        "       all_queries.append(queries)\n",
        "       all_indices.append(top_k_indices)\n",
        "    return self.fc_out(x), all_queries, all_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices):\n",
        "\n",
        "     for i, layer_block in enumerate(self.layers):\n",
        "        queries = all_queries[i].view(-1, layer_block.some_layer.d_model)\n",
        "        indices = all_indices[i].view(-1, layer_block.some_layer.top_k)\n",
        "        layer_block.some_layer.update_keys(queries, indices)\n",
        "\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data):\n",
        "     self.data = tokenized_data\n",
        "   def __len__(self):\n",
        "     return len(self.data)\n",
        "   def __getitem__(self, idx):\n",
        "     item = self.data[idx]\n",
        "     # FIX: Use .clone().detach() for efficient, warning-free tensor copies\n",
        "     inputs = item['input_ids'].clone().detach()\n",
        "     targets = inputs.clone()\n",
        "     targets[:-1] = inputs[1:]\n",
        "     targets[-1] = -100 # Ignore loss for the last token prediction\n",
        "     return inputs, targets\n",
        "\n",
        "def prepare_data(config):\n",
        "  print(\"\\n--- Part 2: Data Preparation & Configuration ---\")\n",
        "  tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "  if not os.path.exists(tokenizer_path):\n",
        "      print(\"Training custom tokenizer...\")\n",
        "      dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "      def get_training_corpus():\n",
        "         for i in range(0, len(dataset), 1000):\n",
        "            yield dataset[i : i + 1000][\"text\"]\n",
        "      tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "      tokenizer_model.pre_tokenizer = Whitespace()\n",
        "      trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"],\n",
        "                     vocab_size=config['model']['vocab_size'])\n",
        "      tokenizer_model.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "      tokenizer_model.save(tokenizer_path)\n",
        "  else:\n",
        "      print(\"Tokenizer already exists. Loading from file.\")\n",
        "\n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "  print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "  print(\"\\nTokenizing dataset...\")\n",
        "  full_dataset = load_dataset(\"roneneldan/TinyStories\", streaming=False)\n",
        "  train_subset = full_dataset['train'].select(range(config['data']['train_subset_size']))\n",
        "  val_subset = full_dataset['validation'].select(range(config['data']['val_subset_size']))\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    text_with_eos = [s + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "    return tokenizer(text_with_eos, truncation=True, padding=\"max_length\",\n",
        "               max_length=config['model']['seq_len'], return_tensors=\"pt\")\n",
        "\n",
        "  tokenized_train = train_subset.map(tokenize_function, batched=True,\n",
        "                         remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
        "  tokenized_val = val_subset.map(tokenize_function, batched=True,\n",
        "                       remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
        "  tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "  tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "  train_dataset = LanguageModelDataset(tokenized_train)\n",
        "  validation_dataset = LanguageModelDataset(tokenized_val)\n",
        "\n",
        "  num_workers = max(2, os.cpu_count() // 2 if os.cpu_count() else 2)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], shuffle=True,\n",
        "                  drop_last=True, num_workers=num_workers, pin_memory=True)\n",
        "  validation_loader = DataLoader(validation_dataset, batch_size=config['data']['batch_size'],\n",
        "                      drop_last=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "  print(f\"\\nTrain dataset size (subset): {len(train_dataset)}\")\n",
        "  print(f\"Using {num_workers} workers for DataLoader.\")\n",
        "  return train_loader, validation_loader, tokenizer\n",
        "\n",
        "\n",
        "# --- 3. Training & Evaluation Functions ---\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0: return 0.0\n",
        "\n",
        "  probs = usage_counts.float() / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp,\n",
        "tokenizer_vocab_size):\n",
        "   model.train()\n",
        "   total_loss = 0\n",
        "   # FIX: Use the modern torch.amp API\n",
        "   scaler = torch.amp.GradScaler(\"cuda\")\n",
        "   progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "  for inputs, targets in progress_bar:\n",
        "     inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "\n",
        "     # FIX: Use the modern torch.amp API\n",
        "     with torch.amp.autocast(\"cuda\"):\n",
        "        logits, queries, indices = model(inputs, temperature=current_temp)\n",
        "        loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "     # FIX: Correct order of scheduler and optimizer steps\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "     scheduler.step()\n",
        "\n",
        "     model.update_all_keys(queries, indices)\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, tokenizer_vocab_size):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "  with torch.no_grad():\n",
        "     for inputs, targets in progress_bar:\n",
        "\n",
        "       inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "       # FIX: Use the modern torch.amp API\n",
        "       with torch.amp.autocast(\"cuda\"):\n",
        "          logits, _, _ = model(inputs, temperature=0.5) # Sharpen during eval\n",
        "          loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "       total_loss += loss.item()\n",
        "       progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, config):\n",
        "  epochs = len(train_losses)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  title = f\"SoME v3.0 Run: {config['run_name']}\"\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "  filename = f\"loss_curve_{config['run_name']}.png\"\n",
        "  plt.savefig(filename)\n",
        "  print(f\"\\nLoss curve plot saved to {filename}\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# --- 4. Main Execution Function ---\n",
        "\n",
        "def main(config):\n",
        "  \"\"\"Main function to run a SoME experiment.\"\"\"\n",
        "  print(f\"\\n--- Starting Experiment: {config['run_name']} ---\")\n",
        "\n",
        "  # 1. Data\n",
        "  train_loader, val_loader, tokenizer = prepare_data(config)\n",
        "\n",
        "  # 2. Model Initialization\n",
        "  print(\"\\n--- Part 3: Model Definition ---\")\n",
        "  model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "\n",
        "  if hasattr(torch, 'compile'):\n",
        "      print(\"\\nCompiling the model for faster training...\")\n",
        "\n",
        "     model = torch.compile(model)\n",
        "\n",
        "  # 3. Training Setup\n",
        "  print(\"\\n--- Part 4: Training, Evaluation, and Metrics ---\")\n",
        "  optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "                        lr=config['training']['learning_rate'], betas=(0.9, 0.95), weight_decay=0.1)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "  total_steps = len(train_loader) * config['training']['num_epochs']\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "   total_params = sum(p.numel() for p in model.parameters())\n",
        "   trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "   print(f\"\\nTotal parameters: {total_params/1e6:.2f}M\")\n",
        "   print(f\"Trainable parameters: {trainable_params/1e6:.2f}M ({100 * trainable_params /\n",
        "total_params:.2f}%)\")\n",
        "   print(f\"Total training steps: {total_steps}\")\n",
        "   print(f\"Using expert initialization method: {config['some_layer']['init_method']}\")\n",
        "\n",
        "  # 4. Training Loop\n",
        "  train_losses, val_losses = [], []\n",
        "  best_val_loss = float('inf')\n",
        "  model_save_path = f\"best_model_{config['run_name']}.pth\"\n",
        "\n",
        "  for epoch in range(config['training']['num_epochs']):\n",
        "     print(f\"\\n--- Epoch {epoch+1}/{config['training']['num_epochs']} ---\")\n",
        "\n",
        "     train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler,\n",
        "                      config['training']['training_temp'], tokenizer.vocab_size)\n",
        "     val_loss = evaluate_epoch(model, val_loader, criterion, tokenizer.vocab_size)\n",
        "     perplexity = math.exp(val_loss)\n",
        "\n",
        "     train_losses.append(train_loss)\n",
        "     val_losses.append(val_loss)\n",
        "\n",
        "     model_to_inspect = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "     # Inspect a representative middle layer\n",
        "     middle_layer_idx = config['model']['num_layers'] // 2\n",
        "     usage_counts = model_to_inspect.layers[middle_layer_idx].some_layer.usage_count\n",
        "     gini_coeff = calculate_gini(usage_counts)\n",
        "     entropy_val = calculate_entropy(usage_counts)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val\n",
        "Perplexity = {perplexity:.2f}\")\n",
        "    print(f\" Middle Layer Expert Metrics: Gini = {gini_coeff:.3f}, Entropy = {entropy_val:.3f}\")\n",
        "\n",
        "     if val_loss < best_val_loss:\n",
        "         best_val_loss = val_loss\n",
        "         torch.save(model_to_inspect.state_dict(), model_save_path)\n",
        "         print(f\" Model saved as {model_save_path}\")\n",
        "\n",
        "  # 5. Finalization\n",
        "  print(f\"\\n--- Training Complete for {config['run_name']} ---\")\n",
        "  plot_losses(train_losses, val_losses, config)\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "#\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Experiment Configuration & Execution\n",
        "#\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "# --- Define the configuration for the experiment ---\n",
        "config = {\n",
        "   # Unique name for this run, used for saving files\n",
        "   \"run_name\": \"v4_MLP_Router_Baseline\",\n",
        "\n",
        "  # --- Data Settings ---\n",
        "  \"data\": {\n",
        "     \"train_subset_size\": 10000,\n",
        "     \"val_subset_size\": 2000,\n",
        "     \"batch_size\": 32,\n",
        "  },\n",
        "\n",
        "  # --- Model Dimensions ---\n",
        "  \"model\": {\n",
        "     \"vocab_size\": 8192,\n",
        "     \"d_model\": 512,\n",
        "     \"num_heads\": 8,\n",
        "     \"num_layers\": 10,\n",
        "     \"seq_len\": 768,\n",
        "  },\n",
        "\n",
        "  # --- SoME Layer Hyperparameters ---\n",
        "  \"some_layer\": {\n",
        "     \"num_experts\": 256,\n",
        "\n",
        "         \"d_ffn\": 1536,\n",
        "         \"top_k\": 8,\n",
        "         \"init_method\": \"default\", # Options: 'default', 'orthogonal', 'sparse'\n",
        "\n",
        "         # Heuristic update rules\n",
        "         \"alpha\": 0.01, # Attraction\n",
        "         \"beta\": 0.005, # Peer Pull / Clustering\n",
        "         \"delta\": 0.001, # Decay\n",
        "\n",
        "         # Key management\n",
        "         \"theta_percentile\": 0.05,\n",
        "         \"warmup_steps\": 400,\n",
        "         \"ema_decay\": 0.99,\n",
        "\n",
        "         # ======================================================\n",
        "         # === ABLATION STUDY CONTROL PANEL ===\n",
        "         # Set these to True/False to enable/disable heuristics\n",
        "         \"ablation_flags\": {\n",
        "           \"use_alpha\": True, # Master switch for the attraction rule\n",
        "           \"use_beta\": True, # Master switch for the peer pull rule\n",
        "           \"use_delta\": True # Master switch for the decay rule\n",
        "         }\n",
        "         # ======================================================\n",
        "    },\n",
        "\n",
        "    # --- Training Schedule ---\n",
        "    \"training\": {\n",
        "       \"num_epochs\": 4,\n",
        "       \"learning_rate\": 6e-4,\n",
        "       \"training_temp\": 0.8,\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Run the experiment ---\n",
        "main(config)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}