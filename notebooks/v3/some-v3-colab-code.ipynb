{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SoME v3 Colab Code Notebook\n",
        "\n",
        "Cell One\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "print(\"--- Part 1: Setup and Dependencies ---\")\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs for a free performance boost\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Enable benchmark mode for cuDNN\n",
        "torch.backends.cudnn.benchmark = True\n",
        "____________________________________\n",
        "Cell Two\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Core SoME Framework\n",
        "# (Classes, Data Functions, Training Loop)\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "# --- 1. Model Component Classes ---\n",
        "\n",
        "class Expert(nn.Module):\n",
        "   \"\"\"An expert module with configurable random weight initialization.\"\"\"\n",
        "   def __init__(self, d_model, d_ffn, init_method='default'):\n",
        "      super().__init__()\n",
        "      self.w_down = nn.Linear(d_model, d_ffn)\n",
        "      self.activation = nn.GELU()\n",
        "      self.w_up = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "     if init_method == 'orthogonal':\n",
        "         nn.init.orthogonal_(self.w_down.weight)\n",
        "         nn.init.orthogonal_(self.w_up.weight)\n",
        "     elif init_method == 'sparse':\n",
        "         nn.init.sparse_(self.w_down.weight, sparsity=0.5)\n",
        "         nn.init.sparse_(self.w_up.weight, sparsity=0.5)\n",
        "     elif init_method != 'default':\n",
        "         raise ValueError(f\"Unknown initialization method: {init_method}\")\n",
        "\n",
        "     nn.init.zeros_(self.w_down.bias)\n",
        "     nn.init.zeros_(self.w_up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, some_config):\n",
        "     super().__init__()\n",
        "     self.d_model = d_model\n",
        "     self.num_experts = some_config['num_experts']\n",
        "     self.d_ffn = some_config['d_ffn']\n",
        "     self.top_k = some_config['top_k']\n",
        "\n",
        "     # Heuristic update parameters\n",
        "     self.alpha = some_config['alpha']\n",
        "\n",
        "        self.beta = some_config['beta']\n",
        "        self.delta = some_config['delta']\n",
        "\n",
        "        # Key management parameters\n",
        "        self.theta_percentile = some_config['theta_percentile']\n",
        "        self.warmup_steps = some_config['warmup_steps']\n",
        "        self.ema_decay = some_config['ema_decay']\n",
        "\n",
        "     # --- NEW: Ablation flags ---\n",
        "     self.ablation_flags = some_config.get('ablation_flags', {'use_alpha': True, 'use_beta': True,\n",
        "'use_delta': True})\n",
        "     print(f\"SoME Layer Ablation Flags: {self.ablation_flags}\")\n",
        "\n",
        "        self.query_network = nn.Linear(d_model, d_model)\n",
        "\n",
        "        keys = torch.randn(self.num_experts, d_model)\n",
        "        self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "        self.register_buffer(\"usage_count\", torch.zeros(self.num_experts))\n",
        "        self.register_buffer(\"steps\", torch.tensor([0], dtype=torch.long))\n",
        "\n",
        "     self.experts = nn.ModuleList([Expert(d_model, self.d_ffn,\n",
        "init_method=some_config['init_method']) for _ in range(self.num_experts)])\n",
        "\n",
        "        # Freeze all expert parameters\n",
        "        for expert in self.experts:\n",
        "           for param in expert.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "        if self.top_k > 1:\n",
        "            self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "\n",
        "        queries_raw = self.query_network(x_flat)\n",
        "        queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(queries, self.key_store.t())\n",
        "        top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "        gating_weights = F.softmax(top_k_scores / temperature, dim=-1)\n",
        "\n",
        "        flat_top_k_indices = top_k_indices.view(-1)\n",
        "\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "    flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "    permuted_inputs = flat_inputs[permutation_map]\n",
        "    split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "    output_chunks = []\n",
        "    for i, expert_id in enumerate(unique_expert_ids):\n",
        "       output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "    concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "    inverse_permutation_map = torch.argsort(permutation_map)\n",
        "    expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "     weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "gating_weights.unsqueeze(-1)).sum(dim=1)\n",
        "     final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    return x + final_output, queries, top_k_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_keys(self, queries, top_k_indices):\n",
        "    self.steps += 1\n",
        "    unique_indices, counts = torch.unique(top_k_indices, return_counts=True)\n",
        "    self.usage_count.mul_(self.ema_decay)\n",
        "    self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts.float())\n",
        "\n",
        "    # --- Heuristic 1: Alpha (Attraction) ---\n",
        "    if self.ablation_flags.get('use_alpha', True):\n",
        "        for i in range(self.top_k):\n",
        "           indices = top_k_indices[:, i]\n",
        "           inertia = 1.0 + self.usage_count[indices]\n",
        "           alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "           update_vec = queries - self.key_store[indices]\n",
        "           self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "    # --- Heuristic 2: Beta (Peer Pull / Clustering) ---\n",
        "    if self.top_k > 1 and self.ablation_flags.get('use_beta', True):\n",
        "        indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "        indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "        keys_i, keys_j = self.key_store[indices_i], self.key_store[indices_j]\n",
        "        inertia_i = (1.0 + self.usage_count[indices_i]).unsqueeze(-1)\n",
        "\n",
        "       inertia_j = (1.0 + self.usage_count[indices_j]).unsqueeze(-1)\n",
        "       beta_effective = self.beta / torch.min(inertia_i, inertia_j)\n",
        "\n",
        "       update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "       update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "       self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "       self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "    self.key_store.data = F.normalize(self.key_store.data, p=2, dim=-1)\n",
        "\n",
        "    # --- Heuristic 3: Delta (Decay / \"Use it or Lose it\") ---\n",
        "    if self.steps > self.warmup_steps and self.ablation_flags.get('use_delta', True):\n",
        "        active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "        if active_usage_counts.numel() > 0:\n",
        "            dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "            low_usage_mask = self.usage_count < dynamic_theta\n",
        "            self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_config):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = SOMELayer(d_model, some_config)\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    seq_len = x.size(1)\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device) * float('-inf'), diagonal=1)\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "    x = self.norm1(x + attn_output)\n",
        "    some_output, queries, top_k_indices = self.some_layer(x, temperature=temperature)\n",
        "    x = self.norm2(some_output)\n",
        "    return x, queries, top_k_indices\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "     super().__init__()\n",
        "     position = torch.arange(max_len).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "     pe = torch.zeros(1, max_len, d_model)\n",
        "     pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "     pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "     self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, model_config, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(model_config['vocab_size'], model_config['d_model'])\n",
        "     self.pos_encoder = PositionalEncoding(model_config['d_model'], model_config['seq_len'])\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(model_config['d_model'], model_config['num_heads'],\n",
        "some_config)\n",
        "        for _ in range(model_config['num_layers'])\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(model_config['d_model'], model_config['vocab_size'])\n",
        "     self.d_model = model_config['d_model']\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)\n",
        "    all_queries, all_indices = [], []\n",
        "    for layer in self.layers:\n",
        "       x, queries, top_k_indices = layer(x, temperature=temperature)\n",
        "       all_queries.append(queries)\n",
        "       all_indices.append(top_k_indices)\n",
        "    return self.fc_out(x), all_queries, all_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices):\n",
        "    for i, layer_block in enumerate(self.layers):\n",
        "       queries = all_queries[i].view(-1, layer_block.some_layer.d_model)\n",
        "       indices = all_indices[i].view(-1, layer_block.some_layer.top_k)\n",
        "       layer_block.some_layer.update_keys(queries, indices)\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data):\n",
        "     self.data = tokenized_data\n",
        "   def __len__(self):\n",
        "     return len(self.data)\n",
        "   def __getitem__(self, idx):\n",
        "     item = self.data[idx]\n",
        "     inputs = torch.tensor(item['input_ids'])\n",
        "\n",
        "     targets = inputs.clone()\n",
        "     targets[:-1] = inputs[1:]\n",
        "     targets[-1] = -100 # Ignore loss for the last token prediction\n",
        "     return inputs, targets\n",
        "\n",
        "def prepare_data(config):\n",
        "  print(\"\\n--- Part 2: Data Preparation & Configuration ---\")\n",
        "  tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "  if not os.path.exists(tokenizer_path):\n",
        "      print(\"Training custom tokenizer...\")\n",
        "      dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "      def get_training_corpus():\n",
        "         for i in range(0, len(dataset), 1000):\n",
        "            yield dataset[i : i + 1000][\"text\"]\n",
        "      tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "      tokenizer_model.pre_tokenizer = Whitespace()\n",
        "      trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"],\n",
        "vocab_size=config['model']['vocab_size'])\n",
        "      tokenizer_model.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "      tokenizer_model.save(tokenizer_path)\n",
        "  else:\n",
        "      print(\"Tokenizer already exists. Loading from file.\")\n",
        "\n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "  print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "  print(\"\\nTokenizing dataset...\")\n",
        "  full_dataset = load_dataset(\"roneneldan/TinyStories\", streaming=False)\n",
        "  train_subset = full_dataset['train'].select(range(config['data']['train_subset_size']))\n",
        "  val_subset = full_dataset['validation'].select(range(config['data']['val_subset_size']))\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    text_with_eos = [s + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "    return tokenizer(text_with_eos, truncation=True, padding=\"max_length\",\n",
        "max_length=config['model']['seq_len'], return_tensors=\"pt\")\n",
        "\n",
        "  tokenized_train = train_subset.map(tokenize_function, batched=True,\n",
        "remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
        "  tokenized_val = val_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "\n",
        "  train_dataset = LanguageModelDataset(tokenized_train)\n",
        "  validation_dataset = LanguageModelDataset(tokenized_val)\n",
        "\n",
        "  num_workers = max(2, os.cpu_count() // 2 if os.cpu_count() else 2)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], shuffle=True,\n",
        "drop_last=True, num_workers=num_workers, pin_memory=True)\n",
        "  validation_loader = DataLoader(validation_dataset, batch_size=config['data']['batch_size'],\n",
        "drop_last=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "  print(f\"\\nTrain dataset size (subset): {len(train_dataset)}\")\n",
        "  print(f\"Using {num_workers} workers for DataLoader.\")\n",
        "  return train_loader, validation_loader, tokenizer\n",
        "\n",
        "\n",
        "# --- 3. Training & Evaluation Functions ---\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0: return 0.0\n",
        "  probs = usage_counts / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp,\n",
        "tokenizer_vocab_size):\n",
        "   model.train()\n",
        "   total_loss = 0\n",
        "   scaler = torch.cuda.amp.GradScaler()\n",
        "   progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "  for inputs, targets in progress_bar:\n",
        "     inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "\n",
        "     with torch.cuda.amp.autocast():\n",
        "        logits, queries, indices = model(inputs, temperature=current_temp)\n",
        "        loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "     scheduler.step()\n",
        "\n",
        "     model.update_all_keys(queries, indices)\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, tokenizer_vocab_size):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "           logits, _, _ = model(inputs, temperature=0.5) # Sharpen during eval\n",
        "           loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, config):\n",
        "  epochs = len(train_losses)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  title = f\"SoME v3.0 Run: {config['run_name']}\"\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "  filename = f\"loss_curve_{config['run_name']}.png\"\n",
        "\n",
        "  plt.savefig(filename)\n",
        "  print(f\"\\nLoss curve plot saved to {filename}\")\n",
        "  plt.show()\n",
        "\n",
        "# --- 4. Main Execution Function ---\n",
        "\n",
        "def main(config):\n",
        "  \"\"\"Main function to run a SoME experiment.\"\"\"\n",
        "  print(f\"\\n--- Starting Experiment: {config['run_name']} ---\")\n",
        "\n",
        "  # 1. Data\n",
        "  train_loader, val_loader, tokenizer = prepare_data(config)\n",
        "\n",
        "  # 2. Model Initialization\n",
        "  print(\"\\n--- Part 3: Model Definition ---\")\n",
        "  model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "\n",
        "  if hasattr(torch, 'compile'):\n",
        "      print(\"\\nCompiling the model for faster training...\")\n",
        "      model = torch.compile(model)\n",
        "\n",
        "  # 3. Training Setup\n",
        "  print(\"\\n--- Part 4: Training, Evaluation, and Metrics ---\")\n",
        "  optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "                        lr=config['training']['learning_rate'], betas=(0.9, 0.95), weight_decay=0.1)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "  total_steps = len(train_loader) * config['training']['num_epochs']\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "   total_params = sum(p.numel() for p in model.parameters())\n",
        "   trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "   print(f\"\\nTotal parameters: {total_params/1e6:.2f}M\")\n",
        "   print(f\"Trainable parameters: {trainable_params/1e6:.2f}M ({100 * trainable_params /\n",
        "total_params:.2f}%)\")\n",
        "   print(f\"Total training steps: {total_steps}\")\n",
        "   print(f\"Using expert initialization method: {config['some_layer']['init_method']}\")\n",
        "\n",
        "  # 4. Training Loop\n",
        "  train_losses, val_losses = [], []\n",
        "  best_val_loss = float('inf')\n",
        "  model_save_path = f\"best_model_{config['run_name']}.pth\"\n",
        "\n",
        "  for epoch in range(config['training']['num_epochs']):\n",
        "     print(f\"\\n--- Epoch {epoch+1}/{config['training']['num_epochs']} ---\")\n",
        "\n",
        "     train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler,\n",
        "config['training']['training_temp'], tokenizer.vocab_size)\n",
        "     val_loss = evaluate_epoch(model, val_loader, criterion, tokenizer.vocab_size)\n",
        "     perplexity = math.exp(val_loss)\n",
        "\n",
        "     train_losses.append(train_loss)\n",
        "     val_losses.append(val_loss)\n",
        "\n",
        "     model_to_inspect = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "     usage_counts = model_to_inspect.layers[config['model']['num_layers'] //\n",
        "2].some_layer.usage_count\n",
        "     gini_coeff = calculate_gini(usage_counts)\n",
        "     entropy_val = calculate_entropy(usage_counts)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val\n",
        "Perplexity = {perplexity:.2f}\")\n",
        "    print(f\" Middle Layer Expert Metrics: Gini = {gini_coeff:.3f}, Entropy = {entropy_val:.3f}\")\n",
        "\n",
        "     if val_loss < best_val_loss:\n",
        "         best_val_loss = val_loss\n",
        "         torch.save(model_to_inspect.state_dict(), model_save_path)\n",
        "         print(f\" Model saved as {model_save_path}\")\n",
        "\n",
        "  # 5. Finalization\n",
        "  print(f\"\\n--- Training Complete for {config['run_name']} ---\")\n",
        "  plot_losses(train_losses, val_losses, config)\n",
        "____________________________________\n",
        "Cell Three\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Experiment Configuration & Execution\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "# --- Define the configuration for the experiment ---\n",
        "config = {\n",
        "   # Unique name for this run, used for saving files\n",
        "   \"run_name\": \"v3_baseline_orthogonal\",\n",
        "\n",
        " # --- Data Settings ---\n",
        " \"data\": {\n",
        "\n",
        "     \"train_subset_size\": 10000,\n",
        "     \"val_subset_size\": 1000,\n",
        "     \"batch_size\": 24,\n",
        "},\n",
        "\n",
        "# --- Model Dimensions ---\n",
        "\"model\": {\n",
        "   \"vocab_size\": 8192,\n",
        "   \"d_model\": 768,\n",
        "   \"num_heads\": 12,\n",
        "   \"num_layers\": 12,\n",
        "   \"seq_len\": 1536,\n",
        "},\n",
        "\n",
        "# --- SoME Layer Hyperparameters ---\n",
        "\"some_layer\": {\n",
        "   \"num_experts\": 256,\n",
        "   \"d_ffn\": 1536,\n",
        "   \"top_k\": 8,\n",
        "   \"init_method\": \"sparse\", # Options: 'default', 'orthogonal', 'sparse'\n",
        "\n",
        "     # Heuristic update rules\n",
        "     \"alpha\": 0.015, # Attraction\n",
        "     \"beta\": 0.001, # Peer Pull / Clustering\n",
        "     \"delta\": 0.001, # Decay\n",
        "\n",
        "     # Key management\n",
        "     \"theta_percentile\": 0.05,\n",
        "     \"warmup_steps\": 400,\n",
        "     \"ema_decay\": 0.995,\n",
        "\n",
        "     # ======================================================== #\n",
        "     # === ABLATION STUDY CONTROL PANEL ===\n",
        "     # Set these to True/False to enable/disable heuristics\n",
        "     \"ablation_flags\": {\n",
        "       \"use_alpha\": True, # Master switch for the attraction rule\n",
        "       \"use_beta\": True, # Master switch for the peer pull rule\n",
        "       \"use_delta\": True # Master switch for the decay rule\n",
        "     }\n",
        "     # ======================================================== #\n",
        "},\n",
        "\n",
        "# --- Training Schedule ---\n",
        "\"training\": {\n",
        "\n",
        "        \"num_epochs\": 4,\n",
        "        \"learning_rate\": 6e-4,\n",
        "        \"training_temp\": 1.0,\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Run the experiment ---\n",
        "main(config)\n",
        "____________________________________\n",
        "Cell Four\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Multi-Layer SoME Analysis & Diagnostics Dashboard v1.3\n",
        "#\n",
        "# RUN THIS CELL AFTER A TRAINING RUN (CELL 3) IS COMPLETE\n",
        "#\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "# --- 1. Setup & Artifact Loading ---\n",
        "print(\"\\n--- Part 1: Dashboard Setup ---\")\n",
        "!pip install umap-learn seaborn -q\n",
        "import umap\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the latest configuration is loaded\n",
        "model_path_to_load = f\"best_model_{config['run_name']}.pth\"\n",
        "tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "\n",
        "if os.path.exists(model_path_to_load) and os.path.exists(tokenizer_path):\n",
        "    print(f\"Loading best model from: {model_path_to_load}\")\n",
        "    print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
        "\n",
        "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "\n",
        "    analysis_model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "    analysis_model.load_state_dict(torch.load(model_path_to_load))\n",
        "    analysis_model.eval()\n",
        "\n",
        "    # We will inspect multiple layers, so no single 'layer_to_inspect' is needed here.\n",
        "\n",
        "else:\n",
        "   print(f\"ERROR: Could not find a saved model or tokenizer. Please run a training cell first.\")\n",
        "   analysis_model = None\n",
        "   tokenizer = None\n",
        "\n",
        "if analysis_model and tokenizer:\n",
        "    # --- 2. Aggregate Utilization Analysis (from a representative middle layer) ---\n",
        "    print(\"\\n\\n--- Part 2: Aggregate Utilization Analysis (from Middle Layer) ---\")\n",
        "\n",
        "  middle_layer_idx = config['model']['num_layers'] // 2\n",
        "  middle_layer = analysis_model.layers[middle_layer_idx].some_layer\n",
        "  usage_counts = middle_layer.usage_count.cpu()\n",
        "  total_experts = middle_layer.num_experts\n",
        "\n",
        "   used_experts = torch.sum(usage_counts > 0).item()\n",
        "   usage_percentage = (used_experts / total_experts) * 100\n",
        "   print(f\"Expert Usage (Layer {middle_layer_idx}): {used_experts}/{total_experts}\n",
        "({usage_percentage:.2f}%)\")\n",
        "\n",
        "  final_gini = calculate_gini(usage_counts)\n",
        "  final_entropy = calculate_entropy(usage_counts)\n",
        "  print(f\"Final Gini Coefficient (Layer {middle_layer_idx}): {final_gini:.4f}\")\n",
        "  print(f\"Final Shannon Entropy (Layer {middle_layer_idx}): {final_entropy:.4f} (Max:\n",
        "{math.log2(total_experts):.4f})\")\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.hist(usage_counts.numpy(), bins=50, color='skyblue', edgecolor='black')\n",
        "  plt.yscale('log')\n",
        "  plt.title(f\"Log-Scale Histogram of Expert Usage Counts ({config['run_name']} - Layer\n",
        "{middle_layer_idx})\")\n",
        "  plt.xlabel('Usage Count (EMA)')\n",
        "  plt.ylabel('Number of Experts (Log Scale)')\n",
        "  plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "  plt.show()\n",
        "\n",
        "  # --- 3. Key Store Structure Visualization (from middle layer) ---\n",
        "  print(\"\\n\\n--- Part 3: Key Store Structure Visualization (from Middle Layer) ---\")\n",
        "  key_store_data = middle_layer.key_store.cpu().numpy()\n",
        "  print(\"Running UMAP projection on the key store... (this may take a moment)\")\n",
        "  reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine',\n",
        "random_state=42)\n",
        "  embedding = reducer.fit_transform(key_store_data)\n",
        "\n",
        "  plt.figure(figsize=(12, 10))\n",
        "\n",
        "   scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=usage_counts, cmap='viridis', s=20,\n",
        "alpha=0.7)\n",
        "   plt.title(f\"UMAP Projection of Expert Key Store ({config['run_name']} - Layer\n",
        "{middle_layer_idx})\")\n",
        "   plt.xlabel('UMAP Dimension 1')\n",
        "   plt.ylabel('UMAP Dimension 2')\n",
        "   plt.colorbar(scatter, label='Expert Usage Count (EMA)')\n",
        "   plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "   plt.show()\n",
        "\n",
        "  # --- 4. Multi-Layer Generative Analysis with Expert Tracing ---\n",
        "  print(\"\\n\\n--- Part 4: Multi-Layer Generative Analysis with Expert Tracing ---\")\n",
        "\n",
        "  # === MODIFIED FUNCTION: Accepts a list of layer indices ===\n",
        "  def generate_with_multi_layer_trace(model, tokenizer, prompt, layer_indices,\n",
        "max_new_tokens=50):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "     # New data structure to hold traces for multiple layers\n",
        "     expert_trace = {layer: [] for layer in layer_indices}\n",
        "\n",
        "     print(f\"\\n--- Prompt ---\\n{prompt}\", end=\"\")\n",
        "\n",
        "     for _ in range(max_new_tokens):\n",
        "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
        "           outputs, all_queries, all_indices = model(generated_ids)\n",
        "\n",
        "       # For each layer we are watching, capture the expert indices for the last token\n",
        "       for layer_idx in layer_indices:\n",
        "          last_token_indices = all_indices[layer_idx][-1, :].cpu().numpy().tolist()\n",
        "          expert_trace[layer_idx].append(last_token_indices)\n",
        "\n",
        "       next_token_logits = outputs[:, -1, :]\n",
        "       next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "       if next_token_id.item() == tokenizer.eos_token_id:\n",
        "           break\n",
        "\n",
        "       generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
        "       new_token = tokenizer.decode(next_token_id[0])\n",
        "       print(new_token, end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\n--- End of Generation ---\")\n",
        "\n",
        "    print(f\"\\n--- Multi-Layer Expert Activation Trace ---\")\n",
        "    num_generated_tokens = len(expert_trace[layer_indices[0]])\n",
        "\n",
        "    # New print loop to group by token for easy comparison\n",
        "    for i in range(num_generated_tokens):\n",
        "       token_text = tokenizer.decode(generated_ids[0, input_ids.shape[1] + i]).replace(' ', '_')\n",
        "       print(f\"\\nToken '{token_text}':\")\n",
        "       for layer_idx in layer_indices:\n",
        "           token_experts = expert_trace[layer_idx][i]\n",
        "           print(f\" Layer {layer_idx}:\\tUsed Experts -> {token_experts}\")\n",
        "\n",
        "   # === MODIFIED CALL: Specify the layers to trace ===\n",
        "   layers_to_trace = [1, 5, 9] # Early, Middle, and Late layers\n",
        "   prompts = [\n",
        "      \"Once upon a time, there was a little fox who\",\n",
        "      \"The recipe for the perfect cake is to first\",\n",
        "      \"The robot opened its eyes and saw\",\n",
        "   ]\n",
        "   for p in prompts:\n",
        "      generate_with_multi_layer_trace(analysis_model, tokenizer, p,\n",
        "layer_indices=layers_to_trace)\n",
        "____________________________________\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}