{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SoME v5 Official Ablations Code (IPYNB)\n",
        "\n",
        "____________________________________\n",
        "Cell One:\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "print(\"--- Part 1: Setup and Dependencies ---\")\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs for a free performance boost\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Enable benchmark mode for cuDNN\n",
        "torch.backends.cudnn.benchmark = True\n",
        "____________________________________\n",
        "Cell Two:\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Core SoME Framework (FIXED)\n",
        "# (Classes, Data Functions, Training Loop)\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "#\n",
        "# Fixes implemented (the \u201cbig 8\u201d):\n",
        "# 1) Mask PAD tokens in loss targets\n",
        "# 2) Prevent attention from attending to PAD (key_padding_mask)\n",
        "# 3) Force a single vocab-size source of truth (tokenizer -> config -> model/loss)\n",
        "# 4) Run routing + key updates in FP32 under AMP (preserve SoME geometry)\n",
        "# 5) Standardize eval temperature (no hidden sharpening)\n",
        "# 6) Beta inertia uses MAX mass (heavy experts resist movement)\n",
        "# 7) Normalize usage/inertia updates by token flux (batch/seq/top_k comparable)\n",
        "# 8) Track inequality both by selection-count and by gating-weight mass\n",
        "#\n",
        "# (Everything else is kept intentionally close to your baseline.)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# --- 1. Model Component Classes ---\n",
        "\n",
        "class Expert(nn.Module):\n",
        "   \"\"\"An expert module with configurable random weight initialization.\"\"\"\n",
        "   def __init__(self, d_model, d_ffn, init_method='default'):\n",
        "      super().__init__()\n",
        "      self.w_down = nn.Linear(d_model, d_ffn)\n",
        "      self.activation = nn.GELU()\n",
        "      self.w_up = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "     if init_method == 'orthogonal':\n",
        "         nn.init.orthogonal_(self.w_down.weight)\n",
        "         nn.init.orthogonal_(self.w_up.weight)\n",
        "     elif init_method == 'sparse':\n",
        "         nn.init.sparse_(self.w_down.weight, sparsity=0.5)\n",
        "         nn.init.sparse_(self.w_up.weight, sparsity=0.5)\n",
        "     elif init_method != 'default':\n",
        "         raise ValueError(f\"Unknown initialization method: {init_method}\")\n",
        "\n",
        "     nn.init.zeros_(self.w_down.bias)\n",
        "     nn.init.zeros_(self.w_up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, some_config):\n",
        "     super().__init__()\n",
        "     self.d_model = d_model\n",
        "     self.num_experts = some_config['num_experts']\n",
        "     self.d_ffn = some_config['d_ffn']\n",
        "     self.top_k = some_config['top_k']\n",
        "\n",
        "    # Heuristic update parameters\n",
        "    self.alpha = some_config['alpha']\n",
        "    self.beta = some_config['beta']\n",
        "    self.delta = some_config['delta']\n",
        "\n",
        "    # Key management parameters\n",
        "    self.theta_percentile = some_config['theta_percentile']\n",
        "    self.warmup_steps = some_config['warmup_steps']\n",
        "    self.ema_decay = some_config['ema_decay']\n",
        "\n",
        "    # Ablation flags\n",
        "    self.ablation_flags = some_config.get(\n",
        "       'ablation_flags',\n",
        "       {'use_alpha': True, 'use_beta': True, 'use_delta': True}\n",
        "    )\n",
        "    print(f\"SoME Layer Ablation Flags: {self.ablation_flags}\")\n",
        "\n",
        "    # Router / query network (trainable)\n",
        "    # Toggle via config['model']['router_type'] in the Ablation/Config cell.\n",
        "    self.router_type = (some_config.get(\"router_type\", \"linear\") or \"linear\").lower()\n",
        "    if self.router_type == \"linear\":\n",
        "        self.query_network = nn.Linear(d_model, d_model, bias=True)\n",
        "    elif self.router_type in (\"mlp\", \"mlp_router\", \"mlp-router\"):\n",
        "        mult = float(some_config.get(\"router_mlp_mult\", 1.0))\n",
        "        hidden = max(1, int(d_model * mult))\n",
        "        dropout = float(some_config.get(\"router_dropout\", 0.0))\n",
        "        layers = [nn.Linear(d_model, hidden, bias=True), nn.GELU()]\n",
        "        if dropout > 0.0:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        layers.append(nn.Linear(hidden, d_model, bias=True))\n",
        "        self.query_network = nn.Sequential(*layers)\n",
        "    else:\n",
        "\n",
        "           raise ValueError(f\"Unknown router_type={self.router_type!r} (use 'linear' or 'mlp')\")\n",
        "        # Init query network (match common transformer init for each Linear)\n",
        "        for m in self.query_network.modules():\n",
        "           if isinstance(m, nn.Linear):\n",
        "               nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "               if m.bias is not None:\n",
        "                  nn.init.zeros_(m.bias)\n",
        "\n",
        "        keys = torch.randn(self.num_experts, d_model)\n",
        "        self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "\n",
        "    # Fix #8 + #7: track both selection-frequency and gating-mass, normalized by token flux\n",
        "    self.register_buffer(\"usage_count\", torch.zeros(self.num_experts)) # EMA of selection\n",
        "probability mass\n",
        "    self.register_buffer(\"usage_mass\", torch.zeros(self.num_experts)) # EMA of gating-weight\n",
        "probability mass\n",
        "\n",
        "        # Fix #14 from earlier notes is not part of the \u201cbig 8\u201d, but we make steps a scalar for clarity.\n",
        "        self.register_buffer(\"steps\", torch.zeros((), dtype=torch.long))\n",
        "\n",
        "        self.experts = nn.ModuleList([\n",
        "           Expert(d_model, self.d_ffn, init_method=some_config['init_method'])\n",
        "           for _ in range(self.num_experts)\n",
        "        ])\n",
        "\n",
        "        # Freeze all expert parameters\n",
        "        for expert in self.experts:\n",
        "           for param in expert.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "        if self.top_k > 1:\n",
        "            self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "\n",
        "        # Fix #4: routing computations in fp32 even under AMP\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "           queries_raw = self.query_network(x_flat.float())\n",
        "           queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "\n",
        "       # cosine-style similarity (keys are unit norm unless delta is enabled; baseline behavior\n",
        "preserved)\n",
        "       scores = torch.matmul(queries, self.key_store.t())\n",
        "\n",
        "       top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "       gating_weights = F.softmax(top_k_scores / float(temperature), dim=-1)\n",
        "\n",
        "     # Expert dispatch (can run under AMP)\n",
        "     flat_top_k_indices = top_k_indices.view(-1)\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "     flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "     permuted_inputs = flat_inputs[permutation_map]\n",
        "     split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "     output_chunks = []\n",
        "     for i, expert_id in enumerate(unique_expert_ids):\n",
        "        output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "     concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "     inverse_permutation_map = torch.argsort(permutation_map)\n",
        "     expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "     # Ensure dtype compatibility in the weighted sum\n",
        "     gating_weights_for_mul = gating_weights.to(expert_outputs.dtype)\n",
        "     weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "gating_weights_for_mul.unsqueeze(-1)).sum(dim=1)\n",
        "     final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "     return x + final_output, queries, top_k_indices, gating_weights\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_keys(self, queries, top_k_indices, gating_weights):\n",
        "    self.steps += 1\n",
        "\n",
        "     # queries: [T, d_model], top_k_indices: [T, top_k], gating_weights: [T, top_k]\n",
        "     num_tokens = top_k_indices.shape[0]\n",
        "     if num_tokens == 0:\n",
        "         return\n",
        "\n",
        "     # -------------------------\n",
        "     # Usage EMAs (raw counts + gating-weight mass)\n",
        "\n",
        "# -------------------------\n",
        "flat_idx = top_k_indices.reshape(-1)\n",
        "unique_indices, counts = torch.unique(flat_idx, return_counts=True)\n",
        "counts_f = counts.to(torch.float32)\n",
        "\n",
        "self.usage_count.mul_(self.ema_decay)\n",
        "self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts_f)\n",
        "\n",
        "if gating_weights is not None:\n",
        "    flat_w = gating_weights.reshape(-1).to(torch.float32)\n",
        "    mass_sums = torch.zeros_like(self.usage_mass)\n",
        "    mass_sums.index_add_(0, flat_idx, flat_w)\n",
        "\n",
        "  self.usage_mass.mul_(self.ema_decay)\n",
        "  self.usage_mass.add_((1.0 - self.ema_decay) * mass_sums)\n",
        "\n",
        "# Inertia source for key updates (count-based)\n",
        "inertia_source = self.usage_count\n",
        "\n",
        "# --- Heuristic 1: Alpha (Attraction) ---\n",
        "if self.ablation_flags.get('use_alpha', True):\n",
        "    for i in range(self.top_k):\n",
        "       indices = top_k_indices[:, i]\n",
        "       inertia = 1.0 + inertia_source[indices]\n",
        "       alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "       update_vec = queries - self.key_store[indices]\n",
        "       self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "# --- Heuristic 2: Beta (Peer Pull / Clustering) ---\n",
        "if self.top_k > 1 and self.ablation_flags.get('use_beta', True):\n",
        "    indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "    indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "    keys_i, keys_j = self.key_store[indices_i], self.key_store[indices_j]\n",
        "\n",
        "  inertia_i = (1.0 + inertia_source[indices_i]).unsqueeze(-1)\n",
        "  inertia_j = (1.0 + inertia_source[indices_j]).unsqueeze(-1)\n",
        "\n",
        "  # Fix #6: heavy experts resist movement (use MAX inertia, not MIN)\n",
        "  beta_effective = self.beta / torch.max(inertia_i, inertia_j)\n",
        "\n",
        "  update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "  update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "  self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "  self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "    # Keep baseline normalization behavior\n",
        "    self.key_store.copy_(F.normalize(self.key_store, p=2, dim=-1))\n",
        "\n",
        "    # --- Heuristic 3: Delta (Decay / \"Use it or Lose it\") ---\n",
        "    # (Left intentionally consistent with your baseline; note: this changes key norms.)\n",
        "    if self.steps.item() > self.warmup_steps and self.ablation_flags.get('use_delta', True):\n",
        "        active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "        if active_usage_counts.numel() > 0:\n",
        "            dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "            low_usage_mask = self.usage_count < dynamic_theta\n",
        "            self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_config):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = SOMELayer(d_model, some_config)\n",
        "\n",
        "   def forward(self, x, attention_mask=None, temperature=1.0):\n",
        "     seq_len = x.size(1)\n",
        "     causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device),\n",
        "diagonal=1).bool()\n",
        "\n",
        "    # Fix #2: prevent attention from seeing PAD positions\n",
        "    key_padding_mask = None\n",
        "    if attention_mask is not None:\n",
        "        key_padding_mask = (attention_mask == 0)\n",
        "\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=causal_mask,\n",
        "key_padding_mask=key_padding_mask)\n",
        "    x = self.norm1(x + attn_output)\n",
        "\n",
        "    some_output, queries, top_k_indices, gating_weights = self.some_layer(x,\n",
        "temperature=temperature)\n",
        "    x = self.norm2(some_output)\n",
        "    return x, queries, top_k_indices, gating_weights\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "\n",
        "    super().__init__()\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "    pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, model_config, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(model_config['vocab_size'], model_config['d_model'])\n",
        "     self.pos_encoder = PositionalEncoding(model_config['d_model'], model_config['seq_len'])\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(model_config['d_model'], model_config['num_heads'],\n",
        "some_config)\n",
        "        for _ in range(model_config['num_layers'])\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(model_config['d_model'], model_config['vocab_size'])\n",
        "     self.d_model = model_config['d_model']\n",
        "\n",
        "  def forward(self, x, attention_mask=None, temperature=1.0):\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)\n",
        "\n",
        "    all_queries, all_indices, all_gates = [], [], []\n",
        "    for layer in self.layers:\n",
        "       x, queries, top_k_indices, gating_weights = layer(x, attention_mask=attention_mask,\n",
        "temperature=temperature)\n",
        "       all_queries.append(queries)\n",
        "       all_indices.append(top_k_indices)\n",
        "       all_gates.append(gating_weights)\n",
        "\n",
        "    return self.fc_out(x), all_queries, all_indices, all_gates\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices, all_gates, token_mask=None):\n",
        "    \"\"\"Update each layer's key store.\n",
        "\n",
        "     token_mask (optional):\n",
        "         - shape [B, T] or [B*T]\n",
        "         - True where the token position is valid for key updates (e.g., targets != -100).\n",
        "     \"\"\"\n",
        "     if token_mask is not None:\n",
        "         if token_mask.dim() == 2:\n",
        "             token_mask = token_mask.reshape(-1)\n",
        "         token_mask = token_mask.to(dtype=torch.bool, device=all_indices[0].device)\n",
        "\n",
        "     for layer, q, idx, g in zip(self.layers, all_queries, all_indices, all_gates):\n",
        "        if token_mask is not None:\n",
        "            if q is not None:\n",
        "                q = q[token_mask]\n",
        "            if idx is not None:\n",
        "                idx = idx[token_mask]\n",
        "            if g is not None:\n",
        "                g = g[token_mask]\n",
        "        layer.some_layer.update_keys(q, idx, g)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   \"\"\"Causal LM dataset with STRICT targets:\n",
        "   - next-token prediction\n",
        "   - ignore PAD targets\n",
        "   - ignore all positions at/after the first EOS in the input (so we don't learn to predict\n",
        "PAD/garbage after EOS)\n",
        "   Returns: (input_ids, targets, attention_mask)\n",
        "   \"\"\"\n",
        "   def __init__(self, tokenized_data, pad_token_id: int, eos_token_id: int = None):\n",
        "       self.data = tokenized_data\n",
        "       self.pad_token_id = pad_token_id\n",
        "       self.eos_token_id = eos_token_id\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
        "    # Use provided attention_mask if available; otherwise derive from PAD.\n",
        "    if \"attention_mask\" in item:\n",
        "\n",
        "        attention_mask = torch.tensor(item[\"attention_mask\"], dtype=torch.long)\n",
        "     else:\n",
        "        attention_mask = (input_ids != self.pad_token_id).long()\n",
        "\n",
        "     # Next-token targets (shift left)\n",
        "     targets = input_ids.clone()\n",
        "     targets[:-1] = input_ids[1:]\n",
        "     targets[-1] = -100 # last position has no next token\n",
        "\n",
        "     # Ignore PAD targets (prevents PAD-dominated objective)\n",
        "     if self.pad_token_id is not None:\n",
        "         targets[targets == self.pad_token_id] = -100\n",
        "\n",
        "     # Ignore targets at/after first EOS in the input (keeps EOS prediction itself!)\n",
        "     if self.eos_token_id is not None:\n",
        "         eos_cum = (input_ids == self.eos_token_id).cumsum(dim=0)\n",
        "         targets[eos_cum > 0] = -100\n",
        "\n",
        "     return input_ids, targets, attention_mask\n",
        "\n",
        "\n",
        "def prepare_data(config):\n",
        "  print(\"\\n--- Part 2: Data Preparation & Configuration ---\")\n",
        "  tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "\n",
        "  if not os.path.exists(tokenizer_path):\n",
        "      print(\"Training custom tokenizer...\")\n",
        "      dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "\n",
        "     def get_training_corpus():\n",
        "       for i in range(0, len(dataset), 1000):\n",
        "          yield dataset[i: i + 1000][\"text\"]\n",
        "\n",
        "     tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "     tokenizer_model.pre_tokenizer = Whitespace()\n",
        "     trainer = BpeTrainer(\n",
        "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"],\n",
        "        vocab_size=config['model']['vocab_size']\n",
        "     )\n",
        "     tokenizer_model.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "     tokenizer_model.save(tokenizer_path)\n",
        "  else:\n",
        "     print(\"Tokenizer already exists. Loading from file.\")\n",
        "\n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "  print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "  # Fix #3: single source of truth for vocab size\n",
        "  config['model']['vocab_size'] = tokenizer.vocab_size\n",
        "\n",
        "  print(\"\\nTokenizing dataset...\")\n",
        "  full_dataset = load_dataset(\"roneneldan/TinyStories\", streaming=False)\n",
        "  train_subset = full_dataset['train'].select(range(config['data']['train_subset_size']))\n",
        "  val_subset = full_dataset['validation'].select(range(config['data']['val_subset_size']))\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    text_with_eos = [s + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "    return tokenizer(\n",
        "       text_with_eos,\n",
        "       truncation=True,\n",
        "       padding=\"max_length\",\n",
        "       max_length=config['model']['seq_len'],\n",
        "       return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "  tokenized_train = train_subset.map(tokenize_function, batched=True,\n",
        "remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
        "  tokenized_val = val_subset.map(tokenize_function, batched=True, remove_columns=[\"text\"],\n",
        "num_proc=os.cpu_count())\n",
        "\n",
        "  train_dataset = LanguageModelDataset(tokenized_train,\n",
        "pad_token_id=tokenizer.pad_token_id)\n",
        "  validation_dataset = LanguageModelDataset(tokenized_val,\n",
        "pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "  num_workers = max(2, os.cpu_count() // 2 if os.cpu_count() else 2)\n",
        "  train_loader = DataLoader(\n",
        "     train_dataset,\n",
        "     batch_size=config['data']['batch_size'],\n",
        "     shuffle=True,\n",
        "     drop_last=True,\n",
        "     num_workers=num_workers,\n",
        "     pin_memory=True\n",
        "  )\n",
        "  validation_loader = DataLoader(\n",
        "     validation_dataset,\n",
        "     batch_size=config['data']['batch_size'],\n",
        "\n",
        "      drop_last=False, # keep full validation set\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "\n",
        "  print(f\"\\nTrain dataset size (subset): {len(train_dataset)}\")\n",
        "  print(f\"Using {num_workers} workers for DataLoader.\")\n",
        "  return train_loader, validation_loader, tokenizer\n",
        "\n",
        "\n",
        "# --- 3. Training & Evaluation Functions ---\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0:\n",
        "      return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0:\n",
        "      return 0.0\n",
        "  probs = usage_counts / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp, vocab_size):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "  for input_ids, targets, attention_mask in progress_bar:\n",
        "     input_ids = input_ids.to(device, non_blocking=True)\n",
        "     targets = targets.to(device, non_blocking=True)\n",
        "     attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "      with torch.cuda.amp.autocast():\n",
        "\n",
        "      logits, queries, indices, gates = model(input_ids, attention_mask=attention_mask,\n",
        "temperature=current_temp)\n",
        "      loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "     scheduler.step()\n",
        "\n",
        "     # Fix #4: ensure key updates happen on the underlying module when compiled\n",
        "     model_state = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
        "     model_state.update_all_keys(queries, indices, gates, token_mask=(targets.view(-1) !=\n",
        "-100))\n",
        "\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, vocab_size, eval_temp: float):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for input_ids, targets, attention_mask in progress_bar:\n",
        "        input_ids = input_ids.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "      with torch.cuda.amp.autocast():\n",
        "         # Fix #5: eval temperature is explicit and standardized\n",
        "         logits, _, _, _ = model(input_ids, attention_mask=attention_mask,\n",
        "temperature=eval_temp)\n",
        "         loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "       total_loss += loss.item()\n",
        "       progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, config):\n",
        "  epochs = len(train_losses)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  title = f\"SoME v3.0 (FIXED) Run: {config['run_name']}\"\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "  filename = f\"loss_curve_{config['run_name']}_fixed.png\"\n",
        "  plt.savefig(filename)\n",
        "  print(f\"\\nLoss curve plot saved to {filename}\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# --- 4. Main Execution Function ---\n",
        "\n",
        "def main(config):\n",
        "  \"\"\"Main function to run a SoME experiment.\"\"\"\n",
        "  print(f\"\\n--- Starting Experiment: {config['run_name']} ---\")\n",
        "\n",
        "  # 1. Data\n",
        "  train_loader, val_loader, tokenizer = prepare_data(config)\n",
        "\n",
        "  # 2. Model Initialization\n",
        "  print(\"\\n--- Part 3: Model Definition ---\")\n",
        "  model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "\n",
        "  if hasattr(torch, 'compile'):\n",
        "      print(\"\\nCompiling the model for faster training...\")\n",
        "      model = torch.compile(model)\n",
        "\n",
        "  # 3. Training Setup\n",
        "  print(\"\\n--- Part 4: Training, Evaluation, and Metrics ---\")\n",
        "  optimizer = torch.optim.AdamW(\n",
        "     [p for p in model.parameters() if p.requires_grad],\n",
        "     lr=config['training']['learning_rate'],\n",
        "     betas=(0.9, 0.95),\n",
        "     weight_decay=0.1\n",
        "\n",
        "  )\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "  total_steps = len(train_loader) * config['training']['num_epochs']\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "   total_params = sum(p.numel() for p in model.parameters())\n",
        "   trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "   print(f\"\\nTotal parameters: {total_params/1e6:.2f}M\")\n",
        "   print(f\"Trainable parameters: {trainable_params/1e6:.2f}M ({100 * trainable_params /\n",
        "total_params:.2f}%)\")\n",
        "   print(f\"Total training steps: {total_steps}\")\n",
        "   print(f\"Using expert initialization method: {config['some_layer']['init_method']}\")\n",
        "\n",
        "  # Fix #5: eval temperature is explicit (defaults to training temperature)\n",
        "  eval_temp = float(config['training'].get('eval_temp', config['training']['training_temp']))\n",
        "\n",
        "  # 4. Training Loop\n",
        "  train_losses, val_losses = [], []\n",
        "  best_val_loss = float('inf')\n",
        "  model_save_path = f\"best_model_{config['run_name']}_fixed.pth\"\n",
        "\n",
        "  for epoch in range(config['training']['num_epochs']):\n",
        "     print(f\"\\n--- Epoch {epoch + 1}/{config['training']['num_epochs']} ---\")\n",
        "\n",
        "    train_loss = train_epoch(\n",
        "       model, train_loader, optimizer, criterion, scheduler,\n",
        "       config['training']['training_temp'],\n",
        "       vocab_size=config['model']['vocab_size']\n",
        "    )\n",
        "    val_loss = evaluate_epoch(model, val_loader, criterion,\n",
        "vocab_size=config['model']['vocab_size'], eval_temp=eval_temp)\n",
        "    perplexity = math.exp(val_loss)\n",
        "\n",
        "     train_losses.append(train_loss)\n",
        "     val_losses.append(val_loss)\n",
        "\n",
        "     model_to_inspect = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "     middle = config['model']['num_layers'] // 2\n",
        "     layer = model_to_inspect.layers[middle].some_layer\n",
        "\n",
        "     # Fix #8: report both inequality views\n",
        "     gini_count = calculate_gini(layer.usage_count)\n",
        "     entropy_count = calculate_entropy(layer.usage_count)\n",
        "\n",
        "     gini_mass = calculate_gini(layer.usage_mass)\n",
        "     entropy_mass = calculate_entropy(layer.usage_mass)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val\n",
        "Perplexity = {perplexity:.2f}\")\n",
        "    print(f\" Middle Layer (count): Gini = {gini_count:.3f}, Entropy = {entropy_count:.3f}\")\n",
        "    print(f\" Middle Layer (mass): Gini = {gini_mass:.3f}, Entropy = {entropy_mass:.3f}\")\n",
        "\n",
        "     if val_loss < best_val_loss:\n",
        "         best_val_loss = val_loss\n",
        "         torch.save(model_to_inspect.state_dict(), model_save_path)\n",
        "         print(f\" Model saved as {model_save_path}\")\n",
        "\n",
        "  # 5. Finalization\n",
        "  print(f\"\\n--- Training Complete for {config['run_name']} ---\")\n",
        "  plot_losses(train_losses, val_losses, config)\n",
        "____________________________________\n",
        "Cell Three:\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Experiment Configuration & Execution (FIXED)\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "# --- Router selection (toggle) ---\n",
        "ROUTER_TYPE = \"mlp\" # \"linear\" or \"mlp\"\n",
        "ROUTER_MLP_MULT = 2.0 # only used for MLP: hidden = int(d_model *\n",
        "ROUTER_MLP_MULT)\n",
        "ROUTER_DROPOUT = 0.0 # optional dropout inside the router MLP\n",
        "\n",
        "config = {\n",
        "  \"run_name\": \"v3_baseline_orthogonal\",\n",
        "\n",
        "  \"data\": {\n",
        "     \"train_subset_size\": 10000,\n",
        "     \"val_subset_size\": 1000,\n",
        "     \"batch_size\": 24,\n",
        "  },\n",
        "\n",
        "  \"model\": {\n",
        "    # NOTE: vocab_size will be overwritten to tokenizer.vocab_size in prepare_data() (Fix #3)\n",
        "    \"vocab_size\": 8192,\n",
        "\n",
        "         \"d_model\": 512,\n",
        "         \"num_heads\": 8,\n",
        "         \"num_layers\": 10,\n",
        "         \"seq_len\": 1024,\n",
        "    },\n",
        "\n",
        "    \"some_layer\": {\n",
        "      \"num_experts\": 128,\n",
        "      \"d_ffn\": 2048,\n",
        "      \"top_k\": 8,\n",
        "      \"init_method\": \"sparse\",\n",
        "\n",
        "         \"alpha\": 0.005,\n",
        "         \"beta\": 0.001,\n",
        "         \"delta\": 0.001,\n",
        "\n",
        "         \"theta_percentile\": 0.05,\n",
        "         \"warmup_steps\": 400,\n",
        "         \"ema_decay\": 0.995,\n",
        "            \"router_type\": ROUTER_TYPE,\n",
        "            \"router_mlp_mult\": ROUTER_MLP_MULT,\n",
        "            \"router_dropout\": ROUTER_DROPOUT,\n",
        "\n",
        "\n",
        "         \"ablation_flags\": {\n",
        "           \"use_alpha\": True,\n",
        "           \"use_beta\": True,\n",
        "           \"use_delta\": True\n",
        "         }\n",
        "    },\n",
        "\n",
        "    \"training\": {\n",
        "       \"num_epochs\": 4,\n",
        "       \"learning_rate\": 6e-4,\n",
        "       \"training_temp\": 1.0,\n",
        "       # Fix #5: explicit eval temperature (defaults to training_temp if omitted)\n",
        "       \"eval_temp\": 1.0,\n",
        "    }\n",
        "}\n",
        "\n",
        "main(config)\n",
        "____________________________________\n",
        "Cell Four:\n",
        "\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Multi-Layer SoME Analysis & Diagnostics Dashboard v1.3 (FIXED)\n",
        "#\n",
        "# RUN THIS CELL AFTER A TRAINING RUN (CELL 3) IS COMPLETE\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "print(\"\\n--- Part 1: Dashboard Setup ---\")\n",
        "!pip install umap-learn seaborn -q\n",
        "import umap\n",
        "import seaborn as sns\n",
        "\n",
        "model_path_to_load = f\"best_model_{config['run_name']}_fixed.pth\"\n",
        "tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "\n",
        "if os.path.exists(model_path_to_load) and os.path.exists(tokenizer_path):\n",
        "    print(f\"Loading best model from: {model_path_to_load}\")\n",
        "    print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
        "\n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "\n",
        "  # Fix #3: keep analysis model vocab consistent with tokenizer\n",
        "  config['model']['vocab_size'] = tokenizer.vocab_size\n",
        "\n",
        "   analysis_model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "   analysis_model.load_state_dict(torch.load(model_path_to_load, map_location=device))\n",
        "   analysis_model.eval()\n",
        "else:\n",
        "   print(\"ERROR: Could not find a saved model or tokenizer. Please run a training cell first.\")\n",
        "   analysis_model = None\n",
        "   tokenizer = None\n",
        "\n",
        "if analysis_model and tokenizer:\n",
        "    print(\"\\n\\n--- Part 2: Aggregate Utilization Analysis (from Middle Layer) ---\")\n",
        "\n",
        "  middle_layer_idx = config['model']['num_layers'] // 2\n",
        "  middle_layer = analysis_model.layers[middle_layer_idx].some_layer\n",
        "\n",
        "  usage_count = middle_layer.usage_count.detach().cpu()\n",
        "  usage_mass = middle_layer.usage_mass.detach().cpu()\n",
        "\n",
        "  total_experts = middle_layer.num_experts\n",
        "\n",
        "   used_experts = torch.sum(usage_count > 0).item()\n",
        "   usage_percentage = (used_experts / total_experts) * 100\n",
        "   print(f\"Expert Usage (Layer {middle_layer_idx}): {used_experts}/{total_experts}\n",
        "({usage_percentage:.2f}%)\")\n",
        "\n",
        "  gini_count = calculate_gini(usage_count)\n",
        "  ent_count = calculate_entropy(usage_count)\n",
        "  gini_mass = calculate_gini(usage_mass)\n",
        "  ent_mass = calculate_entropy(usage_mass)\n",
        "\n",
        "  print(f\"Final Metrics (count) Layer {middle_layer_idx}: Gini={gini_count:.4f},\n",
        "Entropy={ent_count:.4f} (Max={math.log2(total_experts):.4f})\")\n",
        "  print(f\"Final Metrics (mass) Layer {middle_layer_idx}: Gini={gini_mass:.4f},\n",
        "Entropy={ent_mass:.4f} (Max={math.log2(total_experts):.4f})\")\n",
        "\n",
        "  # Histogram (count)\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.hist(usage_count.numpy(), bins=50)\n",
        "  plt.yscale('log')\n",
        "  plt.title(f\"Log-Scale Histogram of Expert Usage (COUNT) ({config['run_name']} - Layer\n",
        "{middle_layer_idx})\")\n",
        "  plt.xlabel('EMA selection mass')\n",
        "  plt.ylabel('Number of Experts (Log Scale)')\n",
        "  plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "  plt.show()\n",
        "\n",
        "  # Histogram (mass)\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.hist(usage_mass.numpy(), bins=50)\n",
        "  plt.yscale('log')\n",
        "  plt.title(f\"Log-Scale Histogram of Expert Usage (MASS) ({config['run_name']} - Layer\n",
        "{middle_layer_idx})\")\n",
        "  plt.xlabel('EMA gating-weight mass')\n",
        "  plt.ylabel('Number of Experts (Log Scale)')\n",
        "  plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "  plt.show()\n",
        "\n",
        "  # --- 3. Key Store Structure Visualization (middle layer) ---\n",
        "  print(\"\\n\\n--- Part 3: Key Store Structure Visualization (from Middle Layer) ---\")\n",
        "  key_store_data = middle_layer.key_store.detach().cpu().numpy()\n",
        "  print(\"Running UMAP projection on the key store... (this may take a moment)\")\n",
        "\n",
        "  reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine',\n",
        "random_state=42)\n",
        "  embedding = reducer.fit_transform(key_store_data)\n",
        "\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=usage_mass, s=20, alpha=0.7)\n",
        "  plt.title(f\"UMAP Projection of Expert Key Store ({config['run_name']} - Layer\n",
        "{middle_layer_idx})\")\n",
        "  plt.xlabel('UMAP Dimension 1')\n",
        "  plt.ylabel('UMAP Dimension 2')\n",
        "  plt.colorbar(scatter, label='Expert Usage (EMA gating-weight mass)')\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "  plt.show()\n",
        "\n",
        "  # --- 4. Multi-Layer Generative Analysis with Expert Tracing ---\n",
        "  print(\"\\n\\n--- Part 4: Multi-Layer Generative Analysis with Expert Tracing ---\")\n",
        "\n",
        "  def generate_with_multi_layer_trace(model, tokenizer, prompt, layer_indices,\n",
        "max_new_tokens=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    expert_trace = {layer: [] for layer in layer_indices}\n",
        "\n",
        "    print(f\"\\n--- Prompt ---\\n{prompt}\", end=\"\")\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "       with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
        "          outputs, _, all_indices, _ = model(generated_ids, attention_mask=None,\n",
        "temperature=temperature)\n",
        "\n",
        "       for layer_idx in layer_indices:\n",
        "          last_token_indices = all_indices[layer_idx][-1, :].detach().cpu().numpy().tolist()\n",
        "          expert_trace[layer_idx].append(last_token_indices)\n",
        "\n",
        "       next_token_logits = outputs[:, -1, :]\n",
        "       next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "       if next_token_id.item() == tokenizer.eos_token_id:\n",
        "           break\n",
        "\n",
        "       generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
        "       new_token = tokenizer.decode(next_token_id[0])\n",
        "\n",
        "       print(new_token, end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\n--- End of Generation ---\")\n",
        "\n",
        "    print(\"\\n--- Multi-Layer Expert Activation Trace ---\")\n",
        "    num_generated_tokens = len(expert_trace[layer_indices[0]])\n",
        "    for i in range(num_generated_tokens):\n",
        "       token_text = tokenizer.decode(generated_ids[0, input_ids.shape[1] + i]).replace(' ', '_')\n",
        "       print(f\"\\nToken '{token_text}':\")\n",
        "       for layer_idx in layer_indices:\n",
        "           print(f\" Layer {layer_idx}:\\tUsed Experts -> {expert_trace[layer_idx][i]}\")\n",
        "\n",
        "   layers_to_trace = [1, 5, 9]\n",
        "   prompts = [\n",
        "      \"Once upon a time, there was a little fox who\",\n",
        "      \"The recipe for the perfect cake is to first\",\n",
        "      \"The robot opened its eyes and saw\",\n",
        "   ]\n",
        "   for p in prompts:\n",
        "      generate_with_multi_layer_trace(analysis_model, tokenizer, p,\n",
        "layer_indices=layers_to_trace, temperature=1.0)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}