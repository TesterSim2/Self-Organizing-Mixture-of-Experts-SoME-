{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SoME v3.5 (Decay Fix) Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "print(\"--- Part 1: Setup and Dependencies ---\")\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib umap-learn\n",
        "seaborn -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs (Crucial for 40GB optimization)\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "    print(\"A100 GPU detected. Enabling TF32.\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Enable benchmark mode for cuDNN\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "____________________________________\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Core SoME Framework\n",
        "# (Classes, Data Functions, Training Loop)\n",
        "\n",
        "# --- 1. Model Component Classes ---\n",
        "\n",
        "class Expert(nn.Module):\n",
        "   \"\"\"An expert module with configurable random weight initialization.\"\"\"\n",
        "\n",
        "  def __init__(self, d_model, d_ffn, init_method='default'):\n",
        "    super().__init__()\n",
        "    self.w_down = nn.Linear(d_model, d_ffn)\n",
        "    self.activation = nn.GELU()\n",
        "    self.w_up = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "    if init_method == 'orthogonal':\n",
        "        nn.init.orthogonal_(self.w_down.weight)\n",
        "        nn.init.orthogonal_(self.w_up.weight)\n",
        "    elif init_method == 'sparse':\n",
        "        nn.init.sparse_(self.w_down.weight, sparsity=0.5)\n",
        "        nn.init.sparse_(self.w_up.weight, sparsity=0.5)\n",
        "    elif init_method != 'default':\n",
        "        raise ValueError(f\"Unknown initialization method: {init_method}\")\n",
        "\n",
        "    nn.init.zeros_(self.w_down.bias)\n",
        "    nn.init.zeros_(self.w_up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, some_config):\n",
        "     super().__init__()\n",
        "     self.d_model = d_model\n",
        "     self.num_experts = some_config['num_experts']\n",
        "     self.d_ffn = some_config['d_ffn']\n",
        "     self.top_k = some_config['top_k']\n",
        "\n",
        "    # Heuristic update parameters\n",
        "    self.alpha = some_config['alpha']\n",
        "    self.beta = some_config['beta']\n",
        "    self.delta = some_config['delta']\n",
        "\n",
        "    # v4 Feature: Respawn Threshold\n",
        "    self.respawn_threshold = some_config.get('respawn_threshold', 0.1)\n",
        "\n",
        "    # Key management parameters\n",
        "    self.theta_percentile = some_config['theta_percentile']\n",
        "    self.warmup_steps = some_config['warmup_steps']\n",
        "    self.ema_decay = some_config['ema_decay']\n",
        "\n",
        "    self.ablation_flags = some_config.get('ablation_flags',\n",
        "      {'use_alpha': True, 'use_beta': True, 'use_delta': True})\n",
        "\n",
        "    self.query_network = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Initialize keys on unit sphere\n",
        "    keys = torch.randn(self.num_experts, d_model)\n",
        "    self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "    self.register_buffer(\"usage_count\", torch.zeros(self.num_experts))\n",
        "    self.register_buffer(\"steps\", torch.tensor([0], dtype=torch.long))\n",
        "\n",
        "    self.experts = nn.ModuleList([Expert(d_model, self.d_ffn,\n",
        "      init_method=some_config['init_method']) for _ in range(self.num_experts)])\n",
        "\n",
        "    # IN-INFERENCE PROTOCOL: Freeze all expert parameters\n",
        "    # We are teaching the router to use random tools.\n",
        "    for expert in self.experts:\n",
        "       for param in expert.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "    if self.top_k > 1:\n",
        "        self.register_buffer(\"peer_pull_indices\",\n",
        "           torch.combinations(torch.arange(self.top_k), r=2))\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "\n",
        "    queries_raw = self.query_network(x_flat)\n",
        "    queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "\n",
        "    # Dot product scores.\n",
        "    # v4 Fix: We rely on the magnitude of the key for \"health\".\n",
        "    # Decayed keys will naturally have low dot products.\n",
        "    scores = torch.matmul(queries, self.key_store.t())\n",
        "\n",
        "    top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "    gating_weights = F.softmax(top_k_scores / temperature, dim=-1)\n",
        "\n",
        "    flat_top_k_indices = top_k_indices.view(-1)\n",
        "\n",
        "     # Standard MoE Routing Logic\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "  flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "  permuted_inputs = flat_inputs[permutation_map]\n",
        "  split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "  output_chunks = []\n",
        "  for i, expert_id in enumerate(unique_expert_ids):\n",
        "     output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "  concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "  inverse_permutation_map = torch.argsort(permutation_map)\n",
        "  expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "  weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "             gating_weights.unsqueeze(-1)).sum(dim=1)\n",
        "  final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "  return x + final_output, queries, top_k_indices\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_keys(self, queries, top_k_indices):\n",
        "  self.steps += 1\n",
        "\n",
        "  # Update usage counts\n",
        "  unique_indices, counts = torch.unique(top_k_indices, return_counts=True)\n",
        "  self.usage_count.mul_(self.ema_decay)\n",
        "  self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts.float())\n",
        "\n",
        "  # --- Heuristic 1: Alpha (Attraction) ---\n",
        "  if self.ablation_flags.get('use_alpha', True):\n",
        "      for i in range(self.top_k):\n",
        "         indices = top_k_indices[:, i]\n",
        "         # Calculate inertia\n",
        "         inertia = 1.0 + self.usage_count[indices]\n",
        "         alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "\n",
        "       # Update: Move key towards query\n",
        "       # Logic: k_new = k + alpha * (q - k)\n",
        "       update_vec = queries - self.key_store[indices]\n",
        "       self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "  # --- Heuristic 2: Beta (Peer Pull) ---\n",
        "  if self.top_k > 1 and self.ablation_flags.get('use_beta', True):\n",
        "      indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "      indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "\n",
        "       keys_i, keys_j = self.key_store[indices_i], self.key_store[indices_j]\n",
        "\n",
        "       inertia_i = (1.0 + self.usage_count[indices_i]).unsqueeze(-1)\n",
        "       inertia_j = (1.0 + self.usage_count[indices_j]).unsqueeze(-1)\n",
        "       beta_effective = self.beta / torch.min(inertia_i, inertia_j)\n",
        "\n",
        "       update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "       update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "\n",
        "       self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "       self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "    # --- Heuristic 3: Delta (Decay) ---\n",
        "    # v4 Fix: We removed the global F.normalize logic.\n",
        "    # Now, shrinkage is permanent until the expert is used or respawned.\n",
        "    if self.steps > self.warmup_steps and self.ablation_flags.get('use_delta', True):\n",
        "        active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "        if active_usage_counts.numel() > 0:\n",
        "            dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "            low_usage_mask = self.usage_count < dynamic_theta\n",
        "\n",
        "         # Shrink the vectors of unused experts\n",
        "         self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "    # --- New: The Phoenix Mechanism (Respawn) ---\n",
        "    # Check for dead experts (norm < threshold) and reincarnate them\n",
        "    key_norms = self.key_store.norm(p=2, dim=-1)\n",
        "    dead_mask = key_norms < self.respawn_threshold\n",
        "\n",
        "     if dead_mask.any():\n",
        "         num_dead = dead_mask.sum().item()\n",
        "         # Pick random queries from the current batch to seed the new experts\n",
        "         flat_queries = queries.view(-1, self.d_model)\n",
        "         if flat_queries.size(0) >= num_dead:\n",
        "             rand_indices = torch.randperm(flat_queries.size(0),\n",
        "device=queries.device)[:num_dead]\n",
        "             new_keys = flat_queries[rand_indices].clone()\n",
        "\n",
        "         # Add slight noise and normalize to unit sphere\n",
        "         new_keys = F.normalize(new_keys + torch.randn_like(new_keys) * 0.01, p=2, dim=-1)\n",
        "\n",
        "         # Assign new keys\n",
        "         self.key_store[dead_mask] = new_keys\n",
        "\n",
        "          # Reset usage stats for respawned experts (give them a chance)\n",
        "          self.usage_count[dead_mask] = 0.0\n",
        "\n",
        "class SOMETransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_config):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "     self.some_layer = SOMELayer(d_model, some_config)\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    seq_len = x.size(1)\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device) * float('-inf'), diagonal=1)\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "    x = self.norm1(x + attn_output)\n",
        "\n",
        "    # Pass temperature to the SOME layer\n",
        "    some_output, queries, top_k_indices = self.some_layer(x, temperature=temperature)\n",
        "    x = self.norm2(some_output)\n",
        "    return x, queries, top_k_indices\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "     super().__init__()\n",
        "     position = torch.arange(max_len).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "     pe = torch.zeros(1, max_len, d_model)\n",
        "     pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "     pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "     self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class SOMETransformer(nn.Module):\n",
        "   def __init__(self, model_config, some_config):\n",
        "     super().__init__()\n",
        "     self.embedding = nn.Embedding(model_config['vocab_size'], model_config['d_model'])\n",
        "     self.pos_encoder = PositionalEncoding(model_config['d_model'], model_config['seq_len'])\n",
        "     self.layers = nn.ModuleList([\n",
        "        SOMETransformerBlock(model_config['d_model'], model_config['num_heads'],\n",
        "some_config)\n",
        "\n",
        "        for _ in range(model_config['num_layers'])\n",
        "     ])\n",
        "     self.fc_out = nn.Linear(model_config['d_model'], model_config['vocab_size'])\n",
        "     self.d_model = model_config['d_model']\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)\n",
        "    all_queries, all_indices = [], []\n",
        "    for layer in self.layers:\n",
        "       x, queries, top_k_indices = layer(x, temperature=temperature)\n",
        "       all_queries.append(queries)\n",
        "       all_indices.append(top_k_indices)\n",
        "    return self.fc_out(x), all_queries, all_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices):\n",
        "    for i, layer_block in enumerate(self.layers):\n",
        "       queries = all_queries[i].view(-1, layer_block.some_layer.d_model)\n",
        "       indices = all_indices[i].view(-1, layer_block.some_layer.top_k)\n",
        "       layer_block.some_layer.update_keys(queries, indices)\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data):\n",
        "     self.data = tokenized_data\n",
        "   def __len__(self):\n",
        "     return len(self.data)\n",
        "   def __getitem__(self, idx):\n",
        "     item = self.data[idx]\n",
        "     inputs = torch.tensor(item['input_ids'])\n",
        "     targets = inputs.clone()\n",
        "     targets[:-1] = inputs[1:]\n",
        "     targets[-1] = -100\n",
        "     return inputs, targets\n",
        "\n",
        "def prepare_data(config):\n",
        "  print(\"\\n--- Part 2: Data Preparation & Configuration ---\")\n",
        "  tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "  if not os.path.exists(tokenizer_path):\n",
        "      print(\"Training custom tokenizer...\")\n",
        "      dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "      def get_training_corpus():\n",
        "\n",
        "        for i in range(0, len(dataset), 1000):\n",
        "           yield dataset[i : i + 1000][\"text\"]\n",
        "     tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "     tokenizer_model.pre_tokenizer = Whitespace()\n",
        "     trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"],\n",
        "                    vocab_size=config['model']['vocab_size'])\n",
        "     tokenizer_model.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "     tokenizer_model.save(tokenizer_path)\n",
        "  else:\n",
        "     print(\"Tokenizer already exists. Loading from file.\")\n",
        "\n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "  print(f\"Custom tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "  print(\"\\nTokenizing dataset...\")\n",
        "  full_dataset = load_dataset(\"roneneldan/TinyStories\", streaming=False)\n",
        "  train_subset = full_dataset['train'].select(range(config['data']['train_subset_size']))\n",
        "  val_subset = full_dataset['validation'].select(range(config['data']['val_subset_size']))\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    text_with_eos = [s + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "    return tokenizer(text_with_eos, truncation=True, padding=\"max_length\",\n",
        "               max_length=config['model']['seq_len'], return_tensors=\"pt\")\n",
        "\n",
        "  tokenized_train = train_subset.map(tokenize_function, batched=True,\n",
        "                         remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
        "  tokenized_val = val_subset.map(tokenize_function, batched=True,\n",
        "                      remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
        "\n",
        "  train_dataset = LanguageModelDataset(tokenized_train)\n",
        "  validation_dataset = LanguageModelDataset(tokenized_val)\n",
        "\n",
        "   num_workers = max(2, os.cpu_count() // 2 if os.cpu_count() else 2)\n",
        "   train_loader = DataLoader(train_dataset, batch_size=config['data']['batch_size'],\n",
        "                  shuffle=True, drop_last=True, num_workers=num_workers,\n",
        "pin_memory=True)\n",
        "   validation_loader = DataLoader(validation_dataset, batch_size=config['data']['batch_size'],\n",
        "                     drop_last=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "  return train_loader, validation_loader, tokenizer\n",
        "\n",
        "# --- 3. Training & Evaluation Functions ---\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0: return 0.0\n",
        "  probs = usage_counts / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp,\n",
        "tokenizer_vocab_size):\n",
        "   model.train()\n",
        "   total_loss = 0\n",
        "   scaler = torch.cuda.amp.GradScaler()\n",
        "   progress_bar = tqdm(dataloader, desc=f\"Training (Temp={current_temp:.2f})\", leave=False)\n",
        "\n",
        "  for inputs, targets in progress_bar:\n",
        "     inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "\n",
        "     with torch.cuda.amp.autocast():\n",
        "        # Pass annealing temperature to model\n",
        "        logits, queries, indices = model(inputs, temperature=current_temp)\n",
        "        loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "     scheduler.step()\n",
        "\n",
        "     # Update Keys (In-Inference Plasticity)\n",
        "     model.update_all_keys(queries, indices)\n",
        "\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, tokenizer_vocab_size):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "           # Sharpen during eval (low temp)\n",
        "           logits, _, _ = model(inputs, temperature=0.5)\n",
        "           loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_losses(train_losses, val_losses, config):\n",
        "  epochs = len(train_losses)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  title = f\"SoME v4.0 Run: {config['run_name']}\"\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "  filename = f\"loss_curve_{config['run_name']}.png\"\n",
        "  plt.savefig(filename)\n",
        "  plt.show()\n",
        "\n",
        "# --- 4. Main Execution Function ---\n",
        "\n",
        "def main(config):\n",
        "  print(f\"\\n--- Starting Experiment: {config['run_name']} ---\")\n",
        "\n",
        "  # 1. Data\n",
        "  train_loader, val_loader, tokenizer = prepare_data(config)\n",
        "\n",
        "  # 2. Model Initialization\n",
        "  print(\"\\n--- Part 3: Model Definition ---\")\n",
        "  model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "\n",
        "  if hasattr(torch, 'compile'):\n",
        "      print(\"\\nCompiling the model for faster training...\")\n",
        "      model = torch.compile(model)\n",
        "\n",
        "  # 3. Training Setup\n",
        "  print(\"\\n--- Part 4: Training, Evaluation, and Metrics ---\")\n",
        "  # Only optimize parameters that require grad (Router/Keys, NOT Experts)\n",
        "  optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "                      lr=config['training']['learning_rate'],\n",
        "                      betas=(0.9, 0.95), weight_decay=0.1)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "  total_steps = len(train_loader) * config['training']['num_epochs']\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "   total_params = sum(p.numel() for p in model.parameters())\n",
        "   trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "   print(f\"\\nTotal parameters: {total_params/1e6:.2f}M\")\n",
        "   print(f\"Trainable parameters: {trainable_params/1e6:.2f}M ({100 * trainable_params /\n",
        "total_params:.2f}%)\")\n",
        "   print(f\"Total training steps: {total_steps}\")\n",
        "\n",
        "  # 4. Training Loop with Annealing\n",
        "  train_losses, val_losses = [], []\n",
        "  best_val_loss = float('inf')\n",
        "  model_save_path = f\"best_model_{config['run_name']}.pth\"\n",
        "\n",
        "  # Temperature Schedule: High Entropy (2.0) -> Low Entropy (Configured)\n",
        "  start_temp = 2.0\n",
        "  end_temp = config['training']['training_temp']\n",
        "\n",
        "  for epoch in range(config['training']['num_epochs']):\n",
        "     print(f\"\\n--- Epoch {epoch+1}/{config['training']['num_epochs']} ---\")\n",
        "\n",
        "     # Calculate current temperature\n",
        "     progress = epoch / config['training']['num_epochs']\n",
        "     current_temp = start_temp - (start_temp - end_temp) * progress\n",
        "     print(f\"Current Router Temperature: {current_temp:.4f}\")\n",
        "\n",
        "     train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler,\n",
        "                     current_temp, tokenizer.vocab_size)\n",
        "\n",
        "     val_loss = evaluate_epoch(model, val_loader, criterion, tokenizer.vocab_size)\n",
        "     perplexity = math.exp(val_loss)\n",
        "\n",
        "     train_losses.append(train_loss)\n",
        "     val_losses.append(val_loss)\n",
        "\n",
        "     # Metrics Inspection (Middle Layer)\n",
        "     model_to_inspect = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "     mid_layer = model_to_inspect.layers[config['model']['num_layers'] // 2].some_layer\n",
        "     usage_counts = mid_layer.usage_count\n",
        "     gini_coeff = calculate_gini(usage_counts)\n",
        "     entropy_val = calculate_entropy(usage_counts)\n",
        "\n",
        "    # Check Phoenix Activity\n",
        "    dead_experts = (mid_layer.key_store.norm(dim=-1) <\n",
        "mid_layer.respawn_threshold).sum().item()\n",
        "\n",
        "     print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Ppl =\n",
        "{perplexity:.2f}\")\n",
        "     print(f\" Middle Layer Metrics: Gini = {gini_coeff:.3f}, Entropy = {entropy_val:.3f}, Dead\n",
        "Experts Pending Respawn: {dead_experts}\")\n",
        "\n",
        "     if val_loss < best_val_loss:\n",
        "         best_val_loss = val_loss\n",
        "         torch.save(model_to_inspect.state_dict(), model_save_path)\n",
        "         print(f\" Model saved as {model_save_path}\")\n",
        "\n",
        "  # 5. Finalization\n",
        "  print(f\"\\n--- Training Complete for {config['run_name']} ---\")\n",
        "  plot_losses(train_losses, val_losses, config)\n",
        "____________________________________\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Experiment Configuration & Execution\n",
        "\n",
        "# --- Define the configuration for the experiment ---\n",
        "config = {\n",
        "   \"run_name\": \"v4_C2_WidthFix_Phoenix\",\n",
        "\n",
        "  \"data\": {\n",
        "    \"train_subset_size\": 10000,\n",
        "    \"val_subset_size\": 1000,\n",
        "    \"batch_size\": 32, # Adjusted for C2 scale\n",
        "\n",
        "    },\n",
        "\n",
        "    \"model\": {\n",
        "       \"vocab_size\": 8192,\n",
        "       # C2 Dimensions: Width 768 caused collapse in v3\n",
        "       \"d_model\": 768,\n",
        "       \"num_heads\": 12,\n",
        "       \"num_layers\": 8,\n",
        "       \"seq_len\": 768,\n",
        "    },\n",
        "\n",
        "    \"some_layer\": {\n",
        "      \"num_experts\": 128,\n",
        "      \"d_ffn\": 1536,\n",
        "      \"top_k\": 4,\n",
        "      \"init_method\": \"sparse\",\n",
        "\n",
        "         # v4 Heuristics\n",
        "         \"alpha\": 0.015,\n",
        "         \"beta\": 0.001,\n",
        "         \"delta\": 0.001,\n",
        "         \"respawn_threshold\": 0.1, # <--- NEW: Expert death threshold\n",
        "\n",
        "         \"theta_percentile\": 0.05,\n",
        "         \"warmup_steps\": 400,\n",
        "         \"ema_decay\": 0.995,\n",
        "\n",
        "         \"ablation_flags\": {\n",
        "           \"use_alpha\": True,\n",
        "           \"use_beta\": True,\n",
        "           \"use_delta\": True\n",
        "         }\n",
        "    },\n",
        "\n",
        "    \"training\": {\n",
        "       \"num_epochs\": 2,\n",
        "       \"learning_rate\": 6e-4,\n",
        "       \"training_temp\": 1.0, # Annealing will go 2.0 -> 1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Run the experiment ---\n",
        "main(config)\n",
        "____________________________________\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Multi-Layer SoME Analysis & Diagnostics Dashboard v1.3\n",
        "# RUN THIS CELL AFTER A TRAINING RUN\n",
        "\n",
        "print(\"\\n--- Part 1: Dashboard Setup ---\")\n",
        "import umap\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the latest configuration is loaded\n",
        "model_path_to_load = f\"best_model_{config['run_name']}.pth\"\n",
        "tokenizer_path = \"tinystories-tokenizer-v2.json\"\n",
        "\n",
        "if os.path.exists(model_path_to_load) and os.path.exists(tokenizer_path):\n",
        "    print(f\"Loading best model from: {model_path_to_load}\")\n",
        "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "\n",
        "  analysis_model = SOMETransformer(config['model'], config['some_layer']).to(device)\n",
        "  analysis_model.load_state_dict(torch.load(model_path_to_load))\n",
        "  analysis_model.eval()\n",
        "\n",
        "  # --- Aggregate Utilization Analysis ---\n",
        "  middle_layer_idx = config['model']['num_layers'] // 2\n",
        "  middle_layer = analysis_model.layers[middle_layer_idx].some_layer\n",
        "  usage_counts = middle_layer.usage_count.cpu()\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.hist(usage_counts.numpy(), bins=50, color='skyblue', edgecolor='black')\n",
        "  plt.yscale('log')\n",
        "  plt.title(f\"Expert Usage Counts ({config['run_name']} - Layer {middle_layer_idx})\")\n",
        "  plt.show()\n",
        "\n",
        "  # --- Key Store UMAP ---\n",
        "  print(\"Running UMAP projection...\")\n",
        "  key_store_data = middle_layer.key_store.cpu().numpy()\n",
        "  reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine')\n",
        "  embedding = reducer.fit_transform(key_store_data)\n",
        "\n",
        "   plt.figure(figsize=(12, 10))\n",
        "   plt.scatter(embedding[:, 0], embedding[:, 1], c=usage_counts, cmap='viridis', s=20, alpha=0.7)\n",
        "   plt.colorbar(label='Usage Count')\n",
        "   plt.title(f\"UMAP of Expert Keys (Color=Usage)\")\n",
        "   plt.show()\n",
        "else:\n",
        "   print(\"Model file not found. Run training first.\")\n",
        "\n",
        "____________________________________\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}