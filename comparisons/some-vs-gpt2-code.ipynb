{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n",
        "========================================================================\n",
        "======\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "print(\"--- Part 1: Setup and Dependencies ---\")\n",
        "!pip install torch datasets transformers huggingface_hub tokenizers matplotlib -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "try:\n",
        "   drive.mount('/content/drive')\n",
        "   DRIVE_MOUNTED = True\n",
        "   print(\"Google Drive mounted successfully.\")\n",
        "except:\n",
        "   DRIVE_MOUNTED = False\n",
        "   print(\"Could not mount Google Drive. Results will be saved to the local Colab runtime.\")\n",
        "\n",
        "# Verify that a GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable TF32 for A100 GPUs for a free performance boost\n",
        "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
        "\n",
        "  print(\"A100 GPU detected. Enabling TF32.\")\n",
        "  torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Enable benchmark mode for cuDNN for potentially faster convolutions and RNNs\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "print(\"\\nSetup complete.\")\n",
        "____________________________________\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Core Framework - Component Classes\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "class Expert(nn.Module):\n",
        "   \"\"\"An expert module with configurable random weight initialization.\"\"\"\n",
        "   def __init__(self, d_model, d_ffn, init_method='default'):\n",
        "      super().__init__()\n",
        "      self.w_down = nn.Linear(d_model, d_ffn)\n",
        "      self.activation = nn.GELU()\n",
        "      self.w_up = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "     if init_method == 'orthogonal':\n",
        "         nn.init.orthogonal_(self.w_down.weight)\n",
        "         nn.init.orthogonal_(self.w_up.weight)\n",
        "     elif init_method == 'sparse':\n",
        "         nn.init.sparse_(self.w_down.weight, sparsity=0.5)\n",
        "         nn.init.sparse_(self.w_up.weight, sparsity=0.5)\n",
        "     elif init_method != 'default':\n",
        "         raise ValueError(f\"Unknown initialization method: {init_method}\")\n",
        "\n",
        "     nn.init.zeros_(self.w_down.bias)\n",
        "     nn.init.zeros_(self.w_up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_up(self.activation(self.w_down(x)))\n",
        "\n",
        "class StandardFFN(nn.Module):\n",
        "   \"\"\"A standard Feed-Forward Network block for the baseline Transformer.\"\"\"\n",
        "   def __init__(self, d_model):\n",
        "      super().__init__()\n",
        "      d_ffn = d_model * 4\n",
        "\n",
        "     self.linear1 = nn.Linear(d_model, d_ffn)\n",
        "     self.activation = nn.GELU()\n",
        "     self.linear2 = nn.Linear(d_ffn, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear2(self.activation(self.linear1(x)))\n",
        "\n",
        "class SOMELayer(nn.Module):\n",
        "   def __init__(self, d_model, some_config):\n",
        "     super().__init__()\n",
        "     self.d_model = d_model\n",
        "     self.num_experts = some_config['num_experts']\n",
        "     self.d_ffn = some_config['d_ffn']\n",
        "     self.top_k = some_config['top_k']\n",
        "\n",
        "     # Heuristic update parameters\n",
        "     self.alpha = some_config['alpha']\n",
        "     self.beta = some_config['beta']\n",
        "     self.delta = some_config['delta']\n",
        "\n",
        "     # Key management parameters\n",
        "     self.theta_percentile = some_config['theta_percentile']\n",
        "     self.warmup_steps = some_config['warmup_steps']\n",
        "     self.ema_decay = some_config['ema_decay']\n",
        "\n",
        "     self.ablation_flags = some_config.get('ablation_flags', {'use_alpha': True, 'use_beta': True,\n",
        "'use_delta': True})\n",
        "\n",
        "     self.query_network = nn.Linear(d_model, d_model)\n",
        "\n",
        "     keys = torch.randn(self.num_experts, d_model)\n",
        "     self.register_buffer(\"key_store\", F.normalize(keys, p=2, dim=-1))\n",
        "     self.register_buffer(\"usage_count\", torch.zeros(self.num_experts))\n",
        "     self.register_buffer(\"steps\", torch.tensor([0], dtype=torch.long))\n",
        "\n",
        "     self.experts = nn.ModuleList([Expert(d_model, self.d_ffn,\n",
        "init_method=some_config['init_method']) for _ in range(self.num_experts)])\n",
        "\n",
        "     for expert in self.experts:\n",
        "        for param in expert.parameters():\n",
        "           param.requires_grad = False\n",
        "\n",
        "     if self.top_k > 1:\n",
        "\n",
        "          self.register_buffer(\"peer_pull_indices\", torch.combinations(torch.arange(self.top_k),\n",
        "r=2))\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x_flat = x.view(-1, self.d_model)\n",
        "\n",
        "        queries_raw = self.query_network(x_flat)\n",
        "        queries = F.normalize(queries_raw, p=2, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(queries, self.key_store.t())\n",
        "        top_k_scores, top_k_indices = torch.topk(scores, self.top_k, dim=-1)\n",
        "        gating_weights = F.softmax(top_k_scores / temperature, dim=-1)\n",
        "\n",
        "        flat_top_k_indices = top_k_indices.view(-1)\n",
        "\n",
        "     sorted_indices, permutation_map = torch.sort(flat_top_k_indices)\n",
        "     unique_expert_ids, counts = torch.unique_consecutive(sorted_indices,\n",
        "return_counts=True)\n",
        "\n",
        "        flat_inputs = x_flat.repeat_interleave(self.top_k, dim=0)\n",
        "        permuted_inputs = flat_inputs[permutation_map]\n",
        "        split_inputs = torch.split(permuted_inputs, counts.tolist(), dim=0)\n",
        "\n",
        "        output_chunks = []\n",
        "        for i, expert_id in enumerate(unique_expert_ids):\n",
        "           output_chunks.append(self.experts[expert_id](split_inputs[i]))\n",
        "\n",
        "        concatenated_outputs = torch.cat(output_chunks, dim=0)\n",
        "        inverse_permutation_map = torch.argsort(permutation_map)\n",
        "        expert_outputs = concatenated_outputs[inverse_permutation_map]\n",
        "\n",
        "     weighted_outputs = (expert_outputs.view(-1, self.top_k, self.d_model) *\n",
        "gating_weights.unsqueeze(-1)).sum(dim=1)\n",
        "     final_output = weighted_outputs.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return x + final_output, queries, top_k_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_keys(self, queries, top_k_indices):\n",
        "    self.steps += 1\n",
        "    unique_indices, counts = torch.unique(top_k_indices, return_counts=True)\n",
        "    self.usage_count.mul_(self.ema_decay)\n",
        "    self.usage_count.index_add_(0, unique_indices, (1.0 - self.ema_decay) * counts.float())\n",
        "\n",
        "    if self.ablation_flags.get('use_alpha', True):\n",
        "        for i in range(self.top_k):\n",
        "           indices = top_k_indices[:, i]\n",
        "           inertia = 1.0 + self.usage_count[indices]\n",
        "           alpha_effective = self.alpha / inertia.unsqueeze(-1)\n",
        "           update_vec = queries - self.key_store[indices]\n",
        "           self.key_store.index_add_(0, indices, alpha_effective * update_vec)\n",
        "\n",
        "    if self.top_k > 1 and self.ablation_flags.get('use_beta', True):\n",
        "        indices_i = top_k_indices[:, self.peer_pull_indices[:, 0]].reshape(-1)\n",
        "        indices_j = top_k_indices[:, self.peer_pull_indices[:, 1]].reshape(-1)\n",
        "        keys_i, keys_j = self.key_store[indices_i], self.key_store[indices_j]\n",
        "        inertia_i = (1.0 + self.usage_count[indices_i]).unsqueeze(-1)\n",
        "        inertia_j = (1.0 + self.usage_count[indices_j]).unsqueeze(-1)\n",
        "        beta_effective = self.beta / torch.min(inertia_i, inertia_j)\n",
        "\n",
        "       update_vec_i = beta_effective * (keys_j - keys_i)\n",
        "       update_vec_j = beta_effective * (keys_i - keys_j)\n",
        "       self.key_store.index_add_(0, indices_i, update_vec_i)\n",
        "       self.key_store.index_add_(0, indices_j, update_vec_j)\n",
        "\n",
        "    if self.steps > self.warmup_steps and self.ablation_flags.get('use_delta', True):\n",
        "        active_usage_counts = self.usage_count[self.usage_count > 0]\n",
        "        if active_usage_counts.numel() > 0:\n",
        "            dynamic_theta = torch.quantile(active_usage_counts.float(), self.theta_percentile)\n",
        "            low_usage_mask = self.usage_count < dynamic_theta\n",
        "            self.key_store[low_usage_mask] *= (1.0 - self.delta)\n",
        "\n",
        "    self.key_store.data = F.normalize(self.key_store.data, p=2, dim=-1)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "   def __init__(self, d_model, num_heads, some_config, architecture_type):\n",
        "     super().__init__()\n",
        "     self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "     self.norm1 = nn.LayerNorm(d_model)\n",
        "     self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.architecture_type = architecture_type\n",
        "    if self.architecture_type == 'some_candidate':\n",
        "        self.ff_layer = SOMELayer(d_model, some_config)\n",
        "    else:\n",
        "        self.ff_layer = StandardFFN(d_model)\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    seq_len = x.size(1)\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device) * float('-inf'), diagonal=1)\n",
        "    attn_output, _ = self.attention(x, x, x, attn_mask=mask, need_weights=False)\n",
        "    x = self.norm1(x + attn_output)\n",
        "\n",
        "    if self.architecture_type == 'some_candidate':\n",
        "        ff_output, queries, top_k_indices = self.ff_layer(x, temperature=temperature)\n",
        "        x = self.norm2(ff_output)\n",
        "        return x, queries, top_k_indices\n",
        "    else:\n",
        "        ff_output = self.ff_layer(x)\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x, None, None\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "   def __init__(self, d_model, max_len=5000):\n",
        "     super().__init__()\n",
        "     position = torch.arange(max_len).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "     pe = torch.zeros(1, max_len, d_model)\n",
        "     pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "     pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "     self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class UniversalTransformer(nn.Module):\n",
        "   def __init__(self, config):\n",
        "     super().__init__()\n",
        "     self.config = config\n",
        "     model_config = config['model']\n",
        "     some_config = config.get('some_layer')\n",
        "\n",
        "    self.embedding = nn.Embedding(model_config['vocab_size'], model_config['d_model'])\n",
        "    self.pos_encoder = PositionalEncoding(model_config['d_model'], model_config['seq_len'])\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "      TransformerBlock(\n",
        "          model_config['d_model'],\n",
        "          model_config['num_heads'],\n",
        "          some_config,\n",
        "          config['architecture_type']\n",
        "\n",
        "         ) for _ in range(model_config['num_layers'])\n",
        "    ])\n",
        "\n",
        "    self.fc_out = nn.Linear(model_config['d_model'], model_config['vocab_size'])\n",
        "    self.d_model = model_config['d_model']\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoder(x)\n",
        "\n",
        "    all_queries, all_indices = [], []\n",
        "    for layer in self.layers:\n",
        "       x, queries, top_k_indices = layer(x, temperature=temperature)\n",
        "       if queries is not None:\n",
        "           all_queries.append(queries)\n",
        "           all_indices.append(top_k_indices)\n",
        "\n",
        "    return self.fc_out(x), all_queries, all_indices\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_all_keys(self, all_queries, all_indices):\n",
        "    if self.config['architecture_type'] != 'some_candidate':\n",
        "        return\n",
        "\n",
        "    for i, layer_block in enumerate(self.layers):\n",
        "       if isinstance(layer_block.ff_layer, SOMELayer):\n",
        "           queries = all_queries[i].view(-1, layer_block.ff_layer.d_model)\n",
        "           indices = all_indices[i].view(-1, layer_block.ff_layer.top_k)\n",
        "           layer_block.ff_layer.update_keys(queries, indices)\n",
        "\n",
        "print(\"Core model components defined.\")\n",
        "____________________________________\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Data Preparation and Training/Evaluation Functions\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "class LanguageModelDataset(Dataset):\n",
        "   def __init__(self, tokenized_data):\n",
        "     self.data = tokenized_data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    inputs = torch.tensor(item['input_ids'])\n",
        "    targets = inputs.clone()\n",
        "    targets[:-1] = inputs[1:]\n",
        "    targets[-1] = -100 # Ignore loss for the last token prediction\n",
        "    return inputs, targets\n",
        "\n",
        "def prepare_data(config):\n",
        "  print(\"\\n--- Part 2: Data Preparation & Configuration ---\")\n",
        "  tokenizer_path = \"universal_bpe_tokenizer.json\"\n",
        "\n",
        "  if not os.path.exists(tokenizer_path):\n",
        "      print(\"Training universal BPE tokenizer...\")\n",
        "      # Use TinyStories as the base for the tokenizer as it's general-purpose\n",
        "      dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
        "\n",
        "     def get_training_corpus():\n",
        "       iterator = iter(dataset)\n",
        "       for i in range(20000): # Use 20k samples to build tokenizer\n",
        "          try:\n",
        "              yield next(iterator)['text']\n",
        "          except StopIteration:\n",
        "              break\n",
        "\n",
        "    tokenizer_model = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer_model.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"],\n",
        "vocab_size=config['model']['vocab_size'])\n",
        "\n",
        "     tokenizer_model.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "     tokenizer_model.save(tokenizer_path)\n",
        "  else:\n",
        "     print(\"Universal tokenizer already exists. Loading from file.\")\n",
        "\n",
        "  tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[EOS]'})\n",
        "  print(f\"Tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "  print(f\"\\nTokenizing dataset: {config['data']['dataset_name']}...\")\n",
        "  full_dataset = load_dataset(config['data']['dataset_name'])\n",
        "\n",
        "  # Ensure train/validation splits exist\n",
        "  train_split = 'train' if 'train' in full_dataset else list(full_dataset.keys())[0]\n",
        "  val_split = 'validation' if 'validation' in full_dataset else train_split\n",
        "\n",
        "  train_subset = full_dataset[train_split].select(range(min(len(full_dataset[train_split]),\n",
        "config['data']['train_subset_size'])))\n",
        "  val_subset = full_dataset[val_split].select(range(min(len(full_dataset[val_split]),\n",
        "config['data']['val_subset_size'])))\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    text_with_eos = [str(s) + tokenizer.eos_token for s in examples[\"text\"]]\n",
        "    return tokenizer(text_with_eos, truncation=True, padding=\"max_length\",\n",
        "max_length=config['model']['seq_len'], return_tensors=\"pt\")\n",
        "\n",
        "  tokenized_train = train_subset.map(tokenize_function, batched=True,\n",
        "remove_columns=train_subset.column_names, num_proc=os.cpu_count())\n",
        "  tokenized_val = val_subset.map(tokenize_function, batched=True,\n",
        "remove_columns=val_subset.column_names, num_proc=os.cpu_count())\n",
        "\n",
        "  # Set format to PyTorch\n",
        "  tokenized_train.set_format(type='torch', columns=['input_ids'])\n",
        "  tokenized_val.set_format(type='torch', columns=['input_ids'])\n",
        "\n",
        "  train_dataset = LanguageModelDataset(tokenized_train)\n",
        "  validation_dataset = LanguageModelDataset(tokenized_val)\n",
        "\n",
        "  num_workers = max(2, os.cpu_count() // 2 if os.cpu_count() else 2)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], shuffle=True,\n",
        "drop_last=True, num_workers=num_workers, pin_memory=True)\n",
        "  validation_loader = DataLoader(validation_dataset, batch_size=config['data']['batch_size'],\n",
        "drop_last=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "  print(f\"\\nTrain dataset size (subset): {len(train_dataset)}\")\n",
        "  print(f\"Validation dataset size (subset): {len(validation_dataset)}\")\n",
        "  print(f\"Using {num_workers} workers for DataLoader.\")\n",
        "\n",
        "  return train_loader, validation_loader, tokenizer\n",
        "\n",
        "def calculate_gini(usage_counts):\n",
        "  counts = usage_counts.cpu().to(torch.float32).numpy()\n",
        "  if np.sum(counts) == 0: return 0.0\n",
        "  counts = np.sort(counts)\n",
        "  n = len(counts)\n",
        "\n",
        "  index = np.arange(1, n + 1)\n",
        "  return (np.sum((2 * index - n - 1) * counts)) / (n * np.sum(counts))\n",
        "\n",
        "def calculate_entropy(usage_counts):\n",
        "  total_usage = usage_counts.sum()\n",
        "  if total_usage == 0: return 0.0\n",
        "  probs = usage_counts / total_usage\n",
        "  probs = probs[probs > 0]\n",
        "  return -torch.sum(probs * torch.log2(probs)).item()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, scheduler, current_temp,\n",
        "tokenizer_vocab_size):\n",
        "   model.train()\n",
        "   total_loss = 0\n",
        "   scaler = torch.cuda.amp.GradScaler()\n",
        "   progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "  for inputs, targets in progress_bar:\n",
        "     inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "\n",
        "     optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "     with torch.cuda.amp.autocast():\n",
        "        logits, queries, indices = model(inputs, temperature=current_temp)\n",
        "        loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "\n",
        "     scaler.scale(loss).backward()\n",
        "     scaler.unscale_(optimizer)\n",
        "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "     scaler.step(optimizer)\n",
        "     scaler.update()\n",
        "     scheduler.step()\n",
        "\n",
        "     if model.config['architecture_type'] == 'some_candidate':\n",
        "        model.update_all_keys(queries, indices)\n",
        "\n",
        "     total_loss += loss.item()\n",
        "     progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.1e}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, tokenizer_vocab_size):\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "  progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device,\n",
        "non_blocking=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "           logits, _, _ = model(inputs, temperature=0.5) # Sharpen during eval for stability\n",
        "           loss = criterion(logits.view(-1, tokenizer_vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "def plot_and_save_metrics(train_losses, val_losses, some_metrics, config, save_dir):\n",
        "  epochs = len(train_losses)\n",
        "  plt.figure(figsize=(12, 10))\n",
        "\n",
        "  # Plot Loss\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(range(1, epochs + 1), train_losses, 'b-o', label='Training Loss')\n",
        "  plt.plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss')\n",
        "  title = f\"Loss Curve for {config['run_name']}\"\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xticks(range(1, epochs + 1))\n",
        "\n",
        "  # Plot SoME Metrics if available\n",
        "  if config['architecture_type'] == 'some_candidate':\n",
        "      gini_coeffs = [m['gini'] for m in some_metrics]\n",
        "      entropy_vals = [m['entropy'] for m in some_metrics]\n",
        "      plt.subplot(2, 1, 2)\n",
        "      ax1 = plt.gca()\n",
        "      ax2 = ax1.twinx()\n",
        "\n",
        "     ax1.plot(range(1, epochs + 1), gini_coeffs, 'g-s', label='Gini Coefficient')\n",
        "     ax2.plot(range(1, epochs + 1), entropy_vals, 'm-^', label='Shannon Entropy')\n",
        "\n",
        "     plt.title(f\"SoME Expert Metrics (Middle Layer)\")\n",
        "     ax1.set_xlabel('Epochs')\n",
        "\n",
        "     ax1.set_ylabel('Gini Coefficient', color='g')\n",
        "     ax2.set_ylabel('Shannon Entropy', color='m')\n",
        "     ax1.tick_params(axis='y', labelcolor='g')\n",
        "     ax2.tick_params(axis='y', labelcolor='m')\n",
        "     ax1.legend(loc='upper left')\n",
        "     ax2.legend(loc='upper right')\n",
        "     ax1.grid(True)\n",
        "     plt.xticks(range(1, epochs + 1))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  filename = os.path.join(save_dir, f\"metrics_{config['run_name']}.png\")\n",
        "  plt.savefig(filename)\n",
        "  print(f\"\\nMetrics plot saved to {filename}\")\n",
        "  plt.show()\n",
        "\n",
        "print(\"Helper functions for data, training, and evaluation defined.\")\n",
        "____________________________________\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Main Execution Function\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "def main(config):\n",
        "  \"\"\"Main function to run a single, complete experiment.\"\"\"\n",
        "\n",
        "  print(f\"\\n--- Starting Experiment: {config['run_name']} ---\")\n",
        "\n",
        "   # Create a directory for this run's results\n",
        "   base_save_dir = '/content/drive/MyDrive/SoME_Experiments' if DRIVE_MOUNTED else\n",
        "'/content/SoME_Experiments'\n",
        "   run_save_dir = os.path.join(base_save_dir, config['run_name'])\n",
        "   os.makedirs(run_save_dir, exist_ok=True)\n",
        "   print(f\"Results will be saved in: {run_save_dir}\")\n",
        "\n",
        "  # 1. Data\n",
        "  train_loader, val_loader, tokenizer = prepare_data(config)\n",
        "\n",
        "  # 2. Model Initialization\n",
        "  print(\"\\n--- Part 3: Model Definition ---\")\n",
        "  model = UniversalTransformer(config).to(device)\n",
        "\n",
        "   # 3. Training Setup\n",
        "   print(\"\\n--- Part 4: Training, Evaluation, and Metrics ---\")\n",
        "   trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "   total_params = sum(p.numel() for p in model.parameters())\n",
        "   print(f\"Total Parameters: {total_params/1e6:.2f}M\")\n",
        "   print(f\"Trainable Parameters: {trainable_params/1e6:.2f}M ({100 * trainable_params /\n",
        "total_params:.2f}%)\")\n",
        "\n",
        "   optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "lr=config['training']['learning_rate'], betas=(0.9, 0.95), weight_decay=0.1)\n",
        "   criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "   total_steps = len(train_loader) * config['training']['num_epochs']\n",
        "   scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "  # 4. Training Loop\n",
        "  train_losses, val_losses = [], []\n",
        "  some_metrics_log = []\n",
        "  best_val_loss = float('inf')\n",
        "  model_save_path = os.path.join(run_save_dir, f\"best_model_{config['run_name']}.pth\")\n",
        "\n",
        "  for epoch in range(config['training']['num_epochs']):\n",
        "     print(f\"\\n--- Epoch {epoch+1}/{config['training']['num_epochs']} ---\")\n",
        "\n",
        "     train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler,\n",
        "config['training']['training_temp'], tokenizer.vocab_size)\n",
        "     val_loss = evaluate_epoch(model, val_loader, criterion, tokenizer.vocab_size)\n",
        "     perplexity = math.exp(val_loss)\n",
        "\n",
        "     train_losses.append(train_loss)\n",
        "     val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val\n",
        "Perplexity = {perplexity:.2f}\")\n",
        "\n",
        "     if config['architecture_type'] == 'some_candidate':\n",
        "         middle_layer_idx = config['model']['num_layers'] // 2\n",
        "         usage_counts = model.layers[middle_layer_idx].ff_layer.usage_count\n",
        "         gini_coeff = calculate_gini(usage_counts)\n",
        "         entropy_val = calculate_entropy(usage_counts)\n",
        "         some_metrics_log.append({'gini': gini_coeff, 'entropy': entropy_val})\n",
        "         print(f\" Middle Layer Expert Metrics: Gini = {gini_coeff:.3f}, Entropy = {entropy_val:.3f}\")\n",
        "\n",
        "     if val_loss < best_val_loss:\n",
        "         best_val_loss = val_loss\n",
        "\n",
        "       torch.save(model.state_dict(), model_save_path)\n",
        "       print(f\" -> New best model saved to {model_save_path}\")\n",
        "\n",
        "  # 5. Finalization\n",
        "  print(f\"\\n--- Training Complete for {config['run_name']} ---\")\n",
        "  final_perplexity = math.exp(best_val_loss)\n",
        "  print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "  print(f\"Final Validation Perplexity: {final_perplexity:.2f}\")\n",
        "\n",
        "  # Save a summary file\n",
        "  summary_path = os.path.join(run_save_dir, \"summary.txt\")\n",
        "  with open(summary_path, 'w') as f:\n",
        "     f.write(f\"Run Name: {config['run_name']}\\n\")\n",
        "     f.write(f\"Best Validation Loss: {best_val_loss:.4f}\\n\")\n",
        "     f.write(f\"Final Validation Perplexity: {final_perplexity:.2f}\\n\")\n",
        "\n",
        "  plot_and_save_metrics(train_losses, val_losses, some_metrics_log, config, run_save_dir)\n",
        "\n",
        "print(\"Main execution function defined.\")\n",
        "____________________________________\n",
        "#\n",
        "========================================================================\n",
        "======\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Experiment Configuration and Execution Control Panel\n",
        "#\n",
        "========================================================================\n",
        "======\n",
        "\n",
        "def get_config(architecture, dataset_name):\n",
        "  \"\"\"\n",
        "  Generates the configuration for a specific experimental run.\n",
        "  architecture: 'transformer_baseline' or 'some_candidate'\n",
        "  dataset_name: 'tinystories', 'tiny_textbooks', or 'tiny_codes'\n",
        "  \"\"\"\n",
        "  config = {\n",
        "      \"run_name\": f\"{architecture}_{dataset_name}\",\n",
        "      \"architecture_type\": architecture,\n",
        "      \"model\": {\n",
        "         \"vocab_size\": 8192,\n",
        "         \"d_model\": 512,\n",
        "         \"num_layers\": 8,\n",
        "         \"num_heads\": 8,\n",
        "         \"seq_len\": 512,\n",
        "      },\n",
        "\n",
        "      \"data\": {\n",
        "         \"dataset_name\": {\n",
        "            \"tinystories\": \"roneneldan/TinyStories\",\n",
        "            \"tiny_textbooks\": \"nampdn-ai/tiny-textbooks\",\n",
        "            \"tiny_codes\": \"nampdn-ai/tiny-codes\"\n",
        "         },\n",
        "         \"train_subset_size\": 20000,\n",
        "         \"val_subset_size\": 5000,\n",
        "         \"batch_size\": 24,\n",
        "      },\n",
        "      \"training\": {\n",
        "         \"num_epochs\": 4,\n",
        "         \"learning_rate\": 4e-4,\n",
        "         \"training_temp\": 1.0,\n",
        "      },\n",
        "  }\n",
        "\n",
        "  if architecture == 'some_candidate':\n",
        "      config[\"some_layer\"] = {\n",
        "        \"num_experts\": 128,\n",
        "        \"d_ffn\": 1536,\n",
        "        \"top_k\": 8,\n",
        "        \"init_method\": \"sparse\",\n",
        "        \"alpha\": 0.015,\n",
        "        \"beta\": 0.001,\n",
        "        \"delta\": 0.001,\n",
        "        \"theta_percentile\": 0.05,\n",
        "        \"warmup_steps\": 400,\n",
        "        \"ema_decay\": 0.995,\n",
        "        \"ablation_flags\": {\"use_alpha\": True, \"use_beta\": True, \"use_delta\": True}\n",
        "      }\n",
        "  return config\n",
        "\n",
        "# --- SELECT AND RUN YOUR EXPERIMENT ---\n",
        "# Uncomment one of the following lines to choose your run, then execute the cell.\n",
        "\n",
        "# --- TinyStories Runs ---\n",
        "#config = get_config(architecture='transformer_baseline', dataset_name='tinystories')\n",
        "#config = get_config(architecture='some_candidate', dataset_name='tinystories')\n",
        "\n",
        "# --- TinyTextbooks Runs ---\n",
        "#config = get_config(architecture='transformer_baseline', dataset_name='tiny_textbooks')\n",
        "#config = get_config(architecture='some_candidate', dataset_name='tiny_textbooks')\n",
        "\n",
        "# --- TinyCodes Runs ---\n",
        "#config = get_config(architecture='transformer_baseline', dataset_name='tiny_codes')\n",
        "#config = get_config(architecture='some_candidate', dataset_name='tiny_codes')\n",
        "\n",
        "\n",
        "# --- Execute the selected experiment ---\n",
        "main(config)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}